<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>‚õìÔ∏è Autograd Integration &amp; Backward Pass - Mojo üî• GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Mojoüî• GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta property="og:image" content="..//puzzles_images/puzzle-mark.svg">
        <meta property="og:url" content="https://builds.modular.com/puzzles">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="Mojoüî• GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta name="twitter:image" content="..//puzzles_images/puzzle-mark.svg">
        <link rel="icon" type="image/png" href="..//puzzles_images/puzzle-mark.svg">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="-autograd-integration--backward-pass"><a class="header" href="#-autograd-integration--backward-pass">‚õìÔ∏è Autograd Integration &amp; Backward Pass</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>In this puzzle, we explore the backward pass implementation of the fused LayerNorm + Linear operation. The backward pass computes gradients with respect to:</p>
<ul>
<li>Input tensor</li>
<li>LayerNorm scale (\(\gamma\)) and shift (\(\beta\)) parameters</li>
<li>Linear layer weight matrix and bias</li>
</ul>
<p>The mathematical operations we‚Äôre implementing are:</p>
<ol>
<li>
<p>LayerNorm backward (details of derivation in <a href="#detailed-derivation-of-layernorm-backward-pass">Detailed derivation of LayerNorm backward pass</a>):
\[\Large \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \odot \gamma \odot \frac{1}{\sqrt{\sigma^2 + \epsilon}} (1 - \frac{1}{H} - \frac{(x - \mu)^2}{H(\sigma^2 + \epsilon)}) \]</p>
</li>
<li>
<p>Linear backward:
\[\Large \frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}x^T \]
\[\Large \frac{\partial L}{\partial b} = \frac{\partial L}{\partial y} \]
\[\Large \frac{\partial L}{\partial x} = W^T\frac{\partial L}{\partial y} \]</p>
</li>
<li>
<p>Chain Rule for Fused Operation:
\[\Large \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y_{linear}} \frac{\partial y_{linear}}{\partial y_{norm}} \frac{\partial y_{norm}}{\partial x} \]
where:</p>
</li>
</ol>
<ul>
<li>\(y_{norm}\) is the LayerNorm output</li>
<li>\(y_{linear}\) is the Linear layer output</li>
<li>The chain rule ensures proper gradient flow through both operations</li>
</ul>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<ul>
<li>
<p><strong>Thread organization</strong>:</p>
<ul>
<li>One thread block per sequence position (grid: <code>[batch_size, seq_len]</code>)</li>
<li>Single thread per sequence position to avoid redundancy</li>
<li>Compute all gradients for each sequence position in one thread</li>
<li>Ensure proper thread synchronization for atomic operations</li>
</ul>
</li>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Access input tensor with <code>[batch_idx, seq_idx, h]</code></li>
<li>Access output tensor with <code>[batch_idx, seq_idx, out_idx]</code></li>
<li>Access weights with <code>[out_idx, h]</code> for linear layer</li>
<li>Ensure memory alignment for atomic operations</li>
<li>Use shared memory for frequently accessed data</li>
</ul>
</li>
<li>
<p><strong>Computation flow</strong>:</p>
<ul>
<li>Compute LayerNorm statistics in same order as forward pass</li>
<li>Reuse normalized values for all output dimensions</li>
<li>Combine normalization and linear transformation</li>
<li>Maintain numerical stability throughout</li>
<li>Handle edge cases properly</li>
</ul>
</li>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Avoid redundant computation of statistics</li>
<li>Minimize memory traffic by fusing operations</li>
<li>Use proper type casting with <code>rebind[Scalar[dtype]]</code></li>
<li>Ensure proper memory alignment</li>
<li>Optimize for autograd integration</li>
</ul>
</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li>Batch size: <code>BATCH_SIZE = 4</code></li>
<li>Sequence length: <code>SEQ_LEN = 4</code></li>
<li>Hidden dimension: <code>HIDDEN_DIM = 8</code></li>
<li>Output dimension: <code>OUTPUT_DIM = 16</code></li>
<li>Epsilon: <code>EPS = 1e-5</code></li>
<li>Data type: <code>DType.float32</code></li>
</ul>
<h2 id="implementation-challenging"><a class="header" href="#implementation-challenging">Implementation (challenging)</a></h2>
<p>The fused backward kernel combines LayerNorm and Linear backward operations into a single GPU kernel. This is a challenging implementation that requires careful handling of:</p>
<ul>
<li><a href="https://docs.modular.com/mojo/stdlib/os/atomic/Atomic/">Atomic operations</a> for gradient accumulation</li>
<li>Numerical stability in gradient computations</li>
<li>Memory access patterns for efficient GPU utilization</li>
<li>Proper synchronization between operations</li>
</ul>
<pre><code class="language-mojo">fn minimal_fused_kernel_backward[
    grad_output_layout: Layout,
    input_layout: Layout,
    ln_params_layout: Layout,
    weight_layout: Layout,
    grad_input_layout: Layout,
    grad_ln_weight_layout: Layout,
    grad_ln_bias_layout: Layout,
    grad_weight_layout: Layout,
    grad_bias_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
    output_dim: Int,
](
    grad_input: LayoutTensor[mut=True, dtype, grad_input_layout],
    grad_ln_weight: LayoutTensor[mut=True, dtype, grad_ln_weight_layout],
    grad_ln_bias: LayoutTensor[mut=True, dtype, grad_ln_bias_layout],
    grad_weight: LayoutTensor[mut=True, dtype, grad_weight_layout],
    grad_bias: LayoutTensor[mut=True, dtype, grad_bias_layout],
    grad_output: LayoutTensor[mut=False, dtype, grad_output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
    linear_weight: LayoutTensor[mut=False, dtype, weight_layout],
):
    """Fused backward kernel using atomic operations for safe gradient accumulation.
    """
    # Grid: (batch_size, seq_len) - one thread per sequence position
    # Block: (1,) - single thread per sequence position
    batch_idx = block_idx.x
    seq_idx = block_idx.y

    if batch_idx &gt;= batch_size or seq_idx &gt;= seq_len:
        return

    # Step 1: Recompute forward pass statistics (needed for gradients)
    var sum_val: Scalar[dtype] = 0
    var sq_sum: Scalar[dtype] = 0

    # FILL IN roughly 8 lines

    # Step 2: Atomically accumulate gradients w.r.t. linear bias

    # FILL IN roughly 4 lines

    # Step 3: Atomically accumulate gradients w.r.t. linear weight
    # Make sure to use the correct atomic operation to avoid race conditions

    # FILL IN roughly 10 lines

    # Step 4: Atomically accumulate gradients w.r.t. LayerNorm parameters

    # FILL IN roughly 10 lines

    # Step 5: Compute gradients w.r.t. input (LayerNorm backward)
    # Compute sum terms needed for LayerNorm backward
    # Make sure to use the correct atomic operation to avoid race conditions

    # FILL IN roughly 12 lines

    # Compute actual input gradients (no race conditions here - each thread writes to different positions)

    # FILL IN roughly 10 lines


</code></pre>
<p><strong>Key optimizations:</strong></p>
<ul>
<li>Single kernel launch for all gradient computations</li>
<li>Atomic operations for safe gradient accumulation</li>
<li>Coalesced memory access patterns</li>
<li>Reduced memory bandwidth usage</li>
<li>No intermediate tensor allocations</li>
</ul>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>
<p><strong>Thread organization</strong>:</p>
<ul>
<li>One thread block per sequence position</li>
<li>Single thread per sequence position</li>
<li>Compute all gradients in one thread</li>
</ul>
</li>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Coalesced access for input/output tensors</li>
<li>Strided access for weight matrix</li>
<li>Proper alignment for atomic operations</li>
</ul>
</li>
<li>
<p><strong>Computation flow</strong>:</p>
<ul>
<li>Compute statistics in same order as forward pass</li>
<li>Reuse normalized values</li>
<li>Maintain numerical stability</li>
</ul>
</li>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Minimize memory traffic</li>
<li>Use proper type casting</li>
<li>Ensure proper alignment</li>
</ul>
</li>
</ol>
</div>
</details>
<h3 id="running-the-code"><a class="header" href="#running-the-code">Running the code</a></h3>
<p>To test your fused backward implementation, run:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p20 --backward
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p20 --backward
</code></pre>
  </div>
</div>
<p>Your output will look like this:</p>
<pre><code class="language-txt">Testing with dimensions: [4, 4, 8] -&gt; [4, 4, 16]
‚úÖ Loaded Mojo operations library
============================================================
           Comprehensive Backward Pass Test
           Testing Custom LayerNorm + Linear Gradients
============================================================
Testing with dimensions: [4, 4, 8] -&gt; [4, 4, 16]

Testing CPU Backward Pass:

Testing CPU Backward Implementation - Backward Pass
---------------------------------------------------------
   Computing PyTorch autograd reference...
   Computing Mojo backward implementation (CPU)...
‚úÖ CPU Backward Implementation backward completed
   Forward max difference: 1.49e-08
   grad_input: 2.98e-08 ‚úÖ
   grad_ln_weight: 5.96e-08 ‚úÖ
   grad_ln_bias: 2.38e-07 ‚úÖ
   grad_linear_weight: 9.54e-07 ‚úÖ
   grad_linear_bias: 0.00e+00 ‚úÖ

   Forward pass: ‚úÖ CORRECT
   Gradients:    ‚úÖ CORRECT
   Overall:      ‚úÖ CORRECT

Testing GPU Backward Pass:

Testing GPU Backward Implementation - Backward Pass
---------------------------------------------------------
   Computing PyTorch autograd reference...
   Computing Mojo backward implementation (GPU)...

‚úÖ GPU Backward Implementation backward completed
   Forward max difference: 1.86e-08
   grad_input: 4.47e-08 ‚úÖ
   grad_ln_weight: 5.96e-08 ‚úÖ
   grad_ln_bias: 3.58e-07 ‚úÖ
   grad_linear_weight: 9.54e-07 ‚úÖ
   grad_linear_bias: 0.00e+00 ‚úÖ

   Forward pass: ‚úÖ CORRECT
   Gradients:    ‚úÖ CORRECT
   Overall:      ‚úÖ CORRECT

Backward Pass Test Summary:
   - CPU Backward:  ‚úÖ CORRECT
   - GPU Backward:  ‚úÖ CORRECT

   Overall Result: ‚úÖ ALL CORRECT

BACKWARD PASS Test Completed!
</code></pre>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn minimal_fused_kernel_backward[
    grad_output_layout: Layout,
    input_layout: Layout,
    ln_params_layout: Layout,
    weight_layout: Layout,
    grad_input_layout: Layout,
    grad_ln_weight_layout: Layout,
    grad_ln_bias_layout: Layout,
    grad_weight_layout: Layout,
    grad_bias_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
    output_dim: Int,
](
    grad_input: LayoutTensor[mut=True, dtype, grad_input_layout],
    grad_ln_weight: LayoutTensor[mut=True, dtype, grad_ln_weight_layout],
    grad_ln_bias: LayoutTensor[mut=True, dtype, grad_ln_bias_layout],
    grad_weight: LayoutTensor[mut=True, dtype, grad_weight_layout],
    grad_bias: LayoutTensor[mut=True, dtype, grad_bias_layout],
    grad_output: LayoutTensor[mut=False, dtype, grad_output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
    linear_weight: LayoutTensor[mut=False, dtype, weight_layout],
):
    """Fused backward kernel using atomic operations for safe gradient accumulation.
    """
    # Grid: (batch_size, seq_len) - one thread per sequence position
    # Block: (1,) - single thread per sequence position
    batch_idx = block_idx.x
    seq_idx = block_idx.y

    if batch_idx &gt;= batch_size or seq_idx &gt;= seq_len:
        return

    # Step 1: Recompute forward pass statistics (needed for gradients)
    var sum_val: Scalar[dtype] = 0
    var sq_sum: Scalar[dtype] = 0

    @parameter
    for h in range(hidden_dim):
        val = input[batch_idx, seq_idx, h]
        sum_val += rebind[Scalar[dtype]](val)
        sq_sum += rebind[Scalar[dtype]](val * val)

    mean_val = sum_val / hidden_dim
    var_val = (sq_sum / hidden_dim) - (mean_val * mean_val)
    inv_std = 1.0 / sqrt(var_val + 1e-5)

    # Step 2: Atomically accumulate gradients w.r.t. linear bias
    @parameter
    for out_idx in range(output_dim):
        grad_bias_ptr = grad_bias.ptr.offset(out_idx)
        _ = Atomic[dtype].fetch_add(
            grad_bias_ptr,
            rebind[Scalar[dtype]](grad_output[batch_idx, seq_idx, out_idx]),
        )

    # Step 3: Atomically accumulate gradients w.r.t. linear weight
    @parameter
    for out_idx in range(output_dim):

        @parameter
        for h in range(hidden_dim):
            var input_val = input[batch_idx, seq_idx, h]
            var normalized = (input_val - mean_val) * inv_std
            var ln_output_val = normalized * rebind[Scalar[dtype]](
                ln_weight[h]
            ) + rebind[Scalar[dtype]](ln_bias[h])

            # Atomic gradient accumulation for linear weight
            var grad_w = (
                grad_output[batch_idx, seq_idx, out_idx] * ln_output_val
            )
            var grad_weight_ptr = grad_weight.ptr.offset(
                out_idx * hidden_dim + h
            )
            _ = Atomic.fetch_add(grad_weight_ptr, rebind[Scalar[dtype]](grad_w))

    # Step 4: Atomically accumulate gradients w.r.t. LayerNorm parameters
    @parameter
    for h in range(hidden_dim):
        input_val = input[batch_idx, seq_idx, h]
        normalized = (input_val - mean_val) * inv_std

        # Compute gradient w.r.t. LayerNorm output for this h
        var grad_ln_out: Scalar[dtype] = 0

        @parameter
        for out_idx in range(output_dim):
            grad_ln_out = grad_ln_out + rebind[Scalar[dtype]](
                grad_output[batch_idx, seq_idx, out_idx]
                * linear_weight[out_idx, h]
            )

        # Atomic accumulation of LayerNorm parameter gradients
        grad_ln_weight_ptr = grad_ln_weight.ptr.offset(h)
        grad_ln_bias_ptr = grad_ln_bias.ptr.offset(h)
        _ = Atomic[dtype].fetch_add(
            grad_ln_weight_ptr, rebind[Scalar[dtype]](grad_ln_out * normalized)
        )
        _ = Atomic[dtype].fetch_add(
            grad_ln_bias_ptr, rebind[Scalar[dtype]](grad_ln_out)
        )

    # Step 5: Compute gradients w.r.t. input (LayerNorm backward)
    # Compute sum terms needed for LayerNorm backward
    var sum_grad_normalized: Scalar[dtype] = 0
    var sum_grad_normalized_times_normalized: Scalar[dtype] = 0

    @parameter
    for h in range(hidden_dim):
        h_input_val = input[batch_idx, seq_idx, h]
        h_normalized = (h_input_val - mean_val) * inv_std

        var h_grad_ln_out: Scalar[dtype] = 0

        @parameter
        for out_idx in range(output_dim):
            h_grad_ln_out = h_grad_ln_out + rebind[Scalar[dtype]](
                grad_output[batch_idx, seq_idx, out_idx]
                * linear_weight[out_idx, h]
            )

        h_grad_norm = h_grad_ln_out * rebind[Scalar[dtype]](ln_weight[h])
        sum_grad_normalized = sum_grad_normalized + rebind[Scalar[dtype]](
            h_grad_norm
        )
        sum_grad_normalized_times_normalized = (
            sum_grad_normalized_times_normalized
            + rebind[Scalar[dtype]](h_grad_norm * h_normalized)
        )

    # Compute actual input gradients (no race conditions here - each thread writes to different positions)
    @parameter
    for h in range(hidden_dim):
        h_input_val = input[batch_idx, seq_idx, h]
        h_normalized = (h_input_val - mean_val) * inv_std

        var h_grad_ln_out: Scalar[dtype] = 0

        @parameter
        for out_idx in range(output_dim):
            h_grad_ln_out = h_grad_ln_out + rebind[Scalar[dtype]](
                grad_output[batch_idx, seq_idx, out_idx]
                * linear_weight[out_idx, h]
            )

        h_grad_norm = h_grad_ln_out * rebind[Scalar[dtype]](ln_weight[h])
        grad_input[batch_idx, seq_idx, h] = inv_std * (
            h_grad_norm
            - (sum_grad_normalized / hidden_dim)
            - (h_normalized * sum_grad_normalized_times_normalized / hidden_dim)
        )


</code></pre>
<div class="solution-explanation">
<p>The fused backward implementation combines operations efficiently:</p>
<ol>
<li>
<p><strong>Thread organization and memory layout</strong>:</p>
<ul>
<li>Grid dimensions: <code>[batch_size, seq_len]</code> for one thread block per sequence position</li>
<li>Thread indices: <code>batch_idx = block_idx.x</code>, <code>seq_idx = block_idx.y</code></li>
<li>Memory layout:
<ul>
<li>Input tensor: <code>[batch_size, seq_len, hidden_dim]</code></li>
<li>Output tensor: <code>[batch_size, seq_len, output_dim]</code></li>
<li>Weight matrix: <code>[output_dim, hidden_dim]</code></li>
<li>Gradients: <code>[batch_size, seq_len, hidden_dim]</code> for input gradients</li>
<li>Parameter gradients: <code>[hidden_dim]</code> for LayerNorm, <code>[output_dim, hidden_dim]</code> for Linear</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>LayerNorm backward phase</strong>:</p>
<ul>
<li>Recompute forward pass statistics in same order as forward pass:
<ul>
<li>Mean: \[\Large \mu = \frac{1}{H} \sum_{i=1}^{H} x_i \]</li>
<li>Variance: \[\Large \sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2 \]</li>
<li>Inverse standard deviation: \[\Large \text{inv_std} = \frac{1}{\sqrt{\sigma^2 + \epsilon}} \]</li>
</ul>
</li>
<li>Compute normalized values: \[\Large \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \]</li>
<li>Calculate gradients:
<ul>
<li>Input gradient: \[\Large \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \odot \gamma \odot \frac{1}{\sqrt{\sigma^2 + \epsilon}} (1 - \frac{1}{H} - \frac{(x - \mu)^2}{H(\sigma^2 + \epsilon)}) \]</li>
<li>Scale gradient: \[\Large \frac{\partial L}{\partial \gamma} = \sum_{i=1}^{H} \frac{\partial L}{\partial y_i} \odot \hat{x}_i \]</li>
<li>Shift gradient: \[\Large \frac{\partial L}{\partial \beta} = \sum_{i=1}^{H} \frac{\partial L}{\partial y_i} \]</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Linear backward phase</strong>:</p>
<ul>
<li>For each output dimension:
<ul>
<li>Bias gradient: \[\Large \frac{\partial L}{\partial b} = \frac{\partial L}{\partial y} \]</li>
<li>Weight gradient: \[\Large \frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}x^T \]</li>
<li>Input gradient: \[\Large \frac{\partial L}{\partial x} = W^T\frac{\partial L}{\partial y} \]</li>
</ul>
</li>
<li>Use atomic operations for gradient accumulation:
<ul>
<li><code>atomic_add</code> for bias gradients with proper alignment</li>
<li><code>atomic_add</code> for weight gradients with proper alignment</li>
<li><code>atomic_add</code> for LayerNorm parameter gradients with proper alignment</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Memory access patterns</strong>:</p>
<ul>
<li>Coalesced access for input/output tensors</li>
<li>Strided access for weight matrix</li>
<li>Atomic operations for gradient accumulation</li>
<li>Shared memory for intermediate results</li>
<li>Register usage for frequently accessed values</li>
<li>Proper memory alignment for all operations</li>
</ul>
</li>
<li>
<p><strong>Numerical stability</strong>:</p>
<ul>
<li>Careful handling of epsilon in denominator</li>
<li>Proper scaling of gradients</li>
<li>Stable computation of statistics</li>
<li>Type casting with <code>rebind[Scalar[dtype]]</code></li>
<li>Proper handling of edge cases</li>
<li>Maintain same computation order as forward pass</li>
</ul>
</li>
<li>
<p><strong>Performance optimizations</strong>:</p>
<ul>
<li>Single kernel launch for all operations</li>
<li>Reuse of computed statistics</li>
<li>Minimized memory traffic</li>
<li>No intermediate tensor allocations</li>
<li>Efficient thread utilization</li>
<li>Reduced synchronization points</li>
<li>Optimized memory access patterns</li>
<li>Proper memory alignment</li>
</ul>
</li>
<li>
<p><strong>Implementation details</strong>:</p>
<ul>
<li>Use of <code>@parameter</code> for compile-time constants</li>
<li>Proper handling of tensor dimensions</li>
<li>Efficient type casting and conversions</li>
<li>Careful management of shared memory</li>
<li>Proper synchronization between operations</li>
<li>Error handling and boundary checks</li>
<li>Integration with PyTorch‚Äôs autograd system</li>
</ul>
</li>
</ol>
<p>This implementation achieves better performance than the unfused version by:</p>
<ul>
<li>Reducing memory bandwidth usage through kernel fusion</li>
<li>Minimizing kernel launch overhead</li>
<li>Optimizing memory access patterns</li>
<li>Efficient use of GPU resources</li>
<li>Maintaining numerical stability</li>
<li>Proper handling of gradient accumulation</li>
<li>Ensuring proper memory alignment</li>
<li>Efficient autograd integration</li>
</ul>
<p>The fused backward pass is particularly important in transformer architectures where LayerNorm + Linear operations are frequently used together, making the performance benefits significant for real-world applications.</p>
</div>
</details>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance considerations</a></h2>
<p>The backward pass implementation uses <code>torch.compile</code> with optimizations to minimize overhead:</p>
<pre><code class="language-python"># Compilation configuration
torch._dynamo.config.cache_size_limit = 64  # Increase cache
torch._dynamo.config.suppress_errors = True  # Handle errors gracefully
torch._dynamo.config.automatic_dynamic_shapes = True  # Dynamic shapes
</code></pre>
<p>These optimizations are particularly important for the backward pass because:</p>
<ul>
<li>Small tensor operations benefit from compilation caching</li>
<li>Dynamic shapes are common in backward passes</li>
<li>Error handling needs to be robust for gradient computation</li>
<li>Cache size helps with repeated backward operations</li>
<li>Proper error handling is crucial for gradient computation</li>
<li>Compilation overhead can significantly impact training time</li>
</ul>
<p>The backward pass is compiled with <code>reduce-overhead</code> mode to minimize the compilation overhead while maintaining correctness. This is especially important because:</p>
<ul>
<li>Backward passes are called frequently during training</li>
<li>Gradient computation needs to be numerically stable</li>
<li>Memory access patterns need to be optimized</li>
<li>Atomic operations require proper synchronization</li>
<li>Autograd integration needs to be efficient</li>
</ul>
<h2 id="detailed-derivation-of-layernorm-backward-pass"><a class="header" href="#detailed-derivation-of-layernorm-backward-pass">Detailed derivation of LayerNorm backward pass</a></h2>
<p>The backward pass gradient for LayerNorm is derived through careful application of the chain rule. Here‚Äôs the step-by-step derivation:</p>
<h3 id="forward-pass-operations"><a class="header" href="#forward-pass-operations">Forward pass operations</a></h3>
<ul>
<li>Mean: \(\mu = \frac{1}{H} \sum_{i=1}^{H} x_i\)</li>
<li>Variance: \(\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2\)</li>
<li>Normalized value: \(\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}\)</li>
<li>Final output: \(y = \gamma \odot \hat{x} + \beta\)</li>
</ul>
<h3 id="chain-rule-application"><a class="header" href="#chain-rule-application">Chain rule application</a></h3>
<p>To compute \(\frac{\partial L}{\partial x}\), we apply the chain rule:
\[\Large \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial \hat{x}} \frac{\partial \hat{x}}{\partial x}\]</p>
<h3 id="gradient-components"><a class="header" href="#gradient-components">Gradient components</a></h3>
<h4 id="output-to-normalized-value"><a class="header" href="#output-to-normalized-value">Output to normalized value</a></h4>
<ul>
<li>\(\frac{\partial y}{\partial \hat{x}} = \gamma\) (element-wise multiplication)</li>
</ul>
<h4 id="normalized-value-to-input"><a class="header" href="#normalized-value-to-input">Normalized value to input</a></h4>
<p>The gradient \(\frac{\partial \hat{x}}{\partial x}\) has three components:</p>
<ul>
<li>Direct effect through numerator: \(\frac{1}{\sqrt{\sigma^2 + \epsilon}}\)</li>
<li>Indirect effect through mean: \(-\frac{1}{H} \frac{1}{\sqrt{\sigma^2 + \epsilon}}\)</li>
<li>Indirect effect through variance: \(-\frac{(x - \mu)}{H(\sigma^2 + \epsilon)^{3/2}} (x - \mu)\)</li>
</ul>
<h3 id="combining-terms"><a class="header" href="#combining-terms">Combining terms</a></h3>
<p>The gradient through the normalization term can be simplified to:
\[\Large \frac{\partial \hat{x}}{\partial x} = \frac{1}{\sqrt{\sigma^2 + \epsilon}} (1 - \frac{1}{H} - \frac{(x - \mu)^2}{H(\sigma^2 + \epsilon)})\]</p>
<h3 id="final-gradient-expression"><a class="header" href="#final-gradient-expression">Final gradient expression</a></h3>
<p>Combining all terms:
\[\Large \frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \odot \gamma \odot \frac{1}{\sqrt{\sigma^2 + \epsilon}} (1 - \frac{1}{H} - \frac{(x - \mu)^2}{H(\sigma^2 + \epsilon)})\]</p>
<h3 id="key-insights"><a class="header" href="#key-insights">Key insights</a></h3>
<ul>
<li>The chain rule accounts for all paths through which x affects the output</li>
<li>The normalization term \(\sqrt{\sigma^2 + \epsilon}\) appears in both numerator and denominator</li>
<li>The mean and variance terms create additional paths for gradient flow</li>
<li>The final expression combines all effects into a single efficient computation</li>
</ul>
<h3 id="implementation-considerations"><a class="header" href="#implementation-considerations">Implementation considerations</a></h3>
<ul>
<li>The gradient properly accounts for the scaling effect of \(\gamma\)</li>
<li>The normalization effect of mean and variance is preserved</li>
<li>The numerical stability term \(\epsilon\) is maintained</li>
<li>Gradients are properly scaled across the hidden dimension H</li>
<li>The computation order matches the forward pass for numerical stability</li>
</ul>
<p>This derivation ensures that the backward pass maintains the same numerical properties as the forward pass while efficiently computing all necessary gradients.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_20/forward_pass.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../puzzle_21/puzzle_21.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_20/forward_pass.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../puzzle_21/puzzle_21.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
