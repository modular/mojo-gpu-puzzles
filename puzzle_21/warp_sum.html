<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>warp.sum() Essentials - Mojo ðŸ”¥ GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta property="og:image" content="..//puzzles_images/puzzle-mark.svg">
        <meta property="og:url" content="https://builds.modular.com/puzzles">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta name="twitter:image" content="..//puzzles_images/puzzle-mark.svg">
        <link rel="icon" type="image/png" href="..//puzzles_images/puzzle-mark.svg">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="-warpsum-essentials---warp-level-dot-product"><a class="header" href="#-warpsum-essentials---warp-level-dot-product">âš¡ warp.sum() Essentials - Warp-Level Dot Product</a></h1>
<p>Implement the dot product we saw in <a href="../puzzle_10/puzzle_10.html">puzzle 10</a> using Mojoâ€™s warp operations to replace complex shared memory patterns with simple function calls. Each warp lane will process one element and use <code>warp.sum()</code> to combine results automatically, demonstrating how warp programming transforms GPU synchronization.</p>
<p><strong>Key insight:</strong> <em>The <a href="https://docs.modular.com/mojo/stdlib/gpu/warp/sum">warp.sum()</a> operation leverages SIMT execution to replace shared memory + barriers + tree reduction with a single hardware-accelerated instruction.</em></p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<p>In this puzzle, youâ€™ll master:</p>
<ul>
<li><strong>Warp-level reductions</strong> with <code>warp.sum()</code></li>
<li><strong>SIMT execution model</strong> and lane synchronization</li>
<li><strong>Cross-architecture compatibility</strong> with <code>WARP_SIZE</code></li>
<li><strong>Performance transformation</strong> from complex to simple patterns</li>
<li><strong>Lane ID management</strong> and conditional writes</li>
</ul>
<p>The mathematical operation is a dot product (inner product):
\[\Large \text{out}[0] = \sum_{i=0}^{N-1} a[i] \times b[i]\]</p>
<p>But the implementation teaches fundamental patterns for all warp-level GPU programming in Mojo.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li>Vector size: <code>SIZE = WARP_SIZE</code> (32 or 64 depending on GPU architecture)</li>
<li>Data type: <code>DType.float32</code></li>
<li>Block configuration: <code>(WARP_SIZE, 1)</code> threads per block</li>
<li>Grid configuration: <code>(1, 1)</code> blocks per grid</li>
<li>Layout: <code>Layout.row_major(SIZE)</code> (1D row-major)</li>
</ul>
<h2 id="the-traditional-complexity-from-puzzle-10"><a class="header" href="#the-traditional-complexity-from-puzzle-10">The traditional complexity (from Puzzle 10)</a></h2>
<p>Recall the complex approach from <code>p10.mojo</code> that required shared memory, barriers, and tree reduction:</p>
<pre><code class="language-mojo">alias SIZE = WARP_SIZE
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (WARP_SIZE, 1)  # optimal choice for warp kernel
alias dtype = DType.float32
alias SIMD_WIDTH = simdwidthof[dtype]()
alias in_layout = Layout.row_major(SIZE)
alias out_layout = Layout.row_major(1)


fn traditional_dot_product_p10_style[
    in_layout: Layout, out_layout: Layout, size: Int
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    """
    This is the complex approach from p10_layout_tensor.mojo - kept for comparison.
    """
    shared = tb[dtype]().row_major[WARP_SIZE]().shared().alloc()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    if global_i &lt; size:
        shared[local_i] = (a[global_i] * b[global_i]).reduce_add()
    else:
        shared[local_i] = 0.0

    barrier()

    stride = SIZE // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            shared[local_i] += shared[local_i + stride]
        barrier()
        stride //= 2

    if local_i == 0:
        out[0] = shared[0]


</code></pre>
<p><strong>What makes this complex:</strong></p>
<ul>
<li><strong>Shared memory allocation</strong>: Manual memory management within blocks</li>
<li><strong>Explicit barriers</strong>: <code>barrier()</code> calls to synchronize threads</li>
<li><strong>Tree reduction</strong>: Complex loop with stride-based indexing</li>
<li><strong>Conditional writes</strong>: Only thread 0 writes the final result</li>
</ul>
<p>This works, but itâ€™s verbose, error-prone, and requires deep understanding of GPU synchronization.</p>
<p><strong>Test the traditional approach:</strong></p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p21 --traditional
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p21 --traditional
</code></pre>
  </div>
</div>
<h2 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h2>
<h3 id="1-simple-warp-kernel-approach"><a class="header" href="#1-simple-warp-kernel-approach">1. Simple warp kernel approach</a></h3>
<p>Transform the complex traditional approach into a simple warp kernel using <code>warp_sum()</code>:</p>
<pre><code class="language-mojo">from gpu.warp import sum as warp_sum


fn simple_warp_dot_product[
    in_layout: Layout, out_layout: Layout, size: Int
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    # FILL IN (6 lines at most)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p21/p21.mojo" class="filename">View full file: problems/p21/p21.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<h3 id="1-understanding-the-simple-warp-kernel-structure"><a class="header" href="#1-understanding-the-simple-warp-kernel-structure">1. <strong>Understanding the simple warp kernel structure</strong></a></h3>
<p>You need to complete the <code>simple_warp_dot_product</code> function with <strong>6 lines or fewer</strong>:</p>
<pre><code class="language-mojo">fn simple_warp_dot_product[...](out, a, b):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    # FILL IN (6 lines at most)
</code></pre>
<p><strong>Pattern to follow:</strong></p>
<ol>
<li>Compute partial product for this threadâ€™s element</li>
<li>Use <code>warp_sum()</code> to combine across all warp lanes</li>
<li>Lane 0 writes the final result</li>
</ol>
<h3 id="2-computing-partial-products"><a class="header" href="#2-computing-partial-products">2. <strong>Computing partial products</strong></a></h3>
<pre><code class="language-mojo">var partial_product: Scalar[dtype] = 0
if global_i &lt; size:
    partial_product = (a[global_i] * b[global_i]).reduce_add()
</code></pre>
<p><strong>Why <code>.reduce_add()</code>?</strong> Values in Mojo are SIMD-based, so <code>a[global_i] * b[global_i]</code> returns a SIMD vector. Use <code>.reduce_add()</code> to sum the vector into a scalar.</p>
<p><strong>Bounds checking:</strong> Essential because not all threads may have valid data to process.</p>
<h3 id="3-warp-reduction-magic"><a class="header" href="#3-warp-reduction-magic">3. <strong>Warp reduction magic</strong></a></h3>
<pre><code class="language-mojo">total = warp_sum(partial_product)
</code></pre>
<p><strong>What <code>warp_sum()</code> does:</strong></p>
<ul>
<li>Takes each laneâ€™s <code>partial_product</code> value</li>
<li>Sums them across all lanes in the warp (hardware-accelerated)</li>
<li>Returns the same total to <strong>all lanes</strong> (not just lane 0)</li>
<li>Requires <strong>zero explicit synchronization</strong> (SIMT handles it)</li>
</ul>
<h3 id="4-writing-the-result"><a class="header" href="#4-writing-the-result">4. <strong>Writing the result</strong></a></h3>
<pre><code class="language-mojo">if lane_id() == 0:
    out[0] = total
</code></pre>
<p><strong>Why only lane 0?</strong> All lanes have the same <code>total</code> value after <code>warp_sum()</code>, but we only want to write once to avoid race conditions.</p>
<p><strong><code>lane_id()</code>:</strong> Returns 0-31 (NVIDIA) or 0-63 (AMD) - identifies which lane within the warp.</p>
</div>
</details>
<p><strong>Test the simple warp kernel:</strong></p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p21 --kernel
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p21 --kernel
</code></pre>
  </div>
</div>
<p>Expected output when solved:</p>
<pre><code class="language-txt">SIZE: 32
WARP_SIZE: 32
SIMD_WIDTH: 8
=== RESULT ===
out: 10416.0
expected: 10416.0
ðŸš€ Notice how simple the warp version is compared to p10.mojo!
   Same kernel structure, but warp_sum() replaces all the complexity!
</code></pre>
<h3 id="solution"><a class="header" href="#solution">Solution</a></h3>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn simple_warp_dot_product[
    in_layout: Layout, out_layout: Layout, size: Int
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x

    # Each thread computes one partial product using vectorized approach as values in Mojo are SIMD based
    var partial_product: Scalar[dtype] = 0
    if global_i &lt; size:
        partial_product = (a[global_i] * b[global_i]).reduce_add()

    # warp_sum() replaces all the shared memory + barriers + tree reduction
    total = warp_sum(partial_product)

    # Only lane 0 writes the result (all lanes have the same total)
    if lane_id() == 0:
        out[0] = total


</code></pre>
<div class="solution-explanation">
<p>The simple warp kernel demonstrates the fundamental transformation from complex synchronization to hardware-accelerated primitives:</p>
<p><strong>What disappeared from the traditional approach:</strong></p>
<ul>
<li><strong>15+ lines â†’ 6 lines</strong>: Dramatic code reduction</li>
<li><strong>Shared memory allocation</strong>: Zero memory management required</li>
<li><strong>3+ barrier() calls</strong>: Zero explicit synchronization</li>
<li><strong>Complex tree reduction</strong>: Single function call</li>
<li><strong>Stride-based indexing</strong>: Eliminated entirely</li>
</ul>
<p><strong>SIMT execution model:</strong></p>
<pre><code>Warp lanes (SIMT execution):
Lane 0: partial_product = a[0] * b[0]    = 0.0
Lane 1: partial_product = a[1] * b[1]    = 4.0
Lane 2: partial_product = a[2] * b[2]    = 16.0
...
Lane 31: partial_product = a[31] * b[31] = 3844.0

warp_sum() hardware operation:
All lanes â†’ 0.0 + 4.0 + 16.0 + ... + 3844.0 = 10416.0
All lanes receive â†’ total = 10416.0 (broadcast result)
</code></pre>
<p><strong>Why this works without barriers:</strong></p>
<ol>
<li><strong>SIMT execution</strong>: All lanes execute each instruction simultaneously</li>
<li><strong>Hardware synchronization</strong>: When <code>warp_sum()</code> begins, all lanes have computed their <code>partial_product</code></li>
<li><strong>Built-in communication</strong>: GPU hardware handles the reduction operation</li>
<li><strong>Broadcast result</strong>: All lanes receive the same <code>total</code> value</li>
</ol>
</div>
</details>
<h3 id="2-functional-approach"><a class="header" href="#2-functional-approach">2. Functional approach</a></h3>
<p>Now implement the same warp dot product using Mojoâ€™s functional programming patterns:</p>
<pre><code class="language-mojo">fn functional_warp_dot_product[
    layout: Layout, dtype: DType, simd_width: Int, rank: Int, size: Int
](
    out: LayoutTensor[mut=True, dtype, Layout.row_major(1), MutableAnyOrigin],
    a: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],
    b: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],
    ctx: DeviceContext,
) raises:
    @parameter
    @always_inline
    fn compute_dot_product[
        simd_width: Int, rank: Int
    ](indices: IndexList[rank]) capturing -&gt; None:
        idx = indices[0]
        print("idx:", idx)
        # FILL IN (10 lines at most)

    # Launch exactly WARP_SIZE threads (one warp) to process all elements
    elementwise[compute_dot_product, 1, target="gpu"](WARP_SIZE, ctx)


</code></pre>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<h3 id="1-understanding-the-functional-approach-structure"><a class="header" href="#1-understanding-the-functional-approach-structure">1. <strong>Understanding the functional approach structure</strong></a></h3>
<p>You need to complete the <code>compute_dot_product</code> function with <strong>10 lines or fewer</strong>:</p>
<pre><code class="language-mojo">@parameter
@always_inline
fn compute_dot_product[simd_width: Int, rank: Int](indices: IndexList[rank]) capturing -&gt; None:
    idx = indices[0]
    # FILL IN (10 lines at most)
</code></pre>
<p><strong>Functional pattern differences:</strong></p>
<ul>
<li>Uses <code>elementwise</code> to launch exactly <code>WARP_SIZE</code> threads</li>
<li>Each thread processes one element based on <code>idx</code></li>
<li>Same warp operations, different launch mechanism</li>
</ul>
<h3 id="2-computing-partial-products-1"><a class="header" href="#2-computing-partial-products-1">2. <strong>Computing partial products</strong></a></h3>
<pre><code class="language-mojo">var partial_product: Scalar[dtype] = 0.0
if idx &lt; size:
    a_val = a.load[1](idx, 0)
    b_val = b.load[1](idx, 0)
    partial_product = (a_val * b_val).reduce_add()
else:
    partial_product = 0.0
</code></pre>
<p><strong>Loading pattern:</strong> <code>a.load[1](idx, 0)</code> loads exactly 1 element at position <code>idx</code> (not SIMD vectorized).</p>
<p><strong>Bounds handling:</strong> Set <code>partial_product = 0.0</code> for out-of-bounds threads so they donâ€™t contribute to the sum.</p>
<h3 id="3-warp-operations-and-storing"><a class="header" href="#3-warp-operations-and-storing">3. <strong>Warp operations and storing</strong></a></h3>
<pre><code class="language-mojo">total = warp_sum(partial_product)

if lane_id() == 0:
    out.store[1](0, 0, total)
</code></pre>
<p><strong>Storage pattern:</strong> <code>out.store[1](0, 0, total)</code> stores 1 element at position (0, 0) in the output tensor.</p>
<p><strong>Same warp logic:</strong> <code>warp_sum()</code> and lane 0 writing work identically in functional approach.</p>
<h3 id="4-available-functions-from-imports"><a class="header" href="#4-available-functions-from-imports">4. <strong>Available functions from imports</strong></a></h3>
<pre><code class="language-mojo">from gpu import lane_id
from gpu.warp import sum as warp_sum, WARP_SIZE

# Inside your function:
my_lane = lane_id()           # 0 to WARP_SIZE-1
total = warp_sum(my_value)    # Hardware-accelerated reduction
warp_size = WARP_SIZE         # 32 (NVIDIA) or 64 (AMD)
</code></pre>
</div>
</details>
<p><strong>Test the functional approach:</strong></p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p21 --functional
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p21 --functional
</code></pre>
  </div>
</div>
<p>Expected output when solved:</p>
<pre><code class="language-txt">SIZE: 32
WARP_SIZE: 32
SIMD_WIDTH: 8
=== RESULT ===
out: 10416.0
expected: 10416.0
ðŸ”§ Functional approach shows modern Mojo style with warp operations!
   Clean, composable, and still leverages warp hardware primitives!
</code></pre>
<h3 id="solution-1"><a class="header" href="#solution-1">Solution</a></h3>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn functional_warp_dot_product[
    layout: Layout, dtype: DType, simd_width: Int, rank: Int, size: Int
](
    out: LayoutTensor[mut=True, dtype, Layout.row_major(1), MutableAnyOrigin],
    a: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],
    b: LayoutTensor[mut=False, dtype, layout, MutableAnyOrigin],
    ctx: DeviceContext,
) raises:
    @parameter
    @always_inline
    fn compute_dot_product[
        simd_width: Int, rank: Int
    ](indices: IndexList[rank]) capturing -&gt; None:
        idx = indices[0]

        # Each thread computes one partial product
        var partial_product: Scalar[dtype] = 0.0
        if idx &lt; size:
            a_val = a.load[1](idx, 0)
            b_val = b.load[1](idx, 0)
            partial_product = (a_val * b_val).reduce_add()
        else:
            partial_product = 0.0

        # Warp magic - combines all WARP_SIZE partial products!
        total = warp_sum(partial_product)

        # Only lane 0 writes the result (all lanes have the same total)
        if lane_id() == 0:
            out.store[1](0, 0, total)

    # Launch exactly WARP_SIZE threads (one warp) to process all elements
    elementwise[compute_dot_product, 1, target="gpu"](WARP_SIZE, ctx)


</code></pre>
<div class="solution-explanation">
<p>The functional warp approach showcases modern Mojo programming patterns with warp operations:</p>
<p><strong>Functional approach characteristics:</strong></p>
<pre><code class="language-mojo">elementwise[compute_dot_product, 1, target="gpu"](WARP_SIZE, ctx)
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Type safety</strong>: Compile-time tensor layout checking</li>
<li><strong>Composability</strong>: Easy integration with other functional operations</li>
<li><strong>Modern patterns</strong>: Leverages Mojoâ€™s functional programming features</li>
<li><strong>Automatic optimization</strong>: Compiler can apply high-level optimizations</li>
</ul>
<p><strong>Key differences from kernel approach:</strong></p>
<ul>
<li><strong>Launch mechanism</strong>: Uses <code>elementwise</code> instead of <code>enqueue_function</code></li>
<li><strong>Memory access</strong>: Uses <code>.load[1]()</code> and <code>.store[1]()</code> patterns</li>
<li><strong>Integration</strong>: Seamlessly works with other functional operations</li>
</ul>
<p><strong>Same warp benefits:</strong></p>
<ul>
<li><strong>Zero synchronization</strong>: <code>warp_sum()</code> works identically</li>
<li><strong>Hardware acceleration</strong>: Same performance as kernel approach</li>
<li><strong>Cross-architecture</strong>: <code>WARP_SIZE</code> adapts automatically</li>
</ul>
</div>
</details>
<h2 id="performance-comparison-with-benchmarks"><a class="header" href="#performance-comparison-with-benchmarks">Performance comparison with benchmarks</a></h2>
<p>Run comprehensive benchmarks to see how warp operations scale:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p21 --benchmark
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p21 --benchmark
</code></pre>
  </div>
</div>
<p>Hereâ€™s example output from a complete benchmark run:</p>
<pre><code>SIZE: 32
WARP_SIZE: 32
SIMD_WIDTH: 8
--------------------------------------------------------------------------------
Testing SIZE=1 x WARP_SIZE, BLOCKS=1
Running traditional_1x
Running simple_warp_1x
Running functional_warp_1x
--------------------------------------------------------------------------------
Testing SIZE=4 x WARP_SIZE, BLOCKS=4
Running traditional_4x
Running simple_warp_4x
Running functional_warp_4x
--------------------------------------------------------------------------------
Testing SIZE=32 x WARP_SIZE, BLOCKS=32
Running traditional_32x
Running simple_warp_32x
Running functional_warp_32x
--------------------------------------------------------------------------------
Testing SIZE=256 x WARP_SIZE, BLOCKS=256
Running traditional_256x
Running simple_warp_256x
Running functional_warp_256x
--------------------------------------------------------------------------------
Testing SIZE=2048 x WARP_SIZE, BLOCKS=2048
Running traditional_2048x
Running simple_warp_2048x
Running functional_warp_2048x
--------------------------------------------------------------------------------
Testing SIZE=16384 x WARP_SIZE, BLOCKS=16384 (Large Scale)
Running traditional_16384x
Running simple_warp_16384x
Running functional_warp_16384x
--------------------------------------------------------------------------------
Testing SIZE=65536 x WARP_SIZE, BLOCKS=65536 (Massive Scale)
Running traditional_65536x
Running simple_warp_65536x
Running functional_warp_65536x
-------------------------------------------------------
| name                   | met (ms)           | iters |
-------------------------------------------------------
| traditional_1x         | 3.5565388994708993 | 378   |
| simple_warp_1x         | 3.1609036200000005 | 100   |
| functional_warp_1x     | 3.22122741         | 100   |
| traditional_4x         | 3.1741644200000003 | 100   |
| simple_warp_4x         | 4.6268518          | 100   |
| functional_warp_4x     | 3.18364685         | 100   |
| traditional_32x        | 3.19311859         | 100   |
| simple_warp_32x        | 3.18385162         | 100   |
| functional_warp_32x    | 3.18260223         | 100   |
| traditional_256x       | 4.704542839999999  | 100   |
| simple_warp_256x       | 3.599057930294906  | 373   |
| functional_warp_256x   | 3.21388549         | 100   |
| traditional_2048x      | 3.31929595         | 100   |
| simple_warp_2048x      | 4.80178161         | 100   |
| functional_warp_2048x  | 3.734744261111111  | 360   |
| traditional_16384x     | 6.39709167         | 100   |
| simple_warp_16384x     | 7.8748059          | 100   |
| functional_warp_16384x | 7.848806150000001  | 100   |
| traditional_65536x     | 25.155625274509806 | 51    |
| simple_warp_65536x     | 25.10668252830189  | 53    |
| functional_warp_65536x | 25.053512849056602 | 53    |
-------------------------------------------------------

Benchmarks completed!

ðŸš€ WARP OPERATIONS PERFORMANCE ANALYSIS:
   GPU Architecture: NVIDIA (WARP_SIZE=32) vs AMD (WARP_SIZE=64)
   - 1 x WARP_SIZE: Single warp baseline
   - 4 x WARP_SIZE: Few warps, warp overhead visible
   - 32 x WARP_SIZE: Medium scale, warp benefits emerge
   - 256 x WARP_SIZE: Large scale, dramatic warp advantages
   - 2048 x WARP_SIZE: Massive scale, warp operations dominate
   - 16384 x WARP_SIZE: Large scale (512K-1M elements)
   - 65536 x WARP_SIZE: Massive scale (2M-4M elements)
   - Note: AMD GPUs process 2 x elements per warp vs NVIDIA!

   Expected Results at Large Scales:
   â€¢ Traditional: Slower due to more barrier overhead
   â€¢ Warp operations: Faster, scale better with problem size
   â€¢ Memory bandwidth becomes the limiting factor
</code></pre>
<p><strong>Performance insights from this example:</strong></p>
<ul>
<li><strong>Small scales (1x-4x)</strong>: Warp operations show modest improvements (~10-15% faster)</li>
<li><strong>Medium scale (32x-256x)</strong>: Functional approach often performs best</li>
<li><strong>Large scales (16K-65K)</strong>: All approaches converge as memory bandwidth dominates</li>
<li><strong>Variability</strong>: Performance depends heavily on specific GPU architecture and memory subsystem</li>
</ul>
<p><strong>Note:</strong> Your results will vary significantly depending on your hardware (GPU model, memory bandwidth, <code>WARP_SIZE</code>). The key insight is observing the relative performance trends rather than absolute timings.</p>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Once youâ€™ve mastered warp sum operations, youâ€™re ready for:</p>
<ul>
<li><strong><a href="./warp_extra.html">ðŸ“Š When to Use Warp Programming</a></strong>: Strategic decision framework for warp vs traditional approaches</li>
<li><strong>Advanced warp operations</strong>: <code>shuffle_idx()</code>, <code>shuffle_down()</code>, <code>prefix_sum()</code> for complex communication patterns</li>
<li><strong>Multi-warp algorithms</strong>: Combining warp operations with block-level synchronization</li>
<li><strong>Part VII: Memory Coalescing</strong>: Optimizing memory access patterns for maximum bandwidth</li>
</ul>
<p>ðŸ’¡ <strong>Key Takeaway</strong>: Warp operations transform GPU programming by replacing complex synchronization patterns with hardware-accelerated primitives, demonstrating how understanding the execution model enables dramatic simplification without sacrificing performance.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_21/warp_simt.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../puzzle_21/warp_extra.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_21/warp_simt.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../puzzle_21/warp_extra.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
