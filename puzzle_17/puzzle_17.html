<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Puzzle 17: Attention Op - Mojo ðŸ”¥ GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta property="og:image" content="..//puzzles_images/puzzle-mark.svg">
        <meta property="og:url" content="https://builds.modular.com/puzzles">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta name="twitter:image" content="..//puzzles_images/puzzle-mark.svg">
        <link rel="icon" type="image/png" href="..//puzzles_images/puzzle-mark.svg">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="puzzle-17-attention-op"><a class="header" href="#puzzle-17-attention-op">Puzzle 17: Attention Op</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>In this puzzle, weâ€™ll implement the attention mechanism as a custom MAX Graph operation. Attention is a fundamental building block of modern neural networks, poplularized particularly <a href="https://arxiv.org/abs/1706.03762">transformers</a>, that allows models to focus on relevant parts of the input when making predictions.</p>
<p>Mathematically, the attention function is defined as:</p>
<p>$$\Large \text{Attention}(Q, K, V) = \text{softmax}(Q \cdot K^T) \cdot V$$</p>
<p>Where:</p>
<ul>
<li>\(Q\) is the <strong>query vector</strong> of shape \((d,)\) - represents what weâ€™re looking for</li>
<li>\(K\) is the <strong>key matrix</strong> of shape \((\text{seq_len}, d)\) - represents whatâ€™s available to match against</li>
<li>\(V\) is the <strong>value matrix</strong> of shape \((\text{seq_len}, d)\) - represents the information to retrieve</li>
<li>The output is a <strong>weighted combination</strong> vector of shape \((d,)\)</li>
</ul>
<p>The computation involves three main steps:</p>
<ol>
<li><strong>Attention Scores</strong>: Compute \(Q \cdot K^T\) to measure how well the query matches each key vector</li>
<li><strong>Attention Weights</strong>: Apply softmax to convert scores into a probability distribution (weights sum to 1)</li>
<li><strong>Weighted Sum</strong>: Combine value vectors using attention weights to produce the final output</li>
</ol>
<h2 id="understanding-attention-a-step-by-step-breakdown"><a class="header" href="#understanding-attention-a-step-by-step-breakdown">Understanding attention: a step-by-step breakdown</a></h2>
<p>Think of attention as a <strong>smart lookup mechanism</strong>. Given a query (what youâ€™re looking for), attention finds the most relevant information from a collection of key-value pairs:</p>
<ol>
<li>
<p><strong>Step 1 - Similarity Matching</strong>: Compare your query \(Q\) against all keys \(K\) to get similarity scores</p>
<ul>
<li>Compute \(Q \cdot K^T\) where each score measures how well \(Q\) matches each key vector</li>
<li>Higher scores = better matches</li>
</ul>
</li>
<li>
<p><strong>Step 2 - Probability Distribution</strong>: Convert raw scores into normalized weights</p>
<ul>
<li>Apply softmax to ensure all weights sum to 1.0</li>
<li>This creates a probability distribution over which values to focus on</li>
</ul>
</li>
<li>
<p><strong>Step 3 - Weighted Retrieval</strong>: Combine values using the attention weights</p>
<ul>
<li>Multiply each value vector by its corresponding weight</li>
<li>Sum everything up to get the final output</li>
</ul>
</li>
</ol>
<p><strong>Real-world analogy</strong>: Imagine searching a library. Your query is what you want to find, the book titles are keys, and the book contents are values. Attention computes how relevant each book is to your query, then gives you a summary weighted by relevance.</p>
<h3 id="visual-computation-flow"><a class="header" href="#visual-computation-flow">Visual computation flow</a></h3>
<pre><code>Input:  Q(16,)    K(16,16)    V(16,16)
         â†“           â†“           â†“
Step 1: Q(1,16) @ K^T(16,16) â†’ Scores(1,16)
         â†“
Step 2: softmax(Scores) â†’ Weights(1,16)  [sum = 1.0]
         â†“
Step 3: Weights(1,16) @ V(16,16) â†’ Output(1,16) â†’ reshape â†’ Output(16,)
</code></pre>
<p><strong>Key insight</strong>: We reshape the query vector \(Q\) from shape \((16,)\) to \((1,16)\) so we can use matrix multiplication instead of manual dot products. This allows us to leverage the highly optimized tiled matmul kernel from Puzzle 14!</p>
<p>Our GPU implementation <strong>reuses and combines optimized kernels from previous puzzles</strong>:</p>
<ul>
<li><strong><a href="../puzzle_14/puzzle_14.html">Tiled matrix multiplication from Puzzle 14</a></strong> for efficient \(Q \cdot K^T\) and \(\text{weights} \cdot V\) operations</li>
<li><strong>Shared memory transpose</strong> for computing \(K^T\) efficiently</li>
<li><strong><a href="../puzzle_16/puzzle_16.html">Parallel softmax from Puzzle 16</a></strong> for numerically stable attention weight computation</li>
</ul>
<blockquote>
<p><strong>ðŸ”„ Kernel Reuse Strategy</strong>: This puzzle demonstrates how to build complex operations by combining proven, optimized kernels from previous puzzles. Rather than writing everything from scratch, we leverage the <code>matmul_idiomatic_tiled</code> from Puzzle 14 and <code>softmax_kernel</code> from Puzzle 16, showcasing the power of modular GPU kernel design.</p>
</blockquote>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<ul>
<li>Vector attention mechanism for sequence processing</li>
<li><strong>Kernel reuse</strong>: Leveraging proven implementations from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a> and <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a></li>
<li>Efficient matrix multiplication using shared memory tiling</li>
<li>Memory-optimized tensor reshaping to minimize buffer allocation</li>
<li>Integration of multiple optimized kernels into a single operation</li>
<li>Custom MAX Graph operation with multi-input support</li>
<li>CPU fallback implementation for compatibility</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li><strong>Sequence length</strong>: \(\text{SEQ_LEN} = 16\) - number of key/value vectors in our sequence</li>
<li><strong>Model dimension</strong>: \(\text{D} = 16\) - dimensionality of each vector (query, keys, values)</li>
<li><strong>Threads per block</strong>: \(\text{TPB} = 16\) - matches SEQ_LEN for optimal softmax performance</li>
<li><strong>Grid dimensions</strong>: Computed dynamically to handle different matrix sizes efficiently</li>
<li><strong>Shared memory</strong>: Utilized in transpose, matmul, and softmax kernels for performance</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Query tensor: <code>Layout.row_major(d)</code></li>
<li>Key tensor: <code>Layout.row_major(seq_len, d)</code></li>
<li>Value tensor: <code>Layout.row_major(seq_len, d)</code></li>
<li>Output tensor: <code>Layout.row_major(d)</code></li>
<li>Custom op parameters: <code>{"seq_len": seq_len, "d": d, "dtype": dtype}</code></li>
</ul>
<p>Key aspects of this puzzle include:</p>
<ol>
<li><strong>Multi-kernel orchestration</strong>: Combining transpose, matmul, and softmax operations</li>
<li><strong>Memory optimization</strong>: Using reshape operations and buffer reuse to minimize allocations</li>
<li><strong>Numerical stability</strong>: Leveraging the proven softmax implementation from <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a></li>
<li><strong>Performance optimization</strong>: Using tiled algorithms from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a> for all matrix operations</li>
<li><strong>Multi-input operations</strong>: Handling three input tensors (Q, K, V) in a single custom op</li>
</ol>
<p>Our attention custom operation will:</p>
<ul>
<li>Accept query, key, and value tensors from Python</li>
<li>Process them efficiently on GPU using optimized kernels</li>
<li>Return the attention-weighted output vector</li>
<li>Match the results of NumPy reference implementation</li>
</ul>
<h2 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h2>
<p>To complete this puzzle, weâ€™ll leverage the tiled matmul kernel from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a> and the softmax kernel from <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a>. You only need to implement the transpose kernel in the Mojo file using shared memory.</p>
<h3 id="1-implement-the-transpose-kernel"><a class="header" href="#1-implement-the-transpose-kernel">1. Implement the transpose kernel</a></h3>
<pre><code class="language-mojo">fn transpose_kernel[
    layout_in: Layout,  # Layout for input matrix (seq_len, d)
    layout_out: Layout,  # Layout for output matrix (d, seq_len)
    rows: Int,
    cols: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, layout_out, MutableAnyOrigin],
    inp: LayoutTensor[mut=False, dtype, layout_in, MutableAnyOrigin],
):
    # FILL ME IN (roughly 18 lines)
    ...


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p17/op/attention.mojo" class="filename">View full file: problems/p17/op/attention.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<p><strong>Transpose Kernel Implementation Guide:</strong></p>
<ol>
<li>
<p><strong>Shared Memory Setup</strong>: Use <code>tb[dtype]().row_major[TPB, TPB]().shared().alloc()</code> to create a TPBÃ—TPB shared memory tile for efficient data exchange between threads</p>
</li>
<li>
<p><strong>Thread Indexing</strong>: Map threads to matrix elements:</p>
<ul>
<li><code>local_row = thread_idx.y</code>, <code>local_col = thread_idx.x</code> (position within the block)</li>
<li><code>global_row = block_idx.y * TPB + local_row</code> (position in the full matrix)</li>
</ul>
</li>
<li>
<p><strong>Two-Phase Operation</strong>:</p>
<ul>
<li><strong>Phase 1</strong>: Load data from global memory into shared memory with normal indexing</li>
<li><strong>Phase 2</strong>: Store data from shared memory to global memory with swapped indexing</li>
</ul>
</li>
<li>
<p><strong>Critical Synchronization</strong>: Call <code>barrier()</code> between loading and storing to ensure all threads have finished loading before any thread starts storing</p>
</li>
<li>
<p><strong>Transpose Magic</strong>: The transpose happens through swapped indexing: <code>shared_tile[local_col, local_row]</code> instead of <code>shared_tile[local_row, local_col]</code></p>
</li>
<li>
<p><strong>Boundary Handling</strong>: Check bounds when accessing global memory to avoid out-of-bounds reads/writes for matrices that donâ€™t perfectly divide by TPB</p>
</li>
<li>
<p><strong>Memory Coalescing</strong>: This pattern ensures both reads and writes are coalesced for optimal memory bandwidth utilization</p>
</li>
</ol>
</div>
</details>
<h3 id="2-orchestrate-the-attention"><a class="header" href="#2-orchestrate-the-attention">2. Orchestrate the attention</a></h3>
<pre><code class="language-mojo">            var gpu_ctx = rebind[DeviceContext](ctx[])

            # Define layouts for matrix multiplication
            # Q reshaped to (1, d)
            alias layout_q_2d = Layout.row_major(1, d)
            # K^T is (d, seq_len)
            alias layout_k_t = Layout.row_major(d, seq_len)
            # Scores as (1, seq_len)
            alias layout_scores_2d = Layout.row_major(1, seq_len)
            # Weights as (1, seq_len)
            alias layout_weights_2d = Layout.row_major(1, seq_len)
            # Result as (1, d)
            alias layout_result_2d = Layout.row_major(1, d)

            alias scores_blocks_per_grid = (
                (seq_len + TPB - 1) // TPB,
                (1 + TPB - 1) // TPB,
            )
            alias result_blocks_per_grid = (
                (d + TPB - 1) // TPB,
                (1 + TPB - 1) // TPB,
            )
            alias matmul_threads_per_block = (TPB, TPB)
            alias transpose_blocks_per_grid = (
                (seq_len + TPB - 1) // TPB,
                (d + TPB - 1) // TPB,
            )

            # Allocate minimal temporary buffers - reuse same buffer for different shapes
            k_t_buf = gpu_ctx.enqueue_create_buffer[dtype](
                seq_len * d
            )  # K^T as (d, seq_len)
            scores_weights_buf = gpu_ctx.enqueue_create_buffer[dtype](
                seq_len
            )  # Reused for scores and weights

            k_t = LayoutTensor[mut=True, dtype, layout_k_t, MutableAnyOrigin](
                k_t_buf.unsafe_ptr()
            )

            # Step 1: Reshape Q from (d,) to (1, d) - no buffer needed
            # FILL ME IN 1 line

            # Step 2: Transpose K from (seq_len, d) to K^T (d, seq_len)
            # FILL ME IN 1 function call

            # Step 3: Compute attention scores using matmul: Q @ K^T = (1, d) @ (d, seq_len) -&gt; (1, seq_len)
            # GPU: Uses matrix multiplication to compute all Q Â· K[i] scores in parallel
            # Reuse scores_weights_buf as (1, seq_len) for scores
            # FILL ME IN 2 lines

            # Step 4: Reshape scores from (1, seq_len) to (seq_len,) for softmax
            # FILL ME IN 1 line

            # Step 5: Apply softmax to get attention weights
            # FILL ME IN 1 function call

            # Step 6: Reshape weights from (seq_len,) to (1, seq_len) for final matmul
            # FILL ME IN 1 line

            # Step 7: Compute final result using matmul: weights @ V = (1, seq_len) @ (seq_len, d) -&gt; (1, d)
            # Reuse out_tensor reshaped as (1, d) for result
            # FILL ME IN 2 lines

</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p17/op/attention.mojo" class="filename">View full file: problems/p17/op/attention.mojo</a></p>
<h3 id="test-the-kernels"><a class="header" href="#test-the-kernels">Test the kernels</a></h3>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p17
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p17
</code></pre>
  </div>
</div>
<p>When successful, you should see output similar to on CPU and GPU:</p>
<pre><code>Input shapes: Q=(16,), K=(16, 16), V=(16, 16)
Sample Q values: [ 0.04967142 -0.01382643  0.06476886  0.15230298 -0.02341534]
Sample K[0] values: [-0.10128311  0.03142473 -0.09080241 -0.14123037  0.14656489]
Sample V[0] values: [ 0.11631638  0.00102331 -0.09815087  0.04621035  0.01990597]

================================================================================
STEP-BY-STEP VECTOR ATTENTION COMPUTATION DEBUG
================================================================================

1. INPUT SHAPES:
   Q shape: (16,) (query vector)
   K shape: (16, 16) (key matrix)
   V shape: (16, 16) (value matrix)
   Q[:5]: [ 0.04967142 -0.01382643  0.06476886  0.15230298 -0.02341534]

2. ATTENTION SCORES (K[i] Â· Q):
   Scores shape: (16,)
   Scores[:5]: [-0.03479404 -0.01563787  0.04834607  0.06764711  0.04001468]
   Min: -0.061636, Max: 0.067647
   Manual verification:
     Q Â· K[0] = K[0] Â· Q = -0.034794 (computed: -0.034794)
     Q Â· K[1] = K[1] Â· Q = -0.015638 (computed: -0.015638)
     Q Â· K[2] = K[2] Â· Q = 0.048346 (computed: 0.048346)

3. SOFTMAX:
   Max score: 0.067647
   Attention weights shape: (16,)
   Attention weights[:5]: [0.05981331 0.06097015 0.06499878 0.0662655  0.06445949]
   Sum: 1.000000 (should be 1.0)

4. WEIGHTED SUM OF VALUES:
   Output shape: (16,)
   Output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]
   Output norm: 0.092764
   Manual output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]
   Match: True

================================================================================
TESTING INDIVIDUAL OPERATIONS
================================================================================

Test 1: Vector Dot Product
a Â· b = 3.000000

Test 2: Matrix-Vector Multiplication
M @ v = [ 3.  7. 11.]

Test 3: Softmax
Input: [1. 2. 3. 4.]
Softmax: [0.0320586  0.08714432 0.2368828  0.6439143 ]
Sum: 1.000000

================================================================================
TESTING FULL ATTENTION
================================================================================
Compiling attention graph on Device(type=cpu,id=0)
Executing attention on Device(type=cpu,id=0)
====================================================================================================

CPU attention output[:5]: [-0.00935538 -0.02434331  0.00306551  0.02346884  0.019306  ]
CPU matches NumPy: True
Compiling attention graph on Device(type=gpu,id=0)
Executing attention on Device(type=gpu,id=0)
====================================================================================================

GPU attention output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]
Expected output[:5]: [-0.00935538 -0.0243433   0.00306551  0.02346884  0.019306  ]
GPU matches NumPy: True

================================================================================
FINAL VERIFICATION
================================================================================
âœ“ CPU implementation PASSED
âœ“ GPU implementation PASSED

Output vector norms:
  CPU: 0.092764
  GPU: 0.092764
  Expected: 0.092764
</code></pre>
<p>This indicates that your custom MAX Graph operation correctly implements the attention algorithm and produces results matching the NumPy reference implementation.</p>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<p>To solve this puzzle, we need to implement the transpose kernel in Mojo and complete the Python graph definition for our attention custom operation. This puzzle builds upon concepts from previous puzzles, combining <strong>tiled matrix multiplication from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a></strong> and <strong>softmax from <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a></strong> into a complete attention mechanism.</p>
<h3 id="reused-kernels"><a class="header" href="#reused-kernels">Reused kernels</a></h3>
<p>Our implementation directly incorporates these proven kernels:</p>
<ol>
<li><strong><code>matmul_idiomatic_tiled</code></strong> from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a> - Powers both \(Q \times K^T\) and \(\text{weights} \times V\) operations</li>
<li><strong><code>softmax_kernel</code></strong> from <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a> - Provides numerically stable attention weight computation</li>
</ol>
<p>This exemplifies <strong>modular GPU architecture</strong>: complex neural network operations built by orchestrating proven, optimized components rather than monolithic implementations.</p>
<p>The attention operation follows the canonical mathematical definition:</p>
<p>$$\Large \text{Attention}(Q, K, V) = \text{softmax}(Q \cdot K^T) \cdot V$$</p>
<p><strong>Breaking down the math</strong>:</p>
<ul>
<li>\(Q \cdot K^T\): Query-key similarity scores of shape: \((1, \text{seq_len})\)</li>
<li>\(\text{softmax}(\cdot)\): Normalize scores to probabilities of shape: \((1, \text{seq_len})\)</li>
<li>\(\text{weights} \cdot V\): Weighted combination of values of shape: \((1, d)\)</li>
</ul>
<p>This involves several computational steps that we optimize using GPU kernels from previous puzzles.</p>
<h3 id="1-transpose-kernel-implementation"><a class="header" href="#1-transpose-kernel-implementation">1. Transpose kernel implementation:</a></h3>
<pre><code class="language-mojo">fn transpose_kernel[
    layout_in: Layout,  # Layout for input matrix (seq_len, d)
    layout_out: Layout,  # Layout for output matrix (d, seq_len)
    rows: Int,
    cols: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, layout_out, MutableAnyOrigin],
    inp: LayoutTensor[mut=False, dtype, layout_in, MutableAnyOrigin],
):
    """Transpose matrix using shared memory tiling for coalesced access."""
    shared_tile = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    local_row = thread_idx.y
    local_col = thread_idx.x

    global_row = block_idx.y * TPB + local_row
    global_col = block_idx.x * TPB + local_col

    if global_row &lt; rows and global_col &lt; cols:
        shared_tile[local_row, local_col] = inp[global_row, global_col]
    else:
        shared_tile[local_row, local_col] = 0.0

    barrier()

    out_row = block_idx.x * TPB + local_row
    out_col = block_idx.y * TPB + local_col

    # Store data from shared memory to global memory (coalesced write)
    # Note: we transpose the shared memory access pattern
    if out_row &lt; cols and out_col &lt; rows:
        output[out_row, out_col] = shared_tile[local_col, local_row]


</code></pre>
<div class="solution-explanation">
<p>The transpose kernel uses <strong>shared memory tiling</strong> to achieve coalesced memory access patterns. Key implementation details:</p>
<h4 id="critical-transpose-pattern"><a class="header" href="#critical-transpose-pattern">Critical transpose pattern</a></h4>
<pre><code class="language-mojo"># Load with normal indexing
shared_tile[local_row, local_col] = inp[global_row, global_col]
barrier()
# Store with swapped indexing for transpose
output[out_row, out_col] = shared_tile[local_col, local_row]
</code></pre>
<p>The transpose happens through <strong>swapped indexing</strong> in shared memory access (<code>[local_col, local_row]</code> instead of <code>[local_row, local_col]</code>) and <strong>swapped block coordinates</strong> for output positioning. This ensures both reads and writes remain coalesced while achieving the transpose operation.</p>
</div>
<h3 id="2-gpu-kernel-orchestration"><a class="header" href="#2-gpu-kernel-orchestration">2. GPU kernel orchestration:</a></h3>
<pre><code class="language-mojo">
            # Step 1: Reshape Q from (d,) to (1, d) - no buffer needed
            q_2d = q_tensor.reshape[layout_q_2d]()

            # Step 2: Transpose K from (seq_len, d) to K^T (d, seq_len)
            gpu_ctx.enqueue_function[
                transpose_kernel[layout_k, layout_k_t, seq_len, d, dtype]
            ](
                k_t,
                k_tensor,
                grid_dim=transpose_blocks_per_grid,
                block_dim=matmul_threads_per_block,
            )

            # Step 3: Compute attention scores using matmul: Q @ K^T = (1, d) @ (d, seq_len) -&gt; (1, seq_len)
            # This computes Q Â· K^T[i] = Q Â· K[i] for each column i of K^T (which is row i of K)
            # Reuse scores_weights_buf as (1, seq_len) for scores
            scores_2d = LayoutTensor[
                mut=True, dtype, layout_scores_2d, MutableAnyOrigin
            ](scores_weights_buf.unsafe_ptr())
            gpu_ctx.enqueue_function[
                matmul_idiomatic_tiled[layout_q_2d, 1, seq_len, d, dtype]
            ](
                scores_2d,
                q_2d,
                k_t,
                grid_dim=scores_blocks_per_grid,
                block_dim=matmul_threads_per_block,
            )

            # Step 4: Reshape scores from (1, seq_len) to (seq_len,) for softmax
            weights = scores_2d.reshape[layout_scores]()

            # Step 5: Apply softmax to get attention weights
            gpu_ctx.enqueue_function[
                softmax_gpu_kernel[layout_scores, seq_len, dtype]
            ](
                weights,
                weights,
                grid_dim=(1, 1),
                block_dim=(seq_len, 1),
            )

            # Step 6: Reshape weights from (seq_len,) to (1, seq_len) for final matmul
            weights_2d = weights.reshape[layout_weights_2d]()

            # Step 7: Compute final result using matmul: weights @ V = (1, seq_len) @ (seq_len, d) -&gt; (1, d)
            # Reuse out_tensor reshaped as (1, d) for result
            result_2d = output_tensor.reshape[layout_result_2d]()
            gpu_ctx.enqueue_function[
                matmul_idiomatic_tiled[layout_weights_2d, 1, d, seq_len, dtype]
            ](
                result_2d,
                weights_2d,
                v_tensor,
                grid_dim=result_blocks_per_grid,
                block_dim=matmul_threads_per_block,
            )

</code></pre>
<div class="solution-explanation">
<p>The GPU orchestration demonstrates <strong>sophisticated kernel chaining</strong> and <strong>zero-copy memory optimization</strong>:</p>
<h4 id="advanced-memory-optimization-strategies"><a class="header" href="#advanced-memory-optimization-strategies">Advanced memory optimization strategies</a></h4>
<pre><code class="language-mojo"># Zero-copy reshaping - no data movement, just reinterpret tensor shape
q_2d = q_tensor.reshape[layout_q_2d]()
# Aggressive buffer reuse - same memory, different interpretations
weights = scores_2d.reshape[layout_scores]()
</code></pre>
<p>The implementation achieves <strong>maximum memory efficiency</strong> through:</p>
<ul>
<li><strong>Zero-copy reshaping</strong>: Reinterpreting tensor shapes without moving data in memory</li>
<li><strong>Intelligent buffer reuse</strong>: The same <code>scores_weights_buf</code> serves dual purposes as both scores \((1,\text{seq\_len})\) and weights \((\text{seq\_len},)\)</li>
<li><strong>Minimal allocations</strong>: Only 2 temporary buffers power the entire attention operation</li>
<li><strong>Memory coalescing</strong>: All operations maintain optimal memory access patterns</li>
</ul>
<h4 id="strategic-kernel-reuse-pattern"><a class="header" href="#strategic-kernel-reuse-pattern">Strategic kernel reuse pattern</a></h4>
<ul>
<li><strong>Steps 3 &amp; 7</strong>: Both leverage <code>matmul_idiomatic_tiled</code> from <a href="../puzzle_14/puzzle_14.html">Puzzle 14</a>
<ul>
<li>Step 3: \(Q \times K^T\) â†’ attention scores computation \((1,d) \times (d,\text{seq_len}) \rightarrow (1,\text{seq_len})\)</li>
<li>Step 7: \(\text{weights} \times V\) â†’ final weighted output \((1,\text{seq_len}) \times (\text{seq_len},d) \rightarrow (1,d)\)</li>
<li>Both operations include bounds checking for robustness with variable matrix dimensions</li>
</ul>
</li>
<li><strong>Step 5</strong>: Employs <code>softmax_kernel</code> from <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a>
<ul>
<li>Converts raw scores into normalized probability distribution</li>
<li>Ensures numerical stability through max subtraction and parallel reduction</li>
<li>Guarantees \(\sum_{i} \text{weights}[i] = 1.0\)</li>
</ul>
</li>
</ul>
<p>This exemplifies <strong>modular GPU architecture</strong>: complex neural network operations built by orchestrating proven, optimized kernels rather than monolithic implementations!</p>
</div>
<h3 id="key-implementation-insights"><a class="header" href="#key-implementation-insights">Key implementation insights</a></h3>
<div class="solution-explanation">
<h4 id="memory-optimization-strategy"><a class="header" href="#memory-optimization-strategy">Memory optimization strategy</a></h4>
<p>The implementation achieves <strong>minimal memory allocation</strong> through aggressive buffer reuse:</p>
<pre><code class="language-mojo"># Only 2 temporary buffers needed for the entire operation
k_t_buf = gpu_ctx.enqueue_create_buffer[dtype](seq_len * d)
scores_weights_buf = gpu_ctx.enqueue_create_buffer[dtype](seq_len)
</code></pre>
<p><strong>Key optimization insights</strong>:</p>
<ul>
<li>The same <code>scores_weights_buf</code> is reused for both attention scores and weights through reshape operations</li>
<li>Zero-copy tensor reshaping eliminates unnecessary data movement</li>
</ul>
<h4 id="kernel-reuse-architecture"><a class="header" href="#kernel-reuse-architecture">Kernel reuse architecture</a></h4>
<p>This puzzle showcases <strong>modular kernel design</strong> by combining three specialized kernels:</p>
<ul>
<li><strong><code>matmul_idiomatic_tiled</code></strong> (used twice) - Powers both \(Q \times K^T\) and \(\text{weights} \times V\) operations</li>
<li><strong><code>softmax_kernel</code></strong> - Provides numerically stable attention weight computation with parallel reduction</li>
<li><strong><code>transpose_kernel</code></strong> - Enables efficient \(K^T\) computation with coalesced memory access</li>
</ul>
<p><strong>Architectural benefits</strong>:</p>
<ul>
<li><strong>Composability</strong>: Complex operations built from proven components</li>
<li><strong>Maintainability</strong>: Each kernel has a single, well-defined responsibility</li>
<li><strong>Performance</strong>: Leverages highly optimized implementations from previous puzzles</li>
<li><strong>Scalability</strong>: Modular design enables easy extension to larger attention mechanisms</li>
</ul>
<p>The implementation demonstrates that <strong>sophisticated neural network operations</strong> can be built by orchestrating simpler, well-tested GPU kernels rather than writing monolithic implementations.</p>
</div>
</details>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_16/puzzle_16.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../bonuses/part3.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_16/puzzle_16.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../bonuses/part3.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
