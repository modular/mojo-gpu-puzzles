<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Puzzle 17: 1D Convolution Op - Mojo üî• GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Mojoüî• GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta property="og:image" content="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">
        <meta property="og:url" content="https://puzzles.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="Mojoüî• GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta name="twitter:image" content="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">
        <link rel="icon" type="image/png" href="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="puzzle-17-1d-convolution-op"><a class="header" href="#puzzle-17-1d-convolution-op">Puzzle 17: 1D Convolution Op</a></h1>
<blockquote>
<h2 id="bridging-to-python-with-max-graph"><a class="header" href="#bridging-to-python-with-max-graph">Bridging to Python with MAX Graph</a></h2>
<p>We‚Äôre now entering Part IV of our GPU puzzle journey: <strong>Interfacing with Python via MAX Graph Custom Ops</strong>.</p>
<p>In previous puzzles, we‚Äôve learned how to write efficient GPU kernels in Mojo. Now we‚Äôll explore how to:</p>
<ul>
<li>Package these kernels as custom operations that can be called from Python</li>
<li>Integrate with the MAX Graph system for accelerated machine learning</li>
<li>Bridge the gap between high-level Python APIs and low-level GPU code</li>
</ul>
<p>This integration allows us to leverage the performance of Mojo GPU kernels while working in familiar Python environments.</p>
</blockquote>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>In <a href="../puzzle_13/puzzle_13.html">Puzzle 13</a>, we implemented a 1D convolution kernel that runs efficiently on the GPU. Now we‚Äôll take this kernel and transform it into a custom operation that can be called directly from Python using <a href="https://docs.modular.com/max/api/python/graph/">MAX Graph</a>.</p>
<p>The 1D convolution kernel we‚Äôll be working with is already implemented:</p>
<pre><code class="language-mojo">alias TPB = 15
alias BLOCKS_PER_GRID = (2, 1)


fn conv1d_kernel[
    in_layout: Layout,
    out_layout: Layout,
    conv_layout: Layout,
    input_size: Int,
    conv_size: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    input: LayoutTensor[mut=True, dtype, in_layout],
    kernel: LayoutTensor[mut=True, dtype, conv_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # first: need to account for padding
    shared_a = tb[dtype]().row_major[TPB + conv_size - 1]().shared().alloc()
    shared_b = tb[dtype]().row_major[conv_size]().shared().alloc()
    if global_i &lt; input_size:
        shared_a[local_i] = input[global_i]

    # second: load elements needed for convolution at block boundary
    if local_i &lt; conv_size - 1:
        # indices from next block
        next_idx = global_i + TPB
        if next_idx &lt; input_size:
            shared_a[TPB + local_i] = input[next_idx]

    if local_i &lt; conv_size:
        shared_b[local_i] = kernel[local_i]

    barrier()

    if global_i &lt; input_size:
        var local_sum: output.element_type = 0

        @parameter
        for j in range(conv_size):
            if local_i + j &lt; TPB + conv_size - 1:
                local_sum += shared_a[local_i + j] * shared_b[j]

        output[global_i] = local_sum


</code></pre>
<p>The key aspects of this puzzle include:</p>
<ol>
<li><strong>Custom op registration</strong>: Understanding how to expose Mojo functions to Python via the <code>@compiler.register</code> decorator</li>
<li><strong>Packaging custom ops</strong>: Learning how to package Mojo code for use with MAX Graph</li>
<li><strong>Python integration</strong>: Calling custom operations from Python through MAX Graph</li>
<li><strong>Cross-language data flow</strong>: Managing data types and memory between Python and GPU</li>
</ol>
<p>This custom operation will:</p>
<ul>
<li>Accept <a href="https://numpy.org/doc/stable/">NumPy</a> arrays as input from Python</li>
<li>Transfer this data to the GPU</li>
<li>Execute our optimized convolution kernel</li>
<li>Return the results back to Python</li>
</ul>
<p>When you complete this puzzle, you‚Äôll have created a seamless bridge between Python‚Äôs rich ecosystem and Mojo‚Äôs powerful GPU performance.</p>
<h2 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h2>
<p>To complete this puzzle, you only need to fill one line to call the <code>conv1d_kernel</code>:</p>
<pre><code class="language-mojo">import compiler
from runtime.asyncrt import DeviceContextPtr
from tensor import InputTensor, OutputTensor
from memory import UnsafePointer
from gpu.host import DeviceBuffer


@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[
        # The kind of device this will be run on: "cpu" or "gpu"
        target: StaticString,
        input_size: Int,
        conv_size: Int,
        dtype: DType = DType.float32,
    ](
        output: OutputTensor[rank=1],
        input: InputTensor[rank = output.rank],
        kernel: InputTensor[rank = output.rank],
        # the context is needed for some GPU calls
        ctx: DeviceContextPtr,
    ) raises:
        output_tensor = output.to_layout_tensor()
        input_tensor = input.to_layout_tensor()
        kernel_tensor = kernel.to_layout_tensor()
        alias in_layout = input_tensor.layout
        alias output_layout = output_tensor.layout
        alias conv_layout = kernel_tensor.layout

        @parameter
        if target == "gpu":
            gpu_ctx = ctx.get_device_context()
            # making sure the output tensor is zeroed out before the kernel is called
            gpu_ctx.enqueue_memset(
                DeviceBuffer[output_tensor.dtype](
                    gpu_ctx,
                    rebind[UnsafePointer[Scalar[output_tensor.dtype]]](
                        output_tensor.ptr
                    ),
                    input_size,
                    owning=False,
                ),
                0,
            )

            # FILL ME IN with 1 line calling our conv1d_kernel

        elif target == "cpu":
            # we can fallback to CPU
            pass
        else:
            raise Error("Unsupported target: " + target)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p17/op/conv1d.mojo" class="filename">View full file: problems/p17/op/conv1d.mojo</a></p>
<p>You can run the puzzle with:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p17
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p17
</code></pre>
  </div>
</div>
<p>When successful, you should see output similar to:</p>
<pre><code>Input array: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]
Convolution kernel: [0. 1. 2. 3.]
Expected result (NumPy calculation): [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]
Compiling 1D convolution graph...
Executing 1D convolution...
1D Convolution result (custom Mojo kernel): [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]
Verification passed: Custom kernel results match NumPy calculation
</code></pre>
<p>This indicates that your custom MAX Graph operation correctly implements the 1D convolution algorithm.</p>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<p>To solve this puzzle, we need to integrate our 1D convolution kernel with the MAX Graph system. The key is to properly call our kernel from the <code>execute</code> method in the <code>Conv1DCustomOp</code> struct.</p>
<p>The solution is:</p>
<pre><code class="language-mojo">            gpu_ctx.enqueue_function[
                conv1d_kernel[
                    in_layout, out_layout, conv_layout, input_size, conv_size
                ]
            ](
                output_tensor,
                input_tensor,
                kernel_tensor,
                grid_dim=BLOCKS_PER_GRID,
                block_dim=(TPB, 1),
            )
</code></pre>
<div class="solution-explanation">
This single line does several important things:
<ol>
<li>Calls <a href="https://docs.modular.com/mojo/stdlib/gpu/host/device_context/DeviceContext/#enqueue_function">enqueue_function</a> on the GPU context (<code>gpu_ctx</code> is of type <a href="https://docs.modular.com/mojo/stdlib/gpu/host/device_context/DeviceContext/">DeviceContext</a>) to schedule our kernel execution</li>
<li>Passes the necessary layout and size information as <strong>compile-time</strong> parameters</li>
<li>Provides the output, input, and kernel tensors as runtime arguments</li>
<li>Configures the execution grid with the appropriate dimensions</li>
</ol>
<p>Let‚Äôs break down how this works in the larger context:</p>
<h3 id="python-mojo-integration-flow"><a class="header" href="#python-mojo-integration-flow">Python-Mojo integration flow</a></h3>
<ol>
<li>
<p><strong>Python side (<a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p17/p17.py" class="filename">problems/p17/p17.py</a>)</strong>:</p>
<ul>
<li>Creates NumPy arrays for input and kernel</li>
<li>Calls <code>conv_1d()</code> function which wraps our operation in MAX Graph</li>
<li>Converts NumPy arrays to <a href="https://docs.modular.com/max/api/python/driver">MAX driver</a> Tensors with <code>Tensor.from_numpy(input).to(device)</code></li>
<li>Loads the custom operation package with <code>custom_extensions=[mojo_kernels]</code></li>
</ul>
</li>
<li>
<p><strong>Graph building</strong>:</p>
<ul>
<li>Defines input and output tensor types with <a href="https://docs.modular.com/max/api/python/graph/type/#max.graph.type.TensorType">TensorType</a></li>
<li>Specifies parameters for our operation via <code>parameters={...}</code></li>
<li>Creates a computation graph with <a href="https://docs.modular.com/max/api/python/graph/Graph"><code>Graph("conv_1d_graph", ...)</code></a></li>
<li>Calls our operation using <a href="https://docs.modular.com/max/api/python/graph/ops#custom"><code>ops.custom(name="conv1d", ...)</code></a></li>
</ul>
</li>
<li>
<p><strong>Custom op registration</strong>:</p>
<ul>
<li>The <code>@compiler.register("conv1d")</code> decorator exposes our operation to MAX Graph. See <a href="https://docs.modular.com/mojo/manual/decorators/compiler-register/">@compiler.register</a></li>
<li>The <code>execute</code> method parameters define the interface (inputs, outputs, context)</li>
<li>Input/output tensors are converted to LayoutTensors for use in our kernel</li>
<li>Device context manages GPU memory allocation and kernel execution</li>
</ul>
</li>
<li>
<p><strong>Kernel execution</strong>:</p>
<ul>
<li>When <a href="">model.execute(‚Ä¶)</a> is called, our <code>conv1d_kernel</code> receives the data</li>
<li>GPU thread configuration is set with <code>grid_dim</code> and <code>block_dim</code></li>
<li>Results are transferred back to CPU with <code>result.to(CPU())</code></li>
<li>NumPy verification compares our results with the expected output</li>
</ul>
</li>
</ol>
<h3 id="key-components-in-detail"><a class="header" href="#key-components-in-detail">Key Components in Detail</a></h3>
<ol>
<li>
<p><strong>Custom Op Structure</strong>:</p>
<pre><code class="language-mojo">@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[target: StaticString, input_size: Int, conv_size: Int, dtype: DType = DType.float32](
        output: OutputTensor[rank=1],
        input: InputTensor[dtype = output.dtype, rank = output.rank],
        kernel: InputTensor[dtype = output.dtype, rank = output.rank],
        ctx: DeviceContextPtr,
    ) raises:
        # Implementation
</code></pre>
<ul>
<li><code>target</code> indicates the device type (‚Äúgpu‚Äù or ‚Äúcpu‚Äù)</li>
<li><code>input_size</code> and <code>conv_size</code> are parameters passed from Python</li>
<li>Tensor types ensure correct shape and type checking</li>
<li>Return type is <code>raises</code> for proper error handling</li>
</ul>
</li>
<li>
<p><strong>Tensor Conversion</strong>:</p>
<pre><code class="language-mojo">output_tensor = output.to_layout_tensor()
input_tensor = input.to_layout_tensor()
kernel_tensor = kernel.to_layout_tensor()
</code></pre>
<ul>
<li>MAX Graph tensors are converted to Mojo LayoutTensors</li>
<li>This allows our kernel to work with them directly</li>
<li>The layouts are extracted for compile-time optimization</li>
</ul>
</li>
<li>
<p><strong>Device Context Usage</strong>:</p>
<pre><code class="language-mojo">gpu_ctx = ctx.get_device_context()
gpu_ctx.enqueue_memset(...)  # Zero output buffer
gpu_ctx.enqueue_function[...](...) # Schedule kernel
</code></pre>
<ul>
<li>Device context manages GPU resources</li>
<li>Memory operations ensure correct buffer state</li>
<li>Function enqueueing schedules our kernel for execution</li>
</ul>
</li>
</ol>
<p>This solution demonstrates the complete flow from Python data through MAX Graph to GPU execution and back, leveraging Mojo‚Äôs powerful type system and parametric functions to create efficient, type-safe, accelerated operations.</p>
</details>
<h2 id="understanding-max-graph-custom-ops"><a class="header" href="#understanding-max-graph-custom-ops">Understanding MAX Graph custom ops</a></h2>
<blockquote>
<p>Check out the follow tutorials for more details:</p>
<ul>
<li><a href="https://docs.modular.com/max/tutorials/get-started-with-max-graph-in-python/">Get started with MAX Graph in Python</a></li>
<li><a href="https://docs.modular.com/max/tutorials/build-custom-ops/">MAX Graph custom op for GPUs</a></li>
</ul>
</blockquote>
<h3 id="custom-op-registration"><a class="header" href="#custom-op-registration">Custom op registration</a></h3>
<p>The core of creating a custom operation is the <code>@compiler.register</code> decorator and the associated structure:</p>
<pre><code class="language-mojo">@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[...](
        output: OutputTensor[rank=1],
        input: InputTensor[dtype = output.dtype, rank = output.rank],
        kernel: InputTensor[type = output.dtype, rank = output.rank],
        ctx: DeviceContextPtr,
    ) raises:
        # Implementation here
</code></pre>
<p>Key components of the registration:</p>
<ul>
<li>The <strong>name</strong> passed to the decorator (<code>"conv1d"</code>) is what Python code will use to call this operation</li>
<li>The <strong>struct</strong> must have an <code>execute</code> method with the correct signature</li>
<li><strong>OutputTensor</strong> and <strong>InputTensor</strong> types define the interface for Python data</li>
<li><strong>DeviceContextPtr</strong> provides access to the execution environment</li>
</ul>
<h3 id="packaging-custom-ops"><a class="header" href="#packaging-custom-ops">Packaging custom ops</a></h3>
<p>Before the custom operation can be used from Python, it needs to be packaged:</p>
<pre><code class="language-bash">mojo package op -o op.mojopkg
</code></pre>
<p>This command:</p>
<ol>
<li>Compiles the Mojo code into a deployable package</li>
<li>Creates the necessary metadata for MAX Graph to understand the operation</li>
<li>Produces a binary artifact (<code>op.mojopkg</code>) that can be loaded by Python</li>
</ol>
<p>The package must be placed in a location where MAX Graph can find it, typically in a directory accessible to the Python code.</p>
<h3 id="python-integration"><a class="header" href="#python-integration">Python integration</a></h3>
<p>On the Python side, here‚Äôs how the custom operation is used:</p>
<pre><code class="language-python"># Path to the directory containing our Mojo operations
mojo_kernels = Path(__file__).parent / "op"

# Configure our graph with the custom conv1d operation
with Graph(
    "conv_1d_graph",
    input_types=[...],
    custom_extensions=[mojo_kernels],  # Load our custom op package
) as graph:
    # Define inputs to the graph
    input_value, kernel_value = graph.inputs

    # Use our custom operation by name
    output = ops.custom(
        name="conv1d",  # Must match the name in @compiler.register
        values=[input_value, kernel_value],
        out_types=[...],
        parameters={
            "input_size": input_tensor.shape[0],
            "conv_size": kernel_tensor.shape[0],
            "dtype": dtype,
        },
    )[0].tensor
</code></pre>
<p>The key elements are:</p>
<ol>
<li>Specifying the path to our custom operations with <code>custom_extensions</code></li>
<li>Calling <code>ops.custom</code> with the registered operation name</li>
<li>Passing input values and parameters that match our operation‚Äôs signature</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_16/tiled.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../puzzle_18/puzzle_18.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_16/tiled.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../puzzle_18/puzzle_18.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
