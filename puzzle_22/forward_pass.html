<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>‚öõÔ∏è Fused vs Unfused Kernels - Mojo üî• GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Mojoüî• GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta property="og:image" content="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">
        <meta property="og:url" content="https://puzzles.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="Mojoüî• GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta name="twitter:image" content="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">
        <link rel="icon" type="image/png" href="https://puzzles.modular.com/puzzles_images/puzzle-mark.png">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="-fused-vs-unfused-kernels"><a class="header" href="#-fused-vs-unfused-kernels">‚öõÔ∏è Fused vs Unfused Kernels</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>In this puzzle, we explore the performance benefits of kernel fusion by implementing and comparing two approaches to the <a href="https://arxiv.org/abs/1607.06450">LayerNorm</a> and Linear operation:</p>
<ol>
<li><strong>Unfused approach</strong>: Executes LayerNorm and Linear as separate operations</li>
<li><strong>Fused kernel</strong>: Combines LayerNorm and Linear operations into a single GPU kernel</li>
</ol>
<p>This comparison demonstrates how kernel fusion can significantly improve performance by:</p>
<ul>
<li>Reducing memory bandwidth usage</li>
<li>Minimizing kernel launch overhead</li>
<li>Improving cache utilization</li>
<li>Eliminating intermediate memory allocations</li>
</ul>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll master:</p>
<ul>
<li><strong>Kernel fusion techniques</strong> for combining multiple operations</li>
<li><strong>Memory bandwidth optimization</strong> through fused operations</li>
<li><strong>Performance benchmarking</strong> of different kernel implementations</li>
<li><strong>Numerical stability</strong> in fused operations</li>
<li><strong>PyTorch custom operation integration</strong></li>
</ul>
<p>The mathematical operations we‚Äôre fusing are:</p>
<ol>
<li>
<p>LayerNorm:
\[\Large \text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \]</p>
</li>
<li>
<p>Linear:
\[\Large \text{Linear}(x) = Wx + b \]</p>
</li>
</ol>
<p>When fused, we compute:
\[\Large \text{Fused}(x) = W(\gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta) + b \]</p>
<h2 id="understanding-layernorm"><a class="header" href="#understanding-layernorm">Understanding LayerNorm</a></h2>
<p>LayerNorm is a normalization technique that helps stabilize and accelerate the training of deep neural networks. Let‚Äôs break down its components and parameters:</p>
<h3 id="what-layernorm-does"><a class="header" href="#what-layernorm-does">What LayerNorm does</a></h3>
<ol>
<li>
<p><strong>Normalization</strong>: LayerNorm normalizes the activations across the features (hidden dimensions) for each sample independently. This means:</p>
<ul>
<li>For each sequence position, it computes statistics across the hidden dimension</li>
<li>Each sample in the batch is normalized independently</li>
<li>This is different from <a href="https://arxiv.org/abs/1502.03167">BatchNorm</a>, which normalizes across the batch dimension</li>
</ul>
</li>
<li>
<p><strong>Parameters</strong>:</p>
<ul>
<li>\(\gamma\) (scale): A learnable parameter vector that allows the network to learn the optimal scale for each feature</li>
<li>\(\beta\) (shift): A learnable parameter vector that allows the network to learn the optimal shift for each feature</li>
<li>\(\epsilon\): A small constant (1e-5) added to the variance to prevent division by zero</li>
</ul>
</li>
</ol>
<h3 id="what-layernorm-does-in-practice"><a class="header" href="#what-layernorm-does-in-practice">What LayerNorm does in practice</a></h3>
<p>LayerNorm performs several crucial functions in deep neural networks:</p>
<ol>
<li>
<p><strong>Feature standardization</strong>:</p>
<ul>
<li>Transforms each feature to have zero mean and unit variance</li>
<li>Makes the network‚Äôs learning process more stable</li>
<li>Helps prevent the ‚Äúinternal covariate shift‚Äù problem where the distribution of layer inputs changes during training</li>
</ul>
</li>
<li>
<p><strong>Gradient flow</strong>:</p>
<ul>
<li>Improves gradient flow through the network</li>
<li>Prevents vanishing/exploding gradients</li>
<li>Makes training more efficient by allowing higher learning rates</li>
</ul>
</li>
<li>
<p><strong>Regularization effect</strong>:</p>
<ul>
<li>Acts as a form of implicit regularization</li>
<li>Helps prevent overfitting by normalizing the feature distributions</li>
<li>Makes the network more robust to input variations</li>
</ul>
</li>
<li>
<p><strong>Sequence modeling</strong>:</p>
<ul>
<li>Particularly effective in transformer architectures</li>
<li>Helps maintain consistent signal magnitude across different sequence lengths</li>
<li>Enables better handling of variable-length sequences</li>
</ul>
</li>
<li>
<p><strong>Training dynamics</strong>:</p>
<ul>
<li>Accelerates training convergence</li>
<li>Reduces the need for careful learning rate tuning</li>
<li>Makes the network less sensitive to weight initialization</li>
</ul>
</li>
</ol>
<h3 id="mathematical-components"><a class="header" href="#mathematical-components">Mathematical components</a></h3>
<ol>
<li>
<p><strong>Mean Calculation</strong> (\(\mu\)):
\[\Large \mu = \frac{1}{H} \sum_{i=1}^{H} x_i \]</p>
<ul>
<li>Computes the mean across the hidden dimension (H)</li>
<li>Each sequence position has its own mean</li>
</ul>
</li>
<li>
<p><strong>Variance Calculation</strong> (\(\sigma^2\)):
\[\Large \sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2 \]</p>
<ul>
<li>Computes the variance across the hidden dimension</li>
<li>Used to scale the normalized values</li>
</ul>
</li>
<li>
<p><strong>Normalization and Scaling</strong>:
\[\Large \text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \]</p>
<ul>
<li>First normalizes the input to have zero mean and unit variance</li>
<li>Then applies learnable scale (\(\gamma\)) and shift (\(\beta\)) parameters</li>
<li>The \(\odot\) symbol represents elementwise multiplication (Hadamard product)</li>
<li>For example, if \(\gamma = [1.2, 0.8, 1.5]\)  and normalized input is \([0.5, -0.3, 0.7]\), then \(\gamma \odot x = [0.6, -0.24, 1.05]\)</li>
</ul>
</li>
</ol>
<h3 id="why-layernorm-is-important"><a class="header" href="#why-layernorm-is-important">Why LayerNorm is important</a></h3>
<ol>
<li>
<p><strong>Training Stability</strong>:</p>
<ul>
<li>Prevents activations from growing too large or small</li>
<li>Helps maintain consistent signal magnitude throughout the network</li>
</ul>
</li>
<li>
<p><strong>Feature Learning</strong>:</p>
<ul>
<li>The scale (\(\gamma\)) and shift (\(\beta\)) parameters allow the network to learn which features are important</li>
<li>Can effectively learn to ignore or emphasize certain features</li>
</ul>
</li>
<li>
<p><strong>Independence</strong>:</p>
<ul>
<li>Unlike BatchNorm, LayerNorm‚Äôs statistics are computed independently for each sample</li>
<li>Makes it more suitable for variable-length sequences and small batch sizes</li>
</ul>
</li>
</ol>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li>Batch size: <code>BATCH_SIZE = 4</code></li>
<li>Sequence length: <code>SEQ_LEN = 4</code></li>
<li>Hidden dimension: <code>HIDDEN_DIM = 8</code></li>
<li>Output dimension: <code>OUTPUT_DIM = 16</code></li>
<li>Epsilon: <code>EPS = 1e-5</code></li>
<li>Data type: <code>DType.float32</code></li>
</ul>
<h2 id="implementation-approaches"><a class="header" href="#implementation-approaches">Implementation approaches</a></h2>
<h3 id="1-unfused-implementation"><a class="header" href="#1-unfused-implementation">1. Unfused implementation</a></h3>
<p>The unfused approach executes operations separately using multiple kernels. Here are some of the kernels we wrote in the previous chapters:</p>
<h4 id="matrix-multiplication-kernel"><a class="header" href="#matrix-multiplication-kernel">Matrix multiplication kernel</a></h4>
<p>From <a href="../puzzle_16/puzzle_16.html">Puzzle 16</a>, we reuse the tiled matrix multiplication kernel for the linear transformation. This kernel includes bounds checking to handle variable matrix dimensions safely:</p>
<pre><code class="language-mojo"># Idiomatic tiled matmul from p14.mojo - adapted for [batch*seq, hidden] @ [hidden, output] -&gt; [batch*seq, output]
fn matmul_idiomatic_tiled[
    a_layout: Layout,
    b_layout: Layout,
    out_layout: Layout,
    rows: Int,
    cols: Int,
    inner_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, a_layout],
    b: LayoutTensor[mut=False, dtype, b_layout],
):
    """Idiomatic tiled matmul following p14.mojo exactly."""
    local_row = thread_idx.x
    local_col = thread_idx.y
    tiled_row = block_idx.y * TPB + local_row
    tiled_col = block_idx.x * TPB + local_col

    # Get the tile of the output matrix that this thread block is responsible for
    out_tile = output.tile[TPB, TPB](block_idx.x, block_idx.y)
    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc().fill(0)

    var acc: output.element_type = 0

    alias load_a_layout = Layout.row_major(1, TPB)
    alias load_b_layout = Layout.row_major(TPB, 1)

    for idx in range((inner_dim + TPB - 1) // TPB):
        # Get tiles from A and B matrices
        a_tile = a.tile[TPB, TPB](block_idx.x, idx)
        b_tile = b.tile[TPB, TPB](idx, block_idx.y)

        # Asynchronously copy tiles to shared memory
        copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)
        copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)

        # Wait for all async copies to complete
        async_copy_wait_all()
        barrier()

        # Compute partial matrix multiplication for this tile
        @parameter
        for k in range(TPB):
            acc += a_shared[local_row, k] * b_shared[k, local_col]

        barrier()

    # Write final result with bounds checking (needed for variable matrix sizes)
    if tiled_row &lt; rows and tiled_col &lt; cols:
        out_tile[local_row, local_col] = acc


</code></pre>
<h4 id="transpose-kernel"><a class="header" href="#transpose-kernel">Transpose kernel</a></h4>
<p>For efficient memory access patterns, we use a transpose kernel with shared memory tiling:</p>
<pre><code class="language-mojo">fn transpose_kernel[
    layout_in: Layout,
    layout_out: Layout,
    rows: Int,
    cols: Int,
](
    output: LayoutTensor[mut=True, dtype, layout_out],
    input: LayoutTensor[mut=False, dtype, layout_in],
):
    """Transpose matrix using shared memory tiling for coalesced access.
    We will learn more about coalesced access in the next part.
    """
    shared_tile = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    local_row = thread_idx.y
    local_col = thread_idx.x

    global_row = block_idx.y * TPB + local_row
    global_col = block_idx.x * TPB + local_col

    if global_row &lt; rows and global_col &lt; cols:
        shared_tile[local_row, local_col] = input[global_row, global_col]
    else:
        shared_tile[local_row, local_col] = 0.0

    barrier()

    out_row = block_idx.x * TPB + local_row
    out_col = block_idx.y * TPB + local_col

    # Store data from shared memory to global memory (coalesced write)
    # Note: we transpose the shared memory access pattern
    if out_row &lt; cols and out_col &lt; rows:
        output[out_row, out_col] = shared_tile[local_col, local_row]


</code></pre>
<h4 id="bias-addition-kernel"><a class="header" href="#bias-addition-kernel">Bias addition kernel</a></h4>
<p>A simple elementwise addition kernel for adding the bias term:</p>
<pre><code class="language-mojo">fn add_bias_kernel[
    input_layout: Layout,
    bias_layout: Layout,
    output_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    output_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    bias: LayoutTensor[mut=False, dtype, bias_layout],
):
    """Simple bias addition."""
    batch_idx = block_idx.x
    seq_idx = block_idx.y
    out_idx = thread_idx.x

    if batch_idx &gt;= batch_size or seq_idx &gt;= seq_len or out_idx &gt;= output_dim:
        return

    output[batch_idx, seq_idx, out_idx] = input[
        batch_idx, seq_idx, out_idx
    ] + rebind[Scalar[dtype]](bias[out_idx])


</code></pre>
<h4 id="layernorm-kernel"><a class="header" href="#layernorm-kernel">LayerNorm kernel</a></h4>
<p>Now complete this kernel to implement the LayerNorm operation. You‚Äôll need to:</p>
<ol>
<li>Compute mean \(\mu\) and variance \(\sigma^2\) for each sequence position</li>
<li>Normalize the input using these statistics</li>
<li>Apply the scale \(\gamma\) and shift \(\beta\) parameters</li>
</ol>
<pre><code class="language-mojo">fn layernorm_kernel[
    input_layout: Layout,
    ln_params_layout: Layout,
    output_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
):
    batch_idx = block_idx.x
    seq_idx = block_idx.y
    hidden_idx = thread_idx.x

    if (
        batch_idx &gt;= batch_size
        or seq_idx &gt;= seq_len
        or hidden_idx &gt;= hidden_dim
    ):
        return

    # Compute statistics for this sequence position (redundant but simple)
    var sum_val: Scalar[dtype] = 0
    var sq_sum: Scalar[dtype] = 0

    # FILL ME IN (roughly 11 lines)


</code></pre>
<p><strong>Implementation steps:</strong></p>
<ol>
<li>First, compute mean and variance using parallel reduction</li>
<li>Then normalize the input using these statistics</li>
<li>Finally, apply the scale and shift parameters</li>
</ol>
<p><strong>Characteristics of unfused approach:</strong></p>
<ul>
<li>Multiple kernel launches (LayerNorm ‚Üí MatMul ‚Üí Bias)</li>
<li>Intermediate tensor allocations between operations</li>
<li>More memory bandwidth usage due to separate passes</li>
<li>Simpler implementation with clear separation of concerns</li>
<li>Easier to debug as each operation is isolated</li>
</ul>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>
<p><strong>Thread organization</strong>:</p>
<ul>
<li>Use one thread block per sequence position (grid: <code>[batch_size, seq_len]</code>)</li>
<li>Each thread handles one hidden dimension element</li>
<li>Avoid redundant computation by computing statistics once per sequence</li>
</ul>
</li>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Access input tensor with <code>[batch_idx, seq_idx, hidden_idx]</code></li>
<li>Access output tensor with <code>[batch_idx, seq_idx, hidden_idx]</code></li>
<li>Access LayerNorm parameters with <code>[hidden_idx]</code></li>
</ul>
</li>
<li>
<p><strong>Numerical stability</strong>:</p>
<ul>
<li>Add epsilon (1e-5) before taking square root</li>
<li>Use <code>rebind[Scalar[dtype]]</code> for proper type casting</li>
<li>Compute variance as (sq_sum / hidden_dim) - (mean * mean)</li>
</ul>
</li>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Compute mean and variance in a single pass</li>
<li>Reuse computed statistics for all elements in sequence</li>
<li>Avoid unnecessary memory barriers</li>
</ul>
</li>
</ol>
</div>
</details>
<h3 id="running-the-code"><a class="header" href="#running-the-code">Running the code</a></h3>
<p>To test your unfused implementation, run:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p22 --unfused
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p22 --unfused
</code></pre>
  </div>
</div>
<p>Your output will look like this:</p>
<pre><code class="language-txt">Testing with dimensions: [4, 4, 8] -&gt; [4, 4, 16]
‚úÖ Loaded Mojo operations library
============================================================
   Puzzle 22: UNFUSED Algorithm Test &amp; Benchmark
============================================================

üß™ Correctness Testing for UNFUSED Algorithm
====================================================

Testing Reference PyTorch Implementation
-----------------------------------------------
‚úÖ Reference PyTorch
   Max difference: 0.00e+00
   Result: ‚úÖ CORRECT

Testing CPU Implementation
---------------------------------
‚úÖ Using Mojo fused kernel (CPU)
   Max difference: 1.86e-08
   Result: ‚úÖ CORRECT

Testing GPU Unfused Implementation
-----------------------------------------
‚úÖ Using Mojo unfused kernel (GPU)
   Max difference: 1.86e-08
   Result: ‚úÖ CORRECT

Correctness Summary:
   - Reference:   ‚úÖ CORRECT
   - CPU:         ‚úÖ CORRECT
   - GPU unfused: ‚úÖ CORRECT

   Overall Correctness: ‚úÖ ALL CORRECT

Benchmarking CPU vs GPU UNFUSED
------------------------------------------
   Testing CPU performance...
   CPU: 3173.70ms (50 iterations)
   Testing GPU unfused performance...
   GPU unfused: 3183.57ms (50 iterations)

   GPU unfused vs CPU: 1.00x slower
   CPU wins (GPU overhead &gt; computation benefit)

UNFUSED Algorithm Test Completed!
</code></pre>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn layernorm_kernel[
    input_layout: Layout,
    ln_params_layout: Layout,
    output_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
):
    batch_idx = block_idx.x
    seq_idx = block_idx.y
    hidden_idx = thread_idx.x

    if (
        batch_idx &gt;= batch_size
        or seq_idx &gt;= seq_len
        or hidden_idx &gt;= hidden_dim
    ):
        return

    # Compute statistics for this sequence position (redundant but simple)
    var sum_val: Scalar[dtype] = 0
    var sq_sum: Scalar[dtype] = 0

    @parameter
    for h in range(hidden_dim):
        val = input[batch_idx, seq_idx, h]
        sum_val += rebind[Scalar[dtype]](val)
        sq_sum += rebind[Scalar[dtype]](val * val)

    mean_val = sum_val / hidden_dim
    var_val = (sq_sum / hidden_dim) - (mean_val * mean_val)
    inv_std = 1.0 / sqrt(var_val + 1e-5)

    # Apply LayerNorm to this element
    input_val = input[batch_idx, seq_idx, hidden_idx]
    normalized = (input_val - mean_val) * inv_std * rebind[Scalar[dtype]](
        ln_weight[hidden_idx]
    ) + rebind[Scalar[dtype]](ln_bias[hidden_idx])
    output[batch_idx, seq_idx, hidden_idx] = normalized


</code></pre>
<div class="solution-explanation">
<p>The unfused implementation follows a straightforward approach where each thread handles one element of the output tensor. Let‚Äôs break down the key components:</p>
<ol>
<li>
<p><strong>Thread and Block Organization</strong>:</p>
<pre><code class="language-mojo">batch_idx = block_idx.x
seq_idx = block_idx.y
hidden_idx = thread_idx.x
</code></pre>
<ul>
<li>Each thread block handles one sequence position in the batch</li>
<li>Grid dimensions: <code>[batch_size, seq_len]</code></li>
<li>Each thread processes one element in the hidden dimension</li>
<li>Early return if indices are out of bounds:
<pre><code class="language-mojo">if (batch_idx &gt;= batch_size or seq_idx &gt;= seq_len or hidden_idx &gt;= hidden_dim):
    return
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Statistics Computation</strong>:</p>
<pre><code class="language-mojo">var sum_val: Scalar[dtype] = 0
var sq_sum: Scalar[dtype] = 0

@parameter
for h in range(hidden_dim):
    val = input[batch_idx, seq_idx, h]
    sum_val += rebind[Scalar[dtype]](val)
    sq_sum += rebind[Scalar[dtype]](val * val)
</code></pre>
<ul>
<li>Compute sum and squared sum in a single pass</li>
<li>Use <code>@parameter</code> for compile-time loop unrolling</li>
<li>Proper type casting with <code>rebind[Scalar[dtype]]</code></li>
<li>Calculate mean and variance:
<pre><code class="language-mojo">mean_val = sum_val / hidden_dim
var_val = (sq_sum / hidden_dim) - (mean_val * mean_val)
inv_std = 1.0 / sqrt(var_val + 1e-5)
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Normalization and Scaling</strong>:</p>
<pre><code class="language-mojo">input_val = input[batch_idx, seq_idx, hidden_idx]
normalized = (input_val - mean_val) * inv_std * rebind[Scalar[dtype]](
    ln_weight[hidden_idx]
) + rebind[Scalar[dtype]](ln_bias[hidden_idx])
output[batch_idx, seq_idx, hidden_idx] = normalized
</code></pre>
<ul>
<li>Apply normalization: \[\Large \text{normalized} = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \]</li>
<li>Scale with learnable parameter <code>Œ≥</code> (ln_weight)</li>
<li>Add learnable bias <code>Œ≤</code> (ln_bias)</li>
<li>Store result in output tensor</li>
</ul>
</li>
<li>
<p><strong>Performance Characteristics</strong>:</p>
<ul>
<li>Each thread computes statistics independently</li>
<li>No shared memory usage (simple but less efficient)</li>
<li>Memory access pattern:
<ul>
<li>Input: <code>[batch_idx, seq_idx, h]</code></li>
<li>Output: <code>[batch_idx, seq_idx, hidden_idx]</code></li>
<li>Parameters: <code>[hidden_idx]</code></li>
</ul>
</li>
<li>Numerical stability ensured by:
<ul>
<li>Adding epsilon (1e-5) before square root</li>
<li>Using proper type casting</li>
<li>Computing variance in a numerically stable way</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Implementation Details</strong>:</p>
<ul>
<li>
<p><strong>Type Safety</strong>:</p>
<ul>
<li>Use <code>Scalar[dtype]</code> for intermediate calculations</li>
<li><code>rebind[Scalar[dtype]]</code> for proper type casting</li>
<li>Ensures consistent floating-point precision</li>
</ul>
</li>
<li>
<p><strong>Memory Access</strong>:</p>
<ul>
<li>Coalesced reads from input tensor</li>
<li>Coalesced writes to output tensor</li>
<li>Sequential access to LayerNorm parameters</li>
</ul>
</li>
<li>
<p><strong>Computation Flow</strong>:</p>
<ul>
<li>Statistics computation: \[\Large O(H) \text{ operations per thread} \]</li>
<li>Normalization: \[\Large O(1) \text{ operations per thread} \]</li>
<li>Total complexity: \[\Large O(H) \text{ per output element} \]</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Redundant computation of statistics</li>
<li>No shared memory for intermediate results</li>
<li>High memory bandwidth usage</li>
<li>Multiple kernel launches required</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>This implementation is correct but not optimal for performance, as shown in the benchmark results where it‚Äôs slightly slower than the CPU version. The fused implementation will address these performance limitations by:</p>
<ul>
<li>Computing statistics once per sequence</li>
<li>Reusing normalized values</li>
<li>Reducing memory traffic</li>
<li>Eliminating intermediate tensor allocations</li>
</ul>
</div>
</details>
<h3 id="2-fused-kernel-implementation"><a class="header" href="#2-fused-kernel-implementation">2. Fused kernel implementation</a></h3>
<p>The fused kernel combines LayerNorm and Linear operations into a single GPU kernel:</p>
<pre><code class="language-mojo">fn minimal_fused_kernel[
    input_layout: Layout,
    ln_params_layout: Layout,
    weight_layout: Layout,
    bias_layout: Layout,
    output_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
    output_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
    linear_weight: LayoutTensor[mut=False, dtype, weight_layout],
    linear_bias: LayoutTensor[mut=False, dtype, bias_layout],
):
    """Minimal fused kernel - one thread per sequence position to avoid redundancy.
    """
    # Grid: (batch_size, seq_len) - one thread block per sequence position
    # Block: (1,) - single thread per sequence position to avoid redundant computation
    batch_idx = block_idx.x
    seq_idx = block_idx.y

    if batch_idx &gt;= batch_size or seq_idx &gt;= seq_len:
        return

    # Step 1: Compute LayerNorm statistics once per sequence position

    # FILL IN roughly 10 lines

    # Step 2: Compute all outputs for this sequence position

    # FILL IN roughly 10 lines


</code></pre>
<p><strong>Key optimizations:</strong></p>
<ul>
<li>Single kernel launch instead of two</li>
<li>Shared memory for intermediate results</li>
<li>Coalesced memory access patterns</li>
<li>Reduced memory bandwidth usage</li>
<li>No intermediate tensor allocations</li>
</ul>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>
<p><strong>Thread organization</strong>:</p>
<ul>
<li>One thread block per sequence position (grid: <code>[batch_size, seq_len]</code>)</li>
<li>Single thread per sequence position to avoid redundancy</li>
<li>Compute all outputs for each sequence position in one thread</li>
</ul>
</li>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Access input tensor with <code>[batch_idx, seq_idx, h]</code></li>
<li>Access output tensor with <code>[batch_idx, seq_idx, out_idx]</code></li>
<li>Access weights with <code>[out_idx, h]</code> for linear layer</li>
</ul>
</li>
<li>
<p><strong>Computation flow</strong>:</p>
<ul>
<li>Compute LayerNorm statistics once per sequence</li>
<li>Reuse normalized values for all output dimensions</li>
<li>Combine normalization and linear transformation</li>
</ul>
</li>
<li>
<p><strong>Performance</strong>:</p>
<ul>
<li>Avoid redundant computation of statistics</li>
<li>Minimize memory traffic by fusing operations</li>
<li>Use proper type casting with <code>rebind[Scalar[dtype]]</code></li>
</ul>
</li>
</ol>
</div>
</details>
<h3 id="running-the-code-1"><a class="header" href="#running-the-code-1">Running the code</a></h3>
<p>To test your fused implementation, run:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p22 --fused
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p22 --fused
</code></pre>
  </div>
</div>
<p>Your output will look like this:</p>
<pre><code class="language-txt">Testing with dimensions: [4, 4, 8] -&gt; [4, 4, 16]
‚úÖ Loaded Mojo operations library
============================================================
   Puzzle 22: FUSED Algorithm Test &amp; Benchmark
============================================================

üß™ Correctness Testing for FUSED Algorithm
==================================================

Testing Reference PyTorch Implementation
-----------------------------------------------
‚úÖ Reference PyTorch
   Max difference: 0.00e+00
   Result: ‚úÖ CORRECT

Testing CPU Implementation
---------------------------------
‚úÖ Using Mojo fused kernel (CPU)
   Max difference: 1.86e-08
   Result: ‚úÖ CORRECT

Testing GPU Fused Implementation
---------------------------------------
‚úÖ Using Mojo fused kernel (GPU)
   Max difference: 1.86e-08
   Result: ‚úÖ CORRECT

Correctness Summary:
   - Reference:   ‚úÖ CORRECT
   - CPU:         ‚úÖ CORRECT
   - GPU fused: ‚úÖ CORRECT

   Overall Correctness: ‚úÖ ALL CORRECT

‚ö° Benchmarking CPU vs GPU FUSED
----------------------------------------
   Testing CPU performance...
   CPU: 3144.75ms (50 iterations)
   Testing GPU fused performance...
   GPU fused: 3116.11ms (50 iterations)

   GPU fused vs CPU: 1.01x faster
   GPU fused wins!

FUSED Algorithm Test Completed!
</code></pre>
<h2 id="solution-1"><a class="header" href="#solution-1">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn minimal_fused_kernel[
    input_layout: Layout,
    ln_params_layout: Layout,
    weight_layout: Layout,
    bias_layout: Layout,
    output_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    hidden_dim: Int,
    output_dim: Int,
](
    output: LayoutTensor[mut=True, dtype, output_layout],
    input: LayoutTensor[mut=False, dtype, input_layout],
    ln_weight: LayoutTensor[mut=False, dtype, ln_params_layout],
    ln_bias: LayoutTensor[mut=False, dtype, ln_params_layout],
    linear_weight: LayoutTensor[mut=False, dtype, weight_layout],
    linear_bias: LayoutTensor[mut=False, dtype, bias_layout],
):
    """Minimal fused kernel - one thread per sequence position to avoid redundancy.
    """
    # Grid: (batch_size, seq_len) - one thread block per sequence position
    # Block: (1,) - single thread per sequence position to avoid redundant computation
    batch_idx = block_idx.x
    seq_idx = block_idx.y

    if batch_idx &gt;= batch_size or seq_idx &gt;= seq_len:
        return

    # Step 1: Compute LayerNorm statistics once per sequence position
    var sum_val: Scalar[dtype] = 0
    var sq_sum: Scalar[dtype] = 0

    @parameter
    for h in range(hidden_dim):
        val = input[batch_idx, seq_idx, h]
        sum_val += rebind[Scalar[dtype]](val)
        sq_sum += rebind[Scalar[dtype]](val * val)

    mean_val = sum_val / hidden_dim
    var_val = (sq_sum / hidden_dim) - (mean_val * mean_val)
    inv_std = 1.0 / sqrt(var_val + 1e-5)

    # Step 2: Compute all outputs for this sequence position
    @parameter
    for out_idx in range(output_dim):
        var acc: Scalar[dtype] = 0

        @parameter
        for h in range(hidden_dim):
            input_val = input[batch_idx, seq_idx, h]
            normalized = (input_val - mean_val) * inv_std * rebind[
                Scalar[dtype]
            ](ln_weight[h]) + rebind[Scalar[dtype]](ln_bias[h])
            acc += rebind[Scalar[dtype]](normalized * linear_weight[out_idx, h])

        output[batch_idx, seq_idx, out_idx] = acc + rebind[Scalar[dtype]](
            linear_bias[out_idx]
        )


</code></pre>
<div class="solution-explanation">
<p>The fused implementation combines operations efficiently:</p>
<ol>
<li>
<p><strong>Thread organization</strong>:</p>
<ul>
<li>One thread block per sequence position (grid: <code>[batch_size, seq_len]</code>)</li>
<li>Single thread per sequence position</li>
<li>Thread indices: <code>batch_idx = block_idx.x</code>, <code>seq_idx = block_idx.y</code></li>
</ul>
</li>
<li>
<p><strong>LayerNorm phase</strong>:</p>
<ul>
<li>Compute sum and squared sum for the sequence position</li>
<li>Calculate mean: \[\Large \mu = \frac{1}{H} \sum_{i=1}^{H} x_i \]</li>
<li>Calculate variance: \[\Large \sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2 \]</li>
<li>Compute inverse standard deviation: \[\Large \text{inv_std} = \frac{1}{\sqrt{\sigma^2 + \epsilon}} \]</li>
</ul>
</li>
<li>
<p><strong>Linear phase</strong>:</p>
<ul>
<li>For each output dimension:
<ul>
<li>Compute normalized value: \[\Large \text{normalized} = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \]</li>
<li>Multiply with linear weight and accumulate: \[\Large \text{acc} = \sum_{h=1}^{H} \text{normalized}<em>h \cdot W</em>{out,h} \]</li>
<li>Add linear bias: \[\Large \text{output} = \text{acc} + b_{out} \]</li>
</ul>
</li>
<li>Store result in <code>output[batch_idx, seq_idx, out_idx]</code></li>
</ul>
</li>
<li>
<p><strong>Performance optimizations</strong>:</p>
<ul>
<li>Single kernel launch for both operations</li>
<li>Reuse computed statistics</li>
<li>Minimize memory traffic</li>
<li>No intermediate tensor allocations</li>
<li>Efficient memory access patterns</li>
</ul>
</li>
</ol>
<p>This implementation achieves better performance than the unfused version by reducing memory bandwidth usage and kernel launch overhead.</p>
</div>
</details>
<h2 id="advantages-of-kernel-fusion"><a class="header" href="#advantages-of-kernel-fusion">Advantages of kernel fusion</a></h2>
<p>In this puzzle, we‚Äôve explored two approaches to implementing LayerNorm + Linear operations:</p>
<ol>
<li>
<p><strong>Unfused implementation</strong>:</p>
<ul>
<li>Separate kernels for LayerNorm and Linear</li>
<li>Simpler implementation but less efficient</li>
<li>Higher memory bandwidth usage</li>
<li>Multiple kernel launches</li>
<li>Benchmark results: 3183.57ms (GPU)</li>
</ul>
</li>
<li>
<p><strong>Fused implementation</strong>:</p>
<ul>
<li>Single kernel combining both operations</li>
<li>More complex but significantly more efficient</li>
<li>Reduced memory bandwidth usage</li>
<li>Single kernel launch</li>
<li>Benchmark results: 3116.11ms (GPU)</li>
</ul>
</li>
</ol>
<h3 id="memory-bandwidth-optimization"><a class="header" href="#memory-bandwidth-optimization">Memory bandwidth optimization</a></h3>
<ol>
<li>
<p><strong>Eliminated memory traffic</strong>:</p>
<ul>
<li>No intermediate tensor allocations between operations</li>
<li>Reduced global memory reads/writes</li>
<li>Reuse of normalized values for linear transformation</li>
<li>Memory bandwidth reduction: \[\Large \text{reduction} = \frac{\text{unfused_bandwidth} - \text{fused_bandwidth}}{\text{unfused_bandwidth}}\]</li>
</ul>
</li>
<li>
<p><strong>Cache efficiency</strong>:</p>
<ul>
<li>Better L1/L2 cache utilization</li>
<li>Reduced cache misses</li>
<li>Improved memory access patterns</li>
<li>Higher arithmetic intensity</li>
</ul>
</li>
</ol>
<h3 id="reduced-overhead"><a class="header" href="#reduced-overhead">Reduced overhead</a></h3>
<ol>
<li>
<p><strong>Kernel launch optimization</strong>:</p>
<ul>
<li>Single kernel launch instead of multiple</li>
<li>Lower driver overhead</li>
<li>Reduced synchronization points</li>
<li>Fewer memory allocations</li>
</ul>
</li>
<li>
<p><strong>Resource management</strong>:</p>
<ul>
<li>Shared memory reuse between operations</li>
<li>Better register utilization</li>
<li>Improved thread occupancy</li>
<li>Higher GPU utilization</li>
</ul>
</li>
</ol>
<h3 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance characteristics</a></h3>
<ol>
<li>
<p><strong>Scalability</strong>:</p>
<ul>
<li>Better performance scaling with input size</li>
<li>Reduced memory bandwidth bottleneck</li>
<li>More efficient use of GPU resources</li>
<li>Improved throughput for large models</li>
</ul>
</li>
<li>
<p><strong>Numerical efficiency</strong>:</p>
<ul>
<li>Maintained numerical stability</li>
<li>Reduced rounding errors</li>
<li>Better precision in intermediate results</li>
<li>Optimized computation order</li>
</ul>
</li>
</ol>
<p>üí° <strong>Key insight</strong>: Kernel fusion is particularly beneficial for operations that are frequently used together in neural networks, like LayerNorm + Linear in transformer architectures. The performance benefits become more significant with larger input sizes and more complex models.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_22/puzzle_22.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../puzzle_22/backward_pass.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_22/puzzle_22.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../puzzle_22/backward_pass.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
