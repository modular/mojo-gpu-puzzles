<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mojo 🔥 GPU Puzzles</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Mojo🔥 GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in Mojo🔥 Through Interactive Puzzles">
        <meta property="og:image" content="images/puzzle-logo.png">
        <meta property="og:url" content="https://puzzles.modular.com">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="Mojo🔥 GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in Mojo🔥 Through Interactive Puzzles">
        <meta name="twitter:image" content="images/puzzle-logo.png">
        <link rel="icon" type="image/png" href="images/puzzle-logo.png">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>

        <script>
            const is_prod = window.location.hostname.includes('modular.com');
            const script = document.createElement('script');
            if (is_prod) {
                script.src = 'https://cdn.amplitude.com/script/3878a0571d1575870a7d0a5f7e644d23.js';
                document.head.appendChild(script);
                window.amplitude.init('3878a0571d1575870a7d0a5f7e644d23', { fetchRemoteConfig: true, autocapture: true });
            } else {
                script.src = 'https://cdn.amplitude.com/script/d8bf208ebdc1b1000d38da8b826a74c4.js';
                document.head.appendChild(script);
                window.amplitude.init('d8bf208ebdc1b1000d38da8b826a74c4', { fetchRemoteConfig: true, autocapture: true });
            }
        </script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="logo-link" href="https://modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="menu-links">
                    <li><a href="https://docs.modular.com/max/get-started/" target="_blank"><div>Product</div></a></li>
                    <li><a href="https://www.modular.com/max/solutions/agent" target="_blank">Solutions</a></li>
                    <li><a href="https://builds.modular.com/?category=featured" target="_blank">Resources</a></li>
                    <li><a href="https://www.modular.com/company/about" target="_blank">Company</a></li>
                    <li><a href="https://www.modular.com/pricing" target="_blank">Pricing</a></li>
                    <li><a href="https://docs.modular.com" target="_blank">Docs</a></li>
                    <li><a href="https://www.modular.com/blog" target="_blank">Blog</a></li>
                </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <p align="center">
  <img src="images/puzzle-logo.png" alt="Mojo GPU Puzzles Logo" width="100">
</p>
<p align="center">
  <h1 align="center">Mojo🔥 GPU Puzzles</h1>
</p>
<p align="center" style="display: flex; justify-content: center; gap: 8px;">
  <a href="https://github.com/modular/mojo-gpu-puzzles">
    <img src="https://img.shields.io/badge/GitHub-Repository-181717?logo=github" alt="GitHub Repository">
  </a>
  <a href="https://docs.modular.com/mojo">
    <img src="https://img.shields.io/badge/Powered%20by-Mojo-FF5F1F" alt="Powered by Mojo">
  </a>
  <a href="https://www.modular.com/company/talk-to-us">
    <img src="https://img.shields.io/badge/Subscribe-Updates-00B5AD?logo=mail.ru" alt="Subscribe for Updates">
  </a>
  <a href="https://forum.modular.com/c/">
    <img src="https://img.shields.io/badge/Modular-Forum-9B59B6?logo=discourse" alt="Modular Forum">
  </a>
  <a href="https://discord.com/channels/1087530497313357884/1098713601386233997">
    <img src="https://img.shields.io/badge/Discord-Join_Chat-5865F2?logo=discord" alt="Discord">
  </a>
</p>
<blockquote>
<p>🚧 This book is a work in progress! Some sections may be incomplete or subject to change. 🚧</p>
</blockquote>
<blockquote>
<p><em>“For the things we have to learn before we can do them, we learn by doing them.”</em>
Aristotle, (Nicomachean Ethics)</p>
</blockquote>
<p>Welcome to <strong>Mojo 🔥 GPU Puzzles</strong>, a hands-on guide to mastering GPU programming using <a href="https://docs.modular.com/mojo/manual/">Mojo</a> 🔥 — the innovative programming language that combines Pythonic syntax with systems-level performance. GPU programming remains one of the most powerful skills in modern computing, driving advances in artificial intelligence, scientific simulation, and high-performance computing.</p>
<p>This book takes a unique approach to teaching GPU programming: learning by solving increasingly challenging puzzles. Rather than traditional textbook learning, you’ll immediately start writing real GPU code and seeing the results.</p>
<p>The early chapters of this book are heavily inspired by <a href="https://github.com/srush/GPU-Puzzles">GPU Puzzles</a>, an interactive CUDA learning project by Sasha Rush. This adaptation reimplements these concepts using Mojo’s powerful abstractions and performance capabilities, while expanding on advanced topics with Mojo-specific optimizations.</p>
<h2 id="why-mojo--for-gpu-programming"><a class="header" href="#why-mojo--for-gpu-programming">Why Mojo 🔥 for GPU Programming?</a></h2>
<p>The computing industry has reached a critical inflection point. We can no longer rely on new CPU generations to automatically increase application performance through higher clock speeds. As power and heat constraints have plateaued CPU speeds, hardware manufacturers have shifted toward increasing the number of physical cores. This multi-core revolution has reached its zenith in modern GPUs, which contain thousands of cores operating in parallel. The NVIDIA H100, for example, can run an astonishing 16,896 threads simultaneously in a single clock cycle, with over 270,000 threads queued and ready for execution.</p>
<p>Mojo represents a fresh approach to GPU programming, making this massive parallelism more accessible and productive:</p>
<ul>
<li><strong>Python-like Syntax</strong> with systems programming capabilities that feels familiar to the largest programming community</li>
<li><strong>Zero-cost Abstractions</strong> that compile to efficient machine code without sacrificing performance</li>
<li><strong>Strong Type System</strong> that catches errors at compile time while maintaining expressiveness</li>
<li><strong>Built-in Tensor Support</strong> with hardware-aware optimizations specifically designed for GPU computation</li>
<li><strong>Direct Access</strong> to low-level CPU and GPU intrinsics for systems-level programming</li>
<li><strong>Cross-Hardware Portability</strong> allowing you to write code that can run on both CPUs and GPUs</li>
<li><strong>Ergonomic and Safety Improvements</strong> over traditional C/C++ GPU programming</li>
<li><strong>Lower Barrier to Entry</strong> enabling more programmers to harness GPU power effectively</li>
</ul>
<blockquote>
<p><strong>Mojo 🔥 aims to fuel innovation by democratizing GPU programming.</strong>
<strong>By expanding on Python’s familiar syntax while adding direct GPU access, Mojo empowers programmers with minimal specialized knowledge to build high-performance, heterogeneous (CPU, GPU-enabled) applications.</strong></p>
</blockquote>
<h2 id="the-gpu-programming-mindset"><a class="header" href="#the-gpu-programming-mindset">The GPU Programming Mindset</a></h2>
<p>Effective GPU programming requires a fundamental shift in how we think about computation. Here are some key mental models that will guide your journey:</p>
<h3 id="from-sequential-to-parallel-eliminating-loops-with-threads"><a class="header" href="#from-sequential-to-parallel-eliminating-loops-with-threads">From Sequential to Parallel: Eliminating Loops with Threads</a></h3>
<p>In traditional CPU programming, we process data sequentially through loops:</p>
<pre><code class="language-python"># CPU approach
for i in range(data_size):
    result[i] = process(data[i])
</code></pre>
<p>With GPUs, we flip this model entirely. Instead of moving sequentially through data, we map thousands of parallel threads directly onto the data:</p>
<pre><code class="language-mojo"># GPU approach (conceptual)
thread_id = get_global_id()
if thread_id &lt; data_size:
    result[thread_id] = process(data[thread_id])
</code></pre>
<p>Each thread becomes responsible for computing a single element, eliminating the need for explicit loops. This mental shift—from “stepping through data” to “blanketing data with compute”—is central to GPU programming.</p>
<h3 id="fitting-a-mesh-of-compute-over-data"><a class="header" href="#fitting-a-mesh-of-compute-over-data">Fitting a Mesh of Compute Over Data</a></h3>
<p>Imagine your data as a grid, and GPU threads as another grid that overlays it. Your task is to design this “compute mesh” to efficiently cover your data:</p>
<ul>
<li><strong>Threads</strong>: Individual compute units that process single data elements</li>
<li><strong>Blocks</strong>: Organized groups of threads that share fast memory</li>
<li><strong>Grid</strong>: The entire collection of blocks that covers your dataset</li>
</ul>
<p>The art of GPU programming lies in crafting this mesh to maximize parallelism while respecting memory and synchronization constraints.</p>
<h3 id="data-movement-vs-computation"><a class="header" href="#data-movement-vs-computation">Data Movement vs. Computation</a></h3>
<p>In GPU programming, data movement is often more expensive than computation:</p>
<ul>
<li>Moving data between CPU and GPU is slow</li>
<li>Moving data between global and shared memory is faster</li>
<li>Operating on data already in registers or shared memory is extremely fast</li>
</ul>
<p>This inverts another common assumption in programming: computation is no longer the bottleneck—data movement is.</p>
<p>Through the puzzles in this book, you’ll develop an intuitive understanding of these principles, transforming how you approach computational problems.</p>
<h2 id="what-you-will-learn"><a class="header" href="#what-you-will-learn">What You Will Learn</a></h2>
<p>This book takes you on a journey from first principles to advanced GPU programming techniques. Rather than treating the GPU as a mysterious black box, we’ll build your understanding layer by layer—starting with how individual threads operate and culminating in sophisticated parallel algorithms. By mastering both low-level memory management and high-level tensor abstractions, you’ll gain the versatility to tackle any GPU programming challenge.</p>
<p>Your learning path includes:</p>
<ul>
<li><strong>GPU Programming Fundamentals</strong>: Thread organization, memory hierarchies, and kernel execution models</li>
<li><strong>Dual Implementation Paths</strong>: Beginning with raw memory approaches using pointers, then transitioning to LayoutTensor abstractions</li>
<li><strong>Memory Management</strong>: Working with global, shared, and thread-local memory for optimal performance</li>
<li><strong>Low-level to High-level Progression</strong>: Understanding the foundation with UnsafePointer before leveraging LayoutTensor’s elegant abstractions</li>
<li><strong>Layout Tensors</strong>: Mastering Mojo’s powerful tensor abstractions for simplified, efficient GPU computation</li>
<li><strong>Parallel Algorithms</strong>: Implementing and optimizing parallel reductions, convolutions, matrix operations, and more</li>
<li><strong>Performance Optimization</strong>: Advanced techniques for memory coalescing, tiling, bank conflict avoidance, and minimizing thread divergence</li>
<li><strong>Real-world Applications</strong>: Applying these concepts to machine learning, signal processing, and computational tasks</li>
</ul>
<p>The book uniquely challenges the status quo approach by first building understanding with low-level memory manipulation, then gradually transitioning to Mojo’s powerful LayoutTensor abstractions. This gives you both deep understanding of GPU memory patterns and practical knowledge of modern tensor-based approaches.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="how-to-use-this-book"><a class="header" href="#how-to-use-this-book">How to Use This Book</a></h2>
<p>Each puzzle follows a consistent format designed to progressively build your skills:</p>
<ul>
<li><strong>Overview</strong>: Clear problem statement and key concepts introduced in each puzzle</li>
<li><strong>Configuration</strong>: Setup parameters and memory organization specific to each challenge</li>
<li><strong>Code to Complete</strong>: Skeleton code with specific sections for you to implement</li>
<li><strong>Tips</strong>: Optional hints if you get stuck, without giving away complete solutions</li>
<li><strong>Solution</strong>: Detailed explanations of the implementation, performance considerations, and underlying concepts</li>
</ul>
<p>The puzzles gradually increase in complexity, introducing new concepts while reinforcing fundamentals. We recommend solving them in order, as later puzzles build on skills developed in earlier ones.</p>
<h2 id="running-the-code"><a class="header" href="#running-the-code">Running the code</a></h2>
<p>All puzzles are designed to be run with the provided testing framework that verifies your implementation against expected results. Each puzzle includes instructions for running the code and validating your solution.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<h3 id="compatible-gpu"><a class="header" href="#compatible-gpu">Compatible GPU</a></h3>
<p>You’ll need a <a href="https://docs.modular.com/max/faq#gpu-requirements">compatible GPU</a> to run the examples.</p>
<h3 id="setting-up-your-environment"><a class="header" href="#setting-up-your-environment">Setting up your environment</a></h3>
<p><a href="https://github.com/modular/mojo-gpu-puzzles">Clone the GitHub repository</a> and make sure you have the <code>magic</code> CLI installed to be able to run the Mojo programs:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/modular/mojo-gpu-puzzles
cd mojo-gpu-puzzles

# Install magic CLI (if not already installed)
curl -ssL https://magic.modular.com/ | bash

# Or update if already installed
magic self-update
</code></pre>
<h3 id="knowledge-prerequisites"><a class="header" href="#knowledge-prerequisites">Knowledge prerequisites</a></h3>
<p>Basic knowledge of:</p>
<ul>
<li>Programming fundamentals (variables, loops, conditionals, functions)</li>
<li>Parallel computing concepts (threads, synchronization, race conditions)</li>
<li>Basic familiarity with <a href="https://docs.modular.com/mojo/manual/">Mojo</a> (language basics parts and <a href="https://docs.modular.com/mojo/manual/pointers/">intro to pointers</a> section)</li>
<li><a href="https://docs.modular.com/mojo/manual/gpu/basics">A tour of GPU basics in Mojo</a> is helpful</li>
</ul>
<p>No prior GPU programming experience is necessary! We’ll build that knowledge through the puzzles.</p>
<p>Let’s begin our journey into the exciting world of GPU computing with Mojo 🔥!</p>
<h2 id="join-the-community"><a class="header" href="#join-the-community">Join the community</a></h2>
<p align="center" style="display: flex; justify-content: center; gap: 10px;">
  <a href="https://www.modular.com/company/talk-to-us">
    <img src="https://img.shields.io/badge/Subscribe-Updates-00B5AD?logo=mail.ru" alt="Subscribe for Updates">
  </a>
  <a href="https://forum.modular.com/c/">
    <img src="https://img.shields.io/badge/Modular-Forum-9B59B6?logo=discourse" alt="Modular Forum">
  </a>
  <a href="https://discord.com/channels/1087530497313357884/1098713601386233997">
    <img src="https://img.shields.io/badge/Discord-Join_Chat-5865F2?logo=discord" alt="Discord">
  </a>
</p>
<p>Join our vibrant community to discuss GPU programming, share solutions, and get help!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-1-map"><a class="header" href="#puzzle-1-map">Puzzle 1: Map</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>GPU programming is all about parallelism. In this puzzle, each thread will process a single element of the input array independently.
Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position.</em></p>
<p><img src="puzzle_01/./media/videos/720p30/puzzle_01_viz.gif" alt="Map" /></p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<ul>
<li>Basic GPU kernel structure</li>
<li>One-to-one thread to data mapping</li>
<li>Memory access patterns</li>
<li>Array operations on GPU</li>
</ul>
<p>For each position \(i\):
\[\Large out[i] = a[i] + 10\]</p>
<h2 id="what-we-cover"><a class="header" href="#what-we-cover">What we cover</a></h2>
<h3 id="-raw-memory-approach"><a class="header" href="#-raw-memory-approach"><a href="puzzle_01/./raw.html">🔰 Raw Memory Approach</a></a></h3>
<p>Start with direct memory manipulation to understand GPU fundamentals.</p>
<h3 id="-preview-modern-approach-with-layouttensor"><a class="header" href="#-preview-modern-approach-with-layouttensor"><a href="puzzle_01/./layout_tensor_preview.html">💡 Preview: Modern Approach with LayoutTensor</a></a></h3>
<p>See how LayoutTensor simplifies GPU programming with safer, cleaner code.</p>
<p>💡 <strong>Tip</strong>: Understanding both approaches helps you better appreciate modern GPU programming patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>
<p>Basic GPU kernel structure</p>
</li>
<li>
<p>Thread indexing with <code>thread_idx.x</code></p>
</li>
<li>
<p>Simple parallel operations</p>
</li>
<li>
<p><strong>Parallelism</strong>: Each thread executes independently</p>
</li>
<li>
<p><strong>Thread indexing</strong>: Access element at position <code>i = thread_idx.x</code></p>
</li>
<li>
<p><strong>Memory access</strong>: Read from <code>a[i]</code> and write to <code>out[i]</code></p>
</li>
<li>
<p><strong>Data independence</strong>: Each output depends only on its corresponding input</p>
</li>
</ul>
<h2 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = SIZE
alias dtype = DType.float32


fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):
    local_i = thread_idx.x
    # FILL ME IN (roughly 1 line)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p01/p01.mojo" class="filename">View full file: problems/p01/p01.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>local_i</code></li>
<li>Add 10 to <code>a[local_i]</code></li>
<li>Store result in <code>out[local_i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-1"><a class="header" href="#running-the-code-1">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p01
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):
    local_i = thread_idx.x
    out[local_i] = a[local_i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>local_i = thread_idx.x</code></li>
<li>Adds 10 to input value: <code>out[local_i] = a[local_i] + 10.0</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="why-consider-layouttensor"><a class="header" href="#why-consider-layouttensor">Why Consider LayoutTensor?</a></h2>
<p>Looking at our traditional implementation above, you might notice some potential issues:</p>
<h3 id="current-approach"><a class="header" href="#current-approach">Current approach</a></h3>
<pre><code class="language-mojo">local_i = thread_idx.x
out[local_i] = a[local_i] + 10.0
</code></pre>
<p>This works for 1D arrays, but what happens when we need to:</p>
<ul>
<li>Handle 2D or 3D data?</li>
<li>Deal with different memory layouts?</li>
<li>Ensure coalesced memory access?</li>
</ul>
<h3 id="preview-of-future-challenges"><a class="header" href="#preview-of-future-challenges">Preview of future challenges</a></h3>
<p>As we progress through the puzzles, array indexing will become more complex:</p>
<pre><code class="language-mojo"># 2D indexing coming in later puzzles
idx = row * WIDTH + col

# 3D indexing
idx = (batch * HEIGHT + row) * WIDTH + col

# With padding
idx = (batch * padded_height + row) * padded_width + col
</code></pre>
<h3 id="layouttensor-preview"><a class="header" href="#layouttensor-preview">LayoutTensor preview</a></h3>
<p><a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a> will help us handle these cases more elegantly:</p>
<pre><code class="language-mojo"># Future preview - don't worry about this syntax yet!
out[i, j] = a[i, j] + 10.0  # 2D indexing
out[b, i, j] = a[b, i, j] + 10.0  # 3D indexing
</code></pre>
<p>We’ll learn about LayoutTensor in detail in Puzzle 4, where these concepts become essential. For now, focus on understanding:</p>
<ul>
<li>Basic thread indexing</li>
<li>Simple memory access patterns</li>
<li>One-to-one mapping of threads to data</li>
</ul>
<p>💡 <strong>Key Takeaway</strong>: While direct indexing works for simple cases, we’ll soon need more sophisticated tools for complex GPU programming patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-2-zip"><a class="header" href="#puzzle-2-zip">Puzzle 2: Zip</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Implement a kernel that adds together each position of vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position.</em></p>
<p><img src="puzzle_02/./media/videos/720p30/puzzle_02_viz.gif" alt="Zip" /></p>
<h2 id="key-concepts-2"><a class="header" href="#key-concepts-2">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Processing multiple input arrays in parallel</li>
<li>Element-wise operations with multiple inputs</li>
<li>Thread-to-data mapping across arrays</li>
<li>Memory access patterns with multiple arrays</li>
</ul>
<p>For each thread \(i\): \[\Large out[i] = a[i] + b[i]\]</p>
<h3 id="memory-access-pattern"><a class="header" href="#memory-access-pattern">Memory access pattern</a></h3>
<pre><code class="language-txt">Thread 0:  a[0] + b[0] → out[0]
Thread 1:  a[1] + b[1] → out[1]
Thread 2:  a[2] + b[2] → out[2]
...
</code></pre>
<p>💡 <strong>Note</strong>: Notice how we’re now managing three arrays (<code>a</code>, <code>b</code>, <code>out</code>) in our kernel. As we progress to more complex operations, managing multiple array accesses will become increasingly challenging.</p>
<h2 id="code-to-complete-1"><a class="header" href="#code-to-complete-1">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = SIZE
alias dtype = DType.float32


fn add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
):
    local_i = thread_idx.x
    # FILL ME IN (roughly 1 line)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p02/p02.mojo" class="filename">View full file: problems/p02/p02.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>local_i</code></li>
<li>Add <code>a[local_i]</code> and <code>b[local_i]</code></li>
<li>Store result in <code>out[local_i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-2"><a class="header" href="#running-the-code-2">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p02
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 2.0, 4.0, 6.0])
</code></pre>
<h2 id="solution-1"><a class="header" href="#solution-1">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
):
    local_i = thread_idx.x
    out[local_i] = a[local_i] + b[local_i]


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>local_i = thread_idx.x</code></li>
<li>Adds values from both arrays: <code>out[local_i] = a[local_i] + b[local_i]</code></li>
</ul>
</div>
</details>
<h3 id="looking-ahead"><a class="header" href="#looking-ahead">Looking ahead</a></h3>
<p>While this direct indexing works for simple element-wise operations, consider:</p>
<ul>
<li>What if arrays have different layouts?</li>
<li>What if we need to broadcast one array to another?</li>
<li>How to ensure coalesced access across multiple arrays?</li>
</ul>
<p>These questions will be addressed when we <a href="puzzle_02/../puzzle_04/">introduce LayoutTensor in Puzzle 4</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-3-guards"><a class="header" href="#puzzle-3-guards">Puzzle 3: Guards</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note</strong>: <em>You have more threads than positions. This means you need to protect against out-of-bounds memory access.</em></p>
<p><img src="puzzle_03/./media/videos/720p30/puzzle_03_viz.gif" alt="Guard" /></p>
<h2 id="key-concepts-3"><a class="header" href="#key-concepts-3">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Handling thread/data size mismatches</li>
<li>Preventing out-of-bounds memory access</li>
<li>Using conditional execution in GPU kernels</li>
<li>Safe memory access patterns</li>
</ul>
<h3 id="mathematical-description"><a class="header" href="#mathematical-description">Mathematical Description</a></h3>
<p>For each thread \(i\):
\[\Large \text{if}\ i &lt; \text{size}: out[i] = a[i] + 10\]</p>
<h3 id="memory-safety-pattern"><a class="header" href="#memory-safety-pattern">Memory Safety Pattern</a></h3>
<pre><code class="language-txt">Thread 0 (i=0):  if 0 &lt; size:  out[0] = a[0] + 10  ✓ Valid
Thread 1 (i=1):  if 1 &lt; size:  out[1] = a[1] + 10  ✓ Valid
Thread 2 (i=2):  if 2 &lt; size:  out[2] = a[2] + 10  ✓ Valid
Thread 3 (i=3):  if 3 &lt; size:  out[3] = a[3] + 10  ✓ Valid
Thread 4 (i=4):  if 4 &lt; size:  ❌ Skip (out of bounds)
Thread 5 (i=5):  if 5 &lt; size:  ❌ Skip (out of bounds)
</code></pre>
<p>💡 <strong>Note</strong>: Boundary checking becomes increasingly complex with:</p>
<ul>
<li>Multi-dimensional arrays</li>
<li>Different array shapes</li>
<li>Complex access patterns</li>
</ul>
<h2 id="code-to-complete-2"><a class="header" href="#code-to-complete-2">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (8, 1)
alias dtype = DType.float32


fn add_10_guard(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p03/p03.mojo" class="filename">View full file: problems/p03/p03.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>local_i</code></li>
<li>Add guard: <code>if local_i &lt; size</code></li>
<li>Inside guard: <code>out[local_i] = a[local_i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-3"><a class="header" href="#running-the-code-3">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p03
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-2"><a class="header" href="#solution-2">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_guard(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    if local_i &lt; size:
        out[local_i] = a[local_i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>local_i = thread_idx.x</code></li>
<li>Guards against out-of-bounds access with <code>if local_i &lt; size</code></li>
<li>Inside guard: adds 10 to input value</li>
</ul>
</div>
</details>
<h3 id="looking-ahead-1"><a class="header" href="#looking-ahead-1">Looking ahead</a></h3>
<p>While simple boundary checks work here, consider these challenges:</p>
<ul>
<li>What about 2D/3D array boundaries?</li>
<li>How to handle different shapes efficiently?</li>
<li>What if we need padding or edge handling?</li>
</ul>
<p>Example of growing complexity:</p>
<pre><code class="language-mojo"># Current: 1D bounds check
if i &lt; size: ...

# Coming soon: 2D bounds check
if i &lt; height and j &lt; width: ...

# Later: 3D with padding
if i &lt; height and j &lt; width and k &lt; depth and
   i &gt;= padding and j &gt;= padding: ...
</code></pre>
<p>These boundary handling patterns will become more elegant when we <a href="puzzle_03/../puzzle_04/">learn about LayoutTensor in Puzzle 4</a>, which provides built-in boundary checking and shape management.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-4-2d-map"><a class="header" href="#puzzle-4-2d-map">Puzzle 4: 2D Map</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D square matrix <code>a</code> and stores it in 2D square matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions</em>.</p>
<p><img src="puzzle_04/./media/videos/720p30/puzzle_04_viz.gif" alt="2D Matrix Mapping" /></p>
<h2 id="key-concepts-4"><a class="header" href="#key-concepts-4">Key concepts</a></h2>
<ul>
<li>2D thread indexing</li>
<li>Matrix operations on GPU</li>
<li>Handling excess threads</li>
<li>Memory layout patterns</li>
</ul>
<p>For each position \((i,j)\):
\[\Large out[i,j] = a[i,j] + 10\]</p>
<h2 id="implementation-approaches"><a class="header" href="#implementation-approaches">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-1"><a class="header" href="#-raw-memory-approach-1"><a href="puzzle_04/./raw.html">🔰 Raw memory approach</a></a></h3>
<p>Learn how 2D indexing works with manual memory management.</p>
<h3 id="-learn-about-layouttensor"><a class="header" href="#-learn-about-layouttensor"><a href="puzzle_04/./introduction_layout_tensor.html">📚 Learn about LayoutTensor</a></a></h3>
<p>Discover a powerful abstraction that simplifies multi-dimensional array operations and memory management on GPU.</p>
<h3 id="-modern-2d-operations"><a class="header" href="#-modern-2d-operations"><a href="puzzle_04/./layout_tensor.html">🚀 Modern 2D operations</a></a></h3>
<p>Put LayoutTensor into practice with natural 2D indexing and automatic bounds checking.</p>
<p>💡 <strong>Note</strong>: From this puzzle onward, we’ll primarily use LayoutTensor for cleaner, safer GPU code.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-5"><a class="header" href="#key-concepts-5">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Working with 2D thread indices (<code>thread_idx.x</code>, <code>thread_idx.y</code>)</li>
<li>Converting 2D coordinates to 1D memory indices</li>
<li>Handling boundary checks in two dimensions</li>
</ul>
<p>The key insight is understanding how to map from 2D thread coordinates \((i,j)\) to elements in a row-major matrix of size \(n \times n\), while ensuring thread indices are within bounds.</p>
<ul>
<li><strong>2D indexing</strong>: Each thread has a unique \((i,j)\) position</li>
<li><strong>Memory layout</strong>: Row-major ordering maps 2D to 1D memory</li>
<li><strong>Guard condition</strong>: Need bounds checking in both dimensions</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than matrix elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-3"><a class="header" href="#code-to-complete-3">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn add_10_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p04/p04.mojo" class="filename">View full file: problems/p04/p04.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Add guard: <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Inside guard: <code>out[local_j * size + local_i] = a[local_j * size + local_i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-4"><a class="header" href="#running-the-code-4">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p04
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-3"><a class="header" href="#solution-3">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    if local_i &lt; size and local_j &lt; size:
        out[local_j * size + local_i] = a[local_j * size + local_i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets 2D thread indices with <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Guards against out-of-bounds with <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Inside guard: adds 10 to input value using row-major indexing</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-layouttensor"><a class="header" href="#introduction-to-layouttensor">Introduction to LayoutTensor</a></h1>
<p>After dealing with manual indexing, bounds checking, and growing complexity in the previous puzzles, it’s time to introduce a powerful abstraction that will make GPU programming more intuitive and safer.</p>
<h2 id="why-layouttensor"><a class="header" href="#why-layouttensor">Why <a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a>?</a></h2>
<p>Let’s look at the challenges we’ve faced:</p>
<pre><code class="language-mojo"># Puzzle 1: Simple indexing
out[local_i] = a[local_i] + 10.0

# Puzzle 2: Multiple array management
out[local_i] = a[local_i] + b[local_i]

# Puzzle 3: Bounds checking
if local_i &lt; size:
    out[local_i] = a[local_i] + 10.0
</code></pre>
<p>As dimensions grow, code becomes more complex:</p>
<pre><code class="language-mojo"># Traditional 2D indexing for row-major 2D matrix
idx = col * WIDTH + row
if row &lt; height and col &lt; width:
    out[idx] = a[idx] + 10.0
</code></pre>
<h2 id="the-layouttensor-solution"><a class="header" href="#the-layouttensor-solution">The LayoutTensor solution</a></h2>
<p><a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a> provides:</p>
<ol>
<li><strong>Natural Indexing</strong>: Use <code>tensor[i, j]</code> instead of manual offset calculations</li>
<li><strong>Automatic Bounds Checking</strong>: Built-in protection against out-of-bounds access</li>
<li><strong>Flexible Memory Layouts</strong>: Support for row-major, column-major, and tiled organizations</li>
<li><strong>Performance Optimization</strong>: Efficient memory access patterns for GPU</li>
</ol>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic usage</a></h3>
<pre><code class="language-mojo">from layout import Layout, LayoutTensor

# Define layout
alias HEIGHT = 2
alias WIDTH = 3
alias layout = Layout.row_major(HEIGHT, WIDTH)

# Create tensor
tensor = LayoutTensor[dtype, layout](buffer.unsafe_ptr())

# Access elements naturally
tensor[0, 0] = 1.0  # First element
tensor[1, 2] = 2.0  # Last element
</code></pre>
<h3 id="memory-layout-control"><a class="header" href="#memory-layout-control">Memory layout control</a></h3>
<p>LayoutTensor supports different memory organizations:</p>
<pre><code class="language-mojo"># Row-major (default)
layout_row = Layout.row_major(HEIGHT, WIDTH)

# Column-major
layout_col = Layout.col_major(HEIGHT, WIDTH)

# Tiled (for better cache utilization)
layout_tiled = tensor.tiled[4, 4](HEIGHT, WIDTH)
</code></pre>
<h3 id="understanding-memory-layouts"><a class="header" href="#understanding-memory-layouts">Understanding memory layouts</a></h3>
<p>Memory layout affects performance dramatically. LayoutTensor supports:</p>
<ul>
<li>
<p><strong>Row-major</strong>: Elements in a row are contiguous</p>
<pre><code class="language-mojo"># [1 2 3]
# [4 5 6] -&gt; [1 2 3 4 5 6]
layout_row = Layout.row_major(2, 3)
</code></pre>
</li>
<li>
<p><strong>Column-major</strong>: Elements in a column are contiguous</p>
<pre><code class="language-mojo"># [1 2 3]
# [4 5 6] -&gt; [1 4 2 5 3 6]
layout_col = Layout.col_major(2, 3)
</code></pre>
</li>
<li>
<p><strong>Tiled</strong>: Elements grouped in tiles for cache efficiency</p>
<pre><code class="language-mojo"># [[1 2] [3 4]] in 2x2 tiles
layout_tiled = Layout.tiled[2, 2](4, 4)
</code></pre>
</li>
</ul>
<h3 id="benefits-over-traditional-approach"><a class="header" href="#benefits-over-traditional-approach">Benefits over traditional approach</a></h3>
<ol>
<li>
<p><strong>Readability</strong>:</p>
<pre><code class="language-mojo"># Traditional
out[col * WIDTH + row] = a[col * WIDTH + row] + 10.0

# LayoutTensor
out[row, col] = a[row, col] + 10.0
</code></pre>
</li>
<li>
<p><strong>Flexibility</strong>:</p>
<ul>
<li>Easy to change memory layouts without modifying computation code</li>
<li>Support for complex access patterns</li>
<li>Built-in optimizations</li>
</ul>
</li>
</ol>
<h2 id="advanced-features-preview"><a class="header" href="#advanced-features-preview">Advanced features preview</a></h2>
<p>While we’ll start with basic operations, LayoutTensor’s true power shines in advanced GPU optimizations:</p>
<h3 id="1-memory-hierarchy-management"><a class="header" href="#1-memory-hierarchy-management">1. Memory hierarchy management</a></h3>
<pre><code class="language-mojo"># Shared memory allocation
shared_mem = tb[dtype]().row_major[BM, BK]().shared().alloc()

# Register allocation
reg_tile = tb[dtype]().row_major[TM, TN]().local().alloc()
</code></pre>
<h3 id="2-tiling-strategies"><a class="header" href="#2-tiling-strategies">2. Tiling strategies</a></h3>
<pre><code class="language-mojo"># Block tiling
block_tile = tensor.tile[BM, BN](block_idx.y, block_idx.x)

# Register tiling
reg_tile = block_tile.tile[TM, TN](thread_row, thread_col)
</code></pre>
<h3 id="3-memory-access-patterns"><a class="header" href="#3-memory-access-patterns">3. Memory access patterns</a></h3>
<pre><code class="language-mojo"># Vectorized access
vec_tensor = tensor.vectorize[1, simd_width]()

# Asynchronous transfers
copy_dram_to_sram_async[thread_layout=layout](dst, src)
</code></pre>
<h3 id="4-hardware-acceleration"><a class="header" href="#4-hardware-acceleration">4. Hardware acceleration</a></h3>
<pre><code class="language-mojo"># Tensor Core operations (coming in later puzzles)
mma_op = TensorCore[dtype, out_type, Index(M, N, K)]()
result = mma_op.mma_op(a_reg, b_reg, c_reg)
</code></pre>
<p>💡 <strong>Looking Ahead</strong>: As we progress through the puzzles, you’ll learn how to:</p>
<ul>
<li>Use shared memory for faster data access</li>
<li>Implement efficient tiling strategies</li>
<li>Leverage vectorized operations</li>
<li>Utilize hardware accelerators</li>
<li>Optimize memory access patterns</li>
</ul>
<p>Each puzzle will introduce these concepts gradually, building on the fundamentals to create highly optimized GPU code.</p>
<p>Ready to start your journey from basic operations to advanced GPU programming? Let’s begin with the fundamentals!</p>
<h2 id="quick-example"><a class="header" href="#quick-example">Quick example</a></h2>
<p>Let’s put everything together with a simple example that demonstrates the basics of LayoutTensor:</p>
<pre><code class="language-mojo">from gpu.host import DeviceContext
from layout import Layout, LayoutTensor

alias HEIGHT = 2
alias WIDTH = 3
alias dtype = DType.float32
alias layout = Layout.row_major(HEIGHT, WIDTH)

fn kernel[dtype: DType, layout: Layout](tensor: LayoutTensor[mut=True, dtype, layout]):
    print("Before:")
    print(tensor)
    tensor[0, 0] += 1
    print("After:")
    print(tensor)

def main():
    ctx = DeviceContext()

    a = ctx.enqueue_create_buffer[dtype](HEIGHT * WIDTH).enqueue_fill(0)
    tensor = LayoutTensor[mut=True, dtype, layout](a.unsafe_ptr())
    # Note: since `tensor` is a device tensor we can't print it without the kernel wrapper
    ctx.enqueue_function[kernel[dtype, layout]](tensor, grid_dim=1, block_dim=1)

    ctx.synchronize()
</code></pre>
<p>When we run this code with <code>magic run layout_tensor_intro</code>, we see:</p>
<pre><code class="language-txt">Before:
0.0 0.0 0.0
0.0 0.0 0.0
After:
1.0 0.0 0.0
0.0 0.0 0.0
</code></pre>
<p>Let’s break down what’s happening:</p>
<ol>
<li>We create a <code>2 x 3</code> tensor with row-major layout</li>
<li>Initially, all elements are zero</li>
<li>Using natural indexing, we modify a single element</li>
<li>The change is reflected in our output</li>
</ol>
<p>This simple example demonstrates key LayoutTensor benefits:</p>
<ul>
<li>Clean syntax for tensor creation and access</li>
<li>Automatic memory layout handling</li>
<li>Built-in bounds checking</li>
<li>Natural multi-dimensional indexing</li>
</ul>
<p>While this example is straightforward, the same patterns will scale to complex GPU operations in upcoming puzzles. You’ll see how these basic concepts extend to:</p>
<ul>
<li>Multi-threaded GPU operations</li>
<li>Shared memory optimizations</li>
<li>Complex tiling strategies</li>
<li>Hardware-accelerated computations</li>
</ul>
<p>Ready to start your GPU programming journey with LayoutTensor? Let’s dive into the puzzles!</p>
<p>💡 <strong>Tip</strong>: Keep this example in mind as we progress - we’ll build upon these fundamental concepts to create increasingly sophisticated GPU programs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version"><a class="header" href="#layouttensor-version">LayoutTensor Version</a></h1>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D <em>LayoutTensor</em> <code>a</code> and stores it in 2D <em>LayoutTensor</em> <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions</em>.</p>
<h2 id="key-concepts-6"><a class="header" href="#key-concepts-6">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> for 2D array access</li>
<li>Direct 2D indexing with <code>tensor[i, j]</code></li>
<li>Handling bounds checking with <code>LayoutTensor</code></li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> provides a natural 2D indexing interface, abstracting away the underlying memory layout while still requiring bounds checking.</p>
<ul>
<li><strong>2D access</strong>: Natural \((i,j)\) indexing with <code>LayoutTensor</code></li>
<li><strong>Memory abstraction</strong>: No manual row-major calculation needed</li>
<li><strong>Guard condition</strong>: Still need bounds checking in both dimensions</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than tensor elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-4"><a class="header" href="#code-to-complete-4">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE, SIZE)


fn add_10_2d(
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p04/p04_layout_tensor.mojo" class="filename">View full file: problems/p04/p04_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Add guard: <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Inside guard: <code>out[local_i, local_j] = a[local_i, local_j] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-5"><a class="header" href="#running-the-code-5">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p04_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-4"><a class="header" href="#solution-4">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_2d(
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    if local_i &lt; size and local_j &lt; size:
        out[local_i, local_j] = a[local_i, local_j] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets 2D thread indices with <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Guards against out-of-bounds with <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Uses <code>LayoutTensor</code>’s 2D indexing: <code>out[local_i, local_j] = a[local_i, local_j] + 10.0</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-5-broadcast"><a class="header" href="#puzzle-5-broadcast">Puzzle 5: Broadcast</a></h1>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>Implement a kernel that broadcast adds vector <code>a</code> and vector <code>b</code> and stores it in 2D matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions.</em></p>
<p><img src="puzzle_05/./media/videos/720p30/puzzle_05_viz.gif" alt="Broadcast visualization" /></p>
<h2 id="key-concepts-7"><a class="header" href="#key-concepts-7">Key concepts</a></h2>
<ul>
<li>Broadcasting vectors to matrix</li>
<li>2D thread management</li>
<li>Mixed dimension operations</li>
<li>Memory layout patterns</li>
</ul>
<h2 id="implementation-approaches-1"><a class="header" href="#implementation-approaches-1">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-2"><a class="header" href="#-raw-memory-approach-2"><a href="puzzle_05/./raw.html">🔰 Raw memory approach</a></a></h3>
<p>Learn how to handle broadcasting with manual memory indexing.</p>
<h3 id="-layouttensor-version"><a class="header" href="#-layouttensor-version"><a href="puzzle_05/./layout_tensor.html">📐 LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor to handle mixed-dimension operations.</p>
<p>💡 <strong>Note</strong>: Notice how LayoutTensor simplifies broadcasting compared to manual indexing.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-8"><a class="header" href="#key-concepts-8">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Broadcasting 1D vectors across different dimensions</li>
<li>Using 2D thread indices for broadcast operations</li>
<li>Handling boundary conditions in broadcast patterns</li>
</ul>
<p>The key insight is understanding how to map elements from two 1D vectors to create a 2D output matrix through broadcasting, while handling thread bounds correctly.</p>
<ul>
<li><strong>Broadcasting</strong>: Each element of <code>a</code> combines with each element of <code>b</code></li>
<li><strong>Thread mapping</strong>: 2D thread grid \((3 \times 3)\) for \(2 \times 2\) output</li>
<li><strong>Vector access</strong>: Different access patterns for <code>a</code> and <code>b</code></li>
<li><strong>Bounds checking</strong>: Guard against threads outside matrix dimensions</li>
</ul>
<h2 id="code-to-complete-5"><a class="header" href="#code-to-complete-5">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn broadcast_add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p05/p05.mojo" class="filename">View full file: problems/p05/p05.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Add guard: <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Inside guard: <code>out[local_j * size + local_i] = a[local_i] + b[local_j]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-6"><a class="header" href="#running-the-code-6">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p05
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 1.0, 2.0])
</code></pre>
<h2 id="solution-5"><a class="header" href="#solution-5">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn broadcast_add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    if local_i &lt; size and local_j &lt; size:
        out[local_j * size + local_i] = a[local_i] + b[local_j]


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets 2D thread indices with <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Guards against out-of-bounds with <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Broadcasts by adding <code>a[local_i]</code> and <code>b[local_j]</code> into the output matrix</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version-1"><a class="header" href="#layouttensor-version-1">LayoutTensor Version</a></h1>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>Implement a kernel that broadcast adds <em>LayoutTensor</em> vector <code>a</code> and <em>LayoutTensor</em> vector <code>b</code> and stores it in <em>LayoutTensor</em> <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions.</em></p>
<h2 id="key-concepts-9"><a class="header" href="#key-concepts-9">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> for broadcast operations</li>
<li>Working with different tensor shapes</li>
<li>Handling 2D indexing with <code>LayoutTensor</code></li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> allows natural broadcasting through different tensor shapes: \((n,1)\) and \((1,n)\) to \((n,n)\), while still requiring bounds checking.</p>
<ul>
<li><strong>Tensor shapes</strong>: Input vectors have shapes \((n,1)\) and \((1,n)\)</li>
<li><strong>Broadcasting</strong>: Output combines both dimensions to \((n,n)\)</li>
<li><strong>Guard condition</strong>: Still need bounds checking for output size</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than tensor elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-6"><a class="header" href="#code-to-complete-6">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias out_layout = Layout.row_major(SIZE, SIZE)
alias a_layout = Layout.row_major(SIZE, 1)
alias b_layout = Layout.row_major(1, SIZE)


fn broadcast_add[
    out_layout: Layout,
    a_layout: Layout,
    b_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, a_layout],
    b: LayoutTensor[mut=True, dtype, b_layout],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p05/p05_layout_tensor.mojo" class="filename">View full file: problems/p05/p05_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Add guard: <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Inside guard: <code>out[local_i, local_j] = a[local_i, 0] + b[0, local_j]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-7"><a class="header" href="#running-the-code-7">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p05_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 1.0, 2.0])
</code></pre>
<h2 id="solution-6"><a class="header" href="#solution-6">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn broadcast_add[
    out_layout: Layout,
    a_layout: Layout,
    b_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, a_layout],
    b: LayoutTensor[mut=True, dtype, b_layout],
    size: Int,
):
    local_i = thread_idx.x
    local_j = thread_idx.y
    if local_i &lt; size and local_j &lt; size:
        out[local_i, local_j] = a[local_i, 0] + b[0, local_j]


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets 2D thread indices with <code>local_i = thread_idx.x</code>, <code>local_j = thread_idx.y</code></li>
<li>Guards against out-of-bounds with <code>if local_i &lt; size and local_j &lt; size</code></li>
<li>Uses <code>LayoutTensor</code> broadcasting: <code>a[local_i, 0] + b[0, local_j]</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-6-blocks"><a class="header" href="#puzzle-6-blocks">Puzzle 6: Blocks</a></h1>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of a.</em></p>
<p><img src="puzzle_06/./media/videos/720p30/puzzle_06_viz.gif" alt="Blocks visualization" /></p>
<h2 id="key-concepts-10"><a class="header" href="#key-concepts-10">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Processing data larger than thread block size</li>
<li>Coordinating multiple blocks of threads</li>
<li>Computing global thread positions</li>
</ul>
<p>The key insight is understanding how blocks of threads work together to process data that’s larger than a single block’s capacity, while maintaining correct element-to-thread mapping.</p>
<h2 id="code-to-complete-7"><a class="header" href="#code-to-complete-7">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 9
alias BLOCKS_PER_GRID = (3, 1)
alias THREADS_PER_BLOCK = (4, 1)
alias dtype = DType.float32


fn add_10_blocks(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p06/p06.mojo" class="filename">View full file: problems/p06/p06.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global index: <code>global_i = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if global_i &lt; size</code></li>
<li>Inside guard: <code>out[global_i] = a[global_i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-8"><a class="header" href="#running-the-code-8">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p06
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0])
</code></pre>
<h2 id="solution-7"><a class="header" href="#solution-7">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    if global_i &lt; size:
        out[global_i] = a[global_i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Computes global thread index from block and thread indices</li>
<li>Guards against out-of-bounds with <code>if global_i &lt; size</code></li>
<li>Inside guard: adds 10 to input value at global index</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-7-2d-blocks"><a class="header" href="#puzzle-7-2d-blocks">Puzzle 7: 2D Blocks</a></h1>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of matrix <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code> in both directions.</em></p>
<p><img src="puzzle_07/./media/videos/720p30/puzzle_07_viz.gif" alt="Blocks 2D visualization" /></p>
<h2 id="key-concepts-11"><a class="header" href="#key-concepts-11">Key concepts</a></h2>
<ul>
<li>Block-based processing</li>
<li>Grid-block coordination</li>
<li>Multi-block indexing</li>
<li>Memory access patterns</li>
</ul>
<h2 id="implementation-approaches-2"><a class="header" href="#implementation-approaches-2">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-3"><a class="header" href="#-raw-memory-approach-3"><a href="puzzle_07/./raw.html">🔰 Raw memory approach</a></a></h3>
<p>Learn how to handle multi-block operations with manual indexing.</p>
<h3 id="-layouttensor-version-1"><a class="header" href="#-layouttensor-version-1"><a href="puzzle_07/./layout_tensor.html">📐 LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor features to elegantly handle block-based processing.</p>
<p>💡 <strong>Note</strong>: See how LayoutTensor simplifies block coordination and memory access patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-12"><a class="header" href="#key-concepts-12">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Working with 2D block and thread arrangements</li>
<li>Handling matrix data larger than block size</li>
<li>Converting between 2D and linear memory access</li>
</ul>
<p>The key insight is understanding how to coordinate multiple blocks of threads to process a 2D matrix that’s larger than a single block’s dimensions.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li><strong>Matrix size</strong>: \(5 \times 5\) elements</li>
<li><strong>2D blocks</strong>: Each block processes a \(3 \times 3\) region</li>
<li><strong>Grid layout</strong>: Blocks arranged in \(2 \times 2\) grid</li>
<li><strong>Total threads</strong>: \(36\) for \(25\) elements</li>
<li><strong>Memory pattern</strong>: Row-major storage for 2D data</li>
<li><strong>Coverage</strong>: Ensuring all matrix elements are processed</li>
</ul>
<h2 id="code-to-complete-8"><a class="header" href="#code-to-complete-8">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 5
alias BLOCKS_PER_GRID = (2, 2)
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn add_10_blocks_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p07/p07.mojo" class="filename">View full file: problems/p07/p07.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global indices: <code>global_i = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if global_i &lt; size and global_j &lt; size</code></li>
<li>Inside guard: <code>out[global_j * size + global_i] = a[global_j * size + global_i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-9"><a class="header" href="#running-the-code-9">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p07
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, ... , 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, ... , 11.0])
</code></pre>
<h2 id="solution-8"><a class="header" href="#solution-8">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    if global_i &lt; size and global_j &lt; size:
        out[global_j * size + global_i] = a[global_j * size + global_i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Computes global indices with <code>block_dim * block_idx + thread_idx</code></li>
<li>Guards against out-of-bounds with <code>if global_i &lt; size and global_j &lt; size</code></li>
<li>Uses row-major indexing to access and update matrix elements</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version-2"><a class="header" href="#layouttensor-version-2">LayoutTensor Version</a></h1>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D LayoutTensor <code>a</code> and stores it in 2D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code> in both directions.</em></p>
<h2 id="key-concepts-13"><a class="header" href="#key-concepts-13">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> with multiple blocks</li>
<li>Handling large matrices with 2D block organization</li>
<li>Combining block indexing with <code>LayoutTensor</code> access</li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> simplifies 2D indexing while still requiring proper block coordination for large matrices.</p>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<ul>
<li><strong>Matrix size</strong>: \(5 \times 5\) elements</li>
<li><strong>Layout handling</strong>: <code>LayoutTensor</code> manages row-major organization</li>
<li><strong>Block coordination</strong>: Multiple blocks cover the full matrix</li>
<li><strong>2D indexing</strong>: Natural \((i,j)\) access with bounds checking</li>
<li><strong>Total threads</strong>: \(36\) for \(25\) elements</li>
<li><strong>Thread mapping</strong>: Each thread processes one matrix element</li>
</ul>
<h2 id="code-to-complete-9"><a class="header" href="#code-to-complete-9">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 5
alias BLOCKS_PER_GRID = (2, 2)
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias out_layout = Layout.row_major(SIZE, SIZE)
alias a_layout = Layout.row_major(SIZE, 1)


fn add_10_blocks_2d[
    out_layout: Layout,
    a_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, a_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p07/p07_layout_tensor.mojo" class="filename">View full file: problems/p07/p07_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global indices: <code>global_i = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if global_i &lt; size and global_j &lt; size</code></li>
<li>Inside guard: <code>out[global_i, global_j] = a[global_i, global_j] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-10"><a class="header" href="#running-the-code-10">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p07_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, ... , 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, ... , 11.0])
</code></pre>
<h2 id="solution-9"><a class="header" href="#solution-9">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks_2d[
    out_layout: Layout,
    a_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, a_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    if global_i &lt; size and global_j &lt; size:
        out[global_i, global_j] = a[global_i, global_j] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Computes global indices with <code>block_dim * block_idx + thread_idx</code></li>
<li>Guards against out-of-bounds with <code>if global_i &lt; size and global_j &lt; size</code></li>
<li>Uses <code>LayoutTensor</code>’s 2D indexing: <code>out[global_i, global_j] = a[global_i, global_j] + 10.0</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-8-shared-memory"><a class="header" href="#puzzle-8-shared-memory">Puzzle 8: Shared Memory</a></h1>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code>.</em></p>
<p><img src="puzzle_08/./media/videos/720p30/puzzle_08_viz.gif" alt="Shared memory visualization" /></p>
<h2 id="implementation-approaches-3"><a class="header" href="#implementation-approaches-3">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-4"><a class="header" href="#-raw-memory-approach-4"><a href="puzzle_08/./raw.html">🔰 Raw memory approach</a></a></h3>
<p>Learn how to manually manage shared memory and synchronization.</p>
<h3 id="-layouttensor-version-2"><a class="header" href="#-layouttensor-version-2"><a href="puzzle_08/./layout_tensor.html">📐 LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor’s built-in shared memory management features.</p>
<p>💡 <strong>Note</strong>: Experience how LayoutTensor simplifies shared memory operations while maintaining performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-14"><a class="header" href="#key-concepts-14">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using shared memory within thread blocks</li>
<li>Synchronizing threads with barriers</li>
<li>Managing block-local data storage</li>
</ul>
<p>The key insight is understanding how shared memory provides fast, block-local storage that all threads in a block can access, requiring careful coordination between threads.</p>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 4</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Shared memory</strong>: Fast storage shared by threads in a block</li>
<li><strong>Thread sync</strong>: Coordination using <code>barrier()</code></li>
<li><strong>Memory scope</strong>: Shared memory only visible within block</li>
<li><strong>Access pattern</strong>: Local vs global indexing</li>
</ul>
<blockquote>
<p><strong>Warning</strong>: Each block can only have a <em>constant</em> amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant, not a variable. After writing to shared memory you need to call <a href="https://docs.modular.com/mojo/stdlib/gpu/sync/barrier/">barrier</a> to ensure that threads do not cross.</p>
</blockquote>
<h2 id="code-to-complete-10"><a class="header" href="#code-to-complete-10">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 4
alias SIZE = 8
alias BLOCKS_PER_GRID = (2, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32


fn add_10_shared(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB * sizeof[dtype](),
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # local data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # wait for all threads to complete
    # works within a thread block
    barrier()

    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p08/p08.mojo" class="filename">View full file: problems/p08/p08.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Wait for shared memory load with <code>barrier()</code></li>
<li>Use <code>local_i</code> to access shared memory: <code>shared[local_i]</code></li>
<li>Use <code>global_i</code> for output: <code>out[global_i]</code></li>
<li>Add guard: <code>if global_i &lt; size</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-11"><a class="header" href="#running-the-code-11">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p08
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])
</code></pre>
<h2 id="solution-10"><a class="header" href="#solution-10">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_shared(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB * sizeof[dtype](),
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # local data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # wait for all threads to complete
    # works within a thread block
    barrier()

    # process using shared memory
    if global_i &lt; size:
        out[global_i] = shared[local_i] + 10


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Waits for shared memory load with <code>barrier()</code></li>
<li>Guards against out-of-bounds with <code>if global_i &lt; size</code></li>
<li>Reads from shared memory using <code>shared[local_i]</code></li>
<li>Writes result to global memory at <code>out[global_i]</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-15"><a class="header" href="#key-concepts-15">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using LayoutTensor’s shared memory features</li>
<li>Thread synchronization with shared memory</li>
<li>Block-local data management with tensor builder</li>
</ul>
<p>The key insight is how LayoutTensor simplifies shared memory management while maintaining the performance benefits of block-local storage.</p>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 4</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<h2 id="key-differences-from-raw-approach"><a class="header" href="#key-differences-from-raw-approach">Key differences from raw approach</a></h2>
<ol>
<li>
<p><strong>Memory allocation</strong>: We will use <a href="https://docs.modular.com/mojo/stdlib/layout/tensor_builder/LayoutTensorBuild">LayoutTensorBuild</a> instead of <a href="https://docs.modular.com/mojo/stdlib/memory/memory/stack_allocation/">stack_allocation</a></p>
<pre><code class="language-mojo"># Raw approach
shared = stack_allocation[TPB * sizeof[dtype](), ...]()

# LayoutTensor approach
shared = LayoutTensorBuild[dtype]().row_major[TPB]().shared().alloc()
</code></pre>
</li>
<li>
<p><strong>Memory access</strong>: Same syntax</p>
<pre><code class="language-mojo"># Raw approach
shared[local_i] = a[global_i]

# LayoutTensor approach
shared[local_i] = a[global_i]
</code></pre>
</li>
<li>
<p><strong>Safety features</strong>:</p>
<ul>
<li>Type safety</li>
<li>Layout management</li>
<li>Memory alignment handling</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Note</strong>: LayoutTensor handles memory layout, but you still need to manage thread synchronization with <code>barrier()</code> when using shared memory.</p>
</blockquote>
<h2 id="code-to-complete-11"><a class="header" href="#code-to-complete-11">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 4
alias SIZE = 8
alias BLOCKS_PER_GRID = (2, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn add_10_shared_layout_tensor[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p08/p08_layout_tensor.mojo" class="filename">View full file: problems/p08/p08_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Load data with natural indexing: <code>shared[local_i] = a[global_i]</code></li>
<li>Synchronize with <code>barrier()</code></li>
<li>Process data using shared memory indices</li>
<li>Guard against out-of-bounds access</li>
</ol>
</div>
</details>
<h2 id="running-the-code-12"><a class="header" href="#running-the-code-12">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p08_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])
</code></pre>
<h2 id="solution-11"><a class="header" href="#solution-11">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_shared_layout_tensor[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    if global_i &lt; size:
        out[global_i] = shared[local_i] + 10


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Creates shared memory using tensor builder’s fluent API</li>
<li>Guards against out-of-bounds with <code>if global_i &lt; size</code></li>
<li>Uses natural indexing for both shared and global memory</li>
<li>Ensures thread synchronization with <code>barrier()</code></li>
<li>Leverages LayoutTensor’s built-in safety features</li>
</ul>
<p>Key steps:</p>
<ol>
<li>Allocate shared memory with proper layout</li>
<li>Load global data into shared memory</li>
<li>Synchronize threads</li>
<li>Process data using shared memory</li>
<li>Write results back to global memory</li>
</ol>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-9-pooling"><a class="header" href="#puzzle-9-pooling">Puzzle 9: Pooling</a></h1>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>Implement a kernel that sums together the last 3 positions of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 1 global read and 1 global write per thread.</em></p>
<p><img src="puzzle_09/./media/videos/720p30/puzzle_09_viz.gif" alt="Pooling visualization" /></p>
<h2 id="implementation-approaches-4"><a class="header" href="#implementation-approaches-4">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-5"><a class="header" href="#-raw-memory-approach-5"><a href="puzzle_09/./raw.html">🔰 Raw memory approach</a></a></h3>
<p>Learn how to implement sliding window operations with manual memory management and synchronization.</p>
<h3 id="-layouttensor-version-3"><a class="header" href="#-layouttensor-version-3"><a href="puzzle_09/./layout_tensor.html">📐 LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor’s features for efficient window-based operations and shared memory management.</p>
<p>💡 <strong>Note</strong>: See how LayoutTensor simplifies sliding window operations while maintaining efficient memory access patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-16"><a class="header" href="#key-concepts-16">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using shared memory for sliding window operations</li>
<li>Handling boundary conditions in pooling</li>
<li>Coordinating thread access to neighboring elements</li>
</ul>
<p>The key insight is understanding how to efficiently access a window of elements using shared memory, with special handling for the first elements in the sequence.</p>
<h2 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Window size: 3 elements</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Window access</strong>: Each output depends on up to 3 previous elements</li>
<li><strong>Edge handling</strong>: First two positions need special treatment</li>
<li><strong>Memory pattern</strong>: One shared memory load per thread</li>
<li><strong>Thread sync</strong>: Coordination before window operations</li>
</ul>
<h2 id="code-to-complete-12"><a class="header" href="#code-to-complete-12">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32


fn pooling(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB * sizeof[dtype](),
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 10 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p09/p09.mojo" class="filename">View full file: problems/p09/p09.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load data and call <code>barrier()</code></li>
<li>Special cases: <code>out[0] = shared[0]</code>, <code>out[1] = shared[0] + shared[1]</code></li>
<li>General case: <code>if 1 &lt; global_i &lt; size</code></li>
<li>Sum three elements: <code>shared[local_i - 2] + shared[local_i - 1] + shared[local_i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-13"><a class="header" href="#running-the-code-13">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p09
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0])
</code></pre>
<h2 id="solution-12"><a class="header" href="#solution-12">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn pooling(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB * sizeof[dtype](),
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    if global_i == 0:
        out[0] = shared[0]
    elif global_i == 1:
        out[1] = shared[0] + shared[1]

    if 1 &lt; global_i &lt; size:
        out[global_i] = (
            shared[local_i - 2] + shared[local_i - 1] + shared[local_i]
        )


</code></pre>
<div class="solution-explanation">
<p>The solution implements a sliding window sum using shared memory with these key steps:</p>
<ol>
<li>
<p><strong>Shared Memory Setup</strong>:</p>
<ul>
<li>Allocates <code>TPB</code> elements in shared memory</li>
<li>Each thread loads one element from global memory</li>
<li>Uses <code>barrier()</code> to ensure all data is loaded</li>
</ul>
</li>
<li>
<p><strong>Boundary Cases</strong>:</p>
<ul>
<li>Position 0: <code>out[0] = shared[0]</code> (only first element)</li>
<li>Position 1: <code>out[1] = shared[0] + shared[1]</code> (sum of first two elements)</li>
</ul>
</li>
<li>
<p><strong>Main Window Operation</strong>:</p>
<ul>
<li>For positions 2 and beyond: <code>out[i] = shared[i-2] + shared[i-1] + shared[i]</code></li>
<li>Uses local indices for shared memory access</li>
<li>Maintains coalesced memory access pattern</li>
</ul>
</li>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>One global read per thread into shared memory</li>
<li>One global write per thread from shared memory</li>
<li>Uses shared memory for efficient neighbor access</li>
<li>Avoids redundant global memory loads</li>
</ul>
</li>
</ol>
<p>This approach is efficient because:</p>
<ul>
<li>Minimizes global memory access</li>
<li>Uses shared memory for fast neighbor lookups</li>
<li>Handles boundary conditions without branching in the main case</li>
<li>Maintains good memory coalescing</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-17"><a class="header" href="#key-concepts-17">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Using LayoutTensor for sliding window operations</li>
<li>Managing shared memory with <code>LayoutTensorBuilder</code> that we saw in <a href="puzzle_09/../puzzle_08/layout_tensor.html">puzzle_08</a></li>
<li>Efficient neighbor access patterns</li>
<li>Boundary condition handling</li>
</ul>
<p>The key insight is how LayoutTensor simplifies shared memory management while maintaining efficient window-based operations.</p>
<h2 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Window size: 3 elements</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Tensor builder</strong>: Use <code>LayoutTensorBuilder[dtype]().row_major[TPB]().shared().alloc()</code></li>
<li><strong>Window access</strong>: Natural indexing for 3-element windows</li>
<li><strong>Edge handling</strong>: Special cases for first two positions</li>
<li><strong>Memory pattern</strong>: One shared memory load per thread</li>
</ul>
<h2 id="code-to-complete-13"><a class="header" href="#code-to-complete-13">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn pooling[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FIX ME IN (roughly 10 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p09/p09_layout_tensor.mojo" class="filename">View full file: problems/p09/p09_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Load data with natural indexing: <code>shared[local_i] = a[global_i]</code></li>
<li>Handle special cases for first two elements</li>
<li>Use shared memory for window operations</li>
<li>Guard against out-of-bounds access</li>
</ol>
</div>
</details>
<h2 id="running-the-code-14"><a class="header" href="#running-the-code-14">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p09_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0])
</code></pre>
<h2 id="solution-13"><a class="header" href="#solution-13">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn pooling[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    # Load data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # Synchronize threads within block
    barrier()

    # Handle first two special cases
    if global_i == 0:
        out[0] = shared[0]
    if global_i == 1:
        out[1] = shared[0] + shared[1]

    # Handle general case
    if 1 &lt; global_i &lt; size:
        out[global_i] = (
            shared[local_i - 2] + shared[local_i - 1] + shared[local_i]
        )


</code></pre>
<div class="solution-explanation">
<p>The solution implements a sliding window sum using LayoutTensor with these key steps:</p>
<ol>
<li>
<p><strong>Shared Memory Setup</strong>:</p>
<ul>
<li>Uses tensor builder for clean shared memory allocation</li>
<li>Natural indexing for data loading</li>
<li>Thread synchronization with <code>barrier()</code></li>
</ul>
</li>
<li>
<p><strong>Boundary Cases</strong>:</p>
<ul>
<li>Position 0: Single element output</li>
<li>Position 1: Sum of first two elements</li>
<li>Clean indexing with LayoutTensor bounds checking</li>
</ul>
</li>
<li>
<p><strong>Main Window Operation</strong>:</p>
<ul>
<li>Natural indexing for 3-element window</li>
<li>Safe access to neighboring elements</li>
<li>Automatic bounds checking</li>
</ul>
</li>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>Efficient shared memory usage</li>
<li>Type-safe operations</li>
<li>Layout-aware indexing</li>
<li>Automatic alignment handling</li>
</ul>
</li>
</ol>
<p>Benefits over raw approach:</p>
<ul>
<li>Cleaner shared memory allocation</li>
<li>Safer memory access</li>
<li>Natural indexing syntax</li>
<li>Built-in bounds checking</li>
<li>Layout management</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-10-dot-product"><a class="header" href="#puzzle-10-dot-product">Puzzle 10: Dot Product</a></h1>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>Implement a kernel that computes the dot-product of vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 2 global reads and 1 global write per thread.</em></p>
<p><img src="puzzle_10/./media/videos/720p30/puzzle_10_viz.gif" alt="Dot product visualization" /></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-18"><a class="header" href="#key-concepts-18">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Implementing parallel reduction operations</li>
<li>Using shared memory for intermediate results</li>
<li>Coordinating threads for collective operations</li>
</ul>
<p>The key insight is understanding how to efficiently combine multiple values into a single result using parallel computation and shared memory.</p>
<h2 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h2>
<ul>
<li>Vector size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Output size: 1 element</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Element access</strong>: Each thread reads corresponding elements from <code>a</code> and <code>b</code></li>
<li><strong>Partial results</strong>: Computing and storing intermediate values</li>
<li><strong>Thread coordination</strong>: Synchronizing before combining results</li>
<li><strong>Final reduction</strong>: Converting partial results to scalar output</li>
</ul>
<p><em>Note: For this problem, you don’t need to worry about number of shared reads. We will
handle that challenge later.</em></p>
<h2 id="code-to-complete-14"><a class="header" href="#code-to-complete-14">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (SIZE, 1)
alias dtype = DType.float32


fn dot_product(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    # FILL ME IN (roughly 13 lines)
    ...


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p10/p10.mojo" class="filename">View full file: problems/p10/p10.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>a[global_i] * b[global_i]</code> in <code>shared[local_i]</code></li>
<li>Call <code>barrier()</code> to synchronize</li>
<li>Use thread 0 to sum all products in shared memory</li>
<li>Write final sum to <code>out[0]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-15"><a class="header" href="#running-the-code-15">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p10
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0])
expected: HostBuffer([140.0])
</code></pre>
<h2 id="solution-14"><a class="header" href="#solution-14">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn dot_product(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB * sizeof[dtype](),
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    if global_i &lt; size:
        shared[local_i] = a[global_i] * b[global_i]

    barrier()

    # The following causes race condition: all threads writing to the same location
    # out[0] += shared[local_i]

    # Instead can do parallel reduction in shared memory as opposed to
    # global memory which has no guarantee on synchronization.
    # Loops using global memory can cause thread divergence because
    # fundamentally GPUs execute threads in warps (groups of 32 threads typically)
    # and warps can be scheduled independently.
    # However, shared memory does not have such issues as long as we use `barrier()`
    # correctly when we're in the same thread block.
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            shared[local_i] += shared[local_i + stride]

        barrier()
        stride //= 2

    # only thread 0 writes the final result
    if local_i == 0:
        out[0] = shared[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel reduction algorithm for dot product computation using shared memory. Here’s a detailed breakdown:</p>
<h3 id="phase-1-element-wise-multiplication"><a class="header" href="#phase-1-element-wise-multiplication">Phase 1: Element-wise Multiplication</a></h3>
<p>Each thread performs one multiplication:</p>
<pre><code class="language-txt">Thread i: shared[i] = a[i] * b[i]
</code></pre>
<h3 id="phase-2-parallel-reduction"><a class="header" href="#phase-2-parallel-reduction">Phase 2: Parallel Reduction</a></h3>
<p>The reduction uses a tree-based approach that halves active threads in each step:</p>
<pre><code class="language-txt">Initial:  [0*0  1*1  2*2  3*3  4*4  5*5  6*6  7*7]
        = [0    1    4    9    16   25   36   49]

Step 1:   [0+16 1+25 4+36 9+49  16   25   36   49]
        = [16   26   40   58   16   25   36   49]

Step 2:   [16+40 26+58 40   58   16   25   36   49]
        = [56   84   40   58   16   25   36   49]

Step 3:   [56+84  84   40   58   16   25   36   49]
        = [140   84   40   58   16   25   36   49]
</code></pre>
<h3 id="key-implementation-features"><a class="header" href="#key-implementation-features">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>Each thread loads exactly two values from global memory (<code>a[i]</code>, <code>b[i]</code>)</li>
<li>Uses shared memory for intermediate results</li>
<li>Final result written once to global memory</li>
</ul>
</li>
<li>
<p><strong>Thread Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> after initial multiplication</li>
<li><code>barrier()</code> after each reduction step</li>
<li>Prevents race conditions between reduction steps</li>
</ul>
</li>
<li>
<p><strong>Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; stride:
        shared[local_i] += shared[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
<ul>
<li>Halves stride in each step</li>
<li>Only active threads perform additions</li>
<li>Maintains work efficiency</li>
</ul>
</li>
<li>
<p><strong>Performance Considerations</strong>:</p>
<ul>
<li>\(\log_2(n)\) steps for \(n\) elements</li>
<li>Coalesced memory access pattern</li>
<li>Minimal thread divergence</li>
<li>Efficient use of shared memory</li>
</ul>
</li>
</ol>
<p>This implementation achieves \(O(\log n)\) time complexity compared to \(O(n)\) in sequential execution, demonstrating the power of parallel reduction algorithms.</p>
<h3 id="barrier-synchronization-importance"><a class="header" href="#barrier-synchronization-importance">Barrier Synchronization Importance</a></h3>
<p>The <code>barrier()</code> between reduction steps is critical for correctness. Here’s why:</p>
<p>Without <code>barrier()</code>, race conditions occur:</p>
<pre><code class="language-text">Initial shared memory: [0 1 4 9 16 25 36 49]

Step 1 (stride = 4):
Thread 0 reads: shared[0] = 0, shared[4] = 16
Thread 1 reads: shared[1] = 1, shared[5] = 25
Thread 2 reads: shared[2] = 4, shared[6] = 36
Thread 3 reads: shared[3] = 9, shared[7] = 49

Without barrier:
- Thread 0 writes: shared[0] = 0 + 16 = 16
- Thread 1 starts next step (stride = 2) before Thread 0 finishes
  and reads old value shared[0] = 0 instead of 16!
</code></pre>
<p>With <code>barrier()</code>:</p>
<pre><code class="language-text">Step 1 (stride = 4):
All threads write their sums:
[16 26 40 58 16 25 36 49]
barrier() ensures ALL threads see these values

Step 2 (stride = 2):
Now threads safely read the updated values:
Thread 0: shared[0] = 16 + 40 = 56
Thread 1: shared[1] = 26 + 58 = 84
</code></pre>
<p>The <code>barrier()</code> ensures:</p>
<ol>
<li>All writes from current step complete</li>
<li>All threads see updated values</li>
<li>No thread starts next iteration early</li>
<li>Consistent shared memory state</li>
</ol>
<p>Without these synchronization points, we could get:</p>
<ul>
<li>Memory race conditions</li>
<li>Threads reading stale values</li>
<li>Non-deterministic results</li>
<li>Incorrect final sum</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-19"><a class="header" href="#key-concepts-19">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Similar to the <a href="puzzle_10/../puzzle_08/layout_tensor.html">puzzle 8</a> and <a href="puzzle_10/../puzzle_09/layout_tensor.html">puzzle 9</a>, implementing parallel reduction with LayoutTensor</li>
<li>Managing shared memory using <code>LayoutTensorBuilder</code></li>
<li>Coordinating threads for collective operations</li>
<li>Using layout-aware tensor operations</li>
</ul>
<p>The key insight is how LayoutTensor simplifies memory management while maintaining efficient parallel reduction patterns.</p>
<h2 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h2>
<ul>
<li>Vector size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Output size: 1 element</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Tensor builder</strong>: Use <code>LayoutTensorBuilder[dtype]().row_major[TPB]().shared().alloc()</code></li>
<li><strong>Element access</strong>: Natural indexing with bounds checking</li>
<li><strong>Layout handling</strong>: Separate layouts for input and output</li>
<li><strong>Thread coordination</strong>: Same synchronization patterns with <code>barrier()</code></li>
</ul>
<h2 id="code-to-complete-15"><a class="header" href="#code-to-complete-15">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (SIZE, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)
alias out_layout = Layout.row_major(1)


fn dot_product[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, in_layout],
    b: LayoutTensor[mut=True, dtype, in_layout],
    size: Int,
):
    # FILL ME IN (roughly 13 lines)
    ...


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p10/p10_layout_tensor.mojo" class="filename">View full file: problems/p10/p10_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Store <code>a[global_i] * b[global_i]</code> in <code>shared[local_i]</code></li>
<li>Use parallel reduction pattern with <code>barrier()</code></li>
<li>Let thread 0 write final result to <code>out[0]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-16"><a class="header" href="#running-the-code-16">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p10_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0])
expected: HostBuffer([140.0])
</code></pre>
<h2 id="solution-15"><a class="header" href="#solution-15">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn dot_product[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, in_layout],
    b: LayoutTensor[mut=True, dtype, in_layout],
    size: Int,
):
    shared = tb[dtype]().row_major[TPB]().shared().alloc()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    # Compute element-wise multiplication into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i] * b[global_i]

    # Synchronize threads within block
    barrier()

    # Parallel reduction in shared memory
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            shared[local_i] += shared[local_i + stride]

        barrier()
        stride //= 2

    # Only thread 0 writes the final result
    if local_i == 0:
        out[0] = shared[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel reduction for dot product using LayoutTensor. Here’s the detailed breakdown:</p>
<h3 id="phase-1-element-wise-multiplication-1"><a class="header" href="#phase-1-element-wise-multiplication-1">Phase 1: Element-wise Multiplication</a></h3>
<p>Each thread performs one multiplication with natural indexing:</p>
<pre><code class="language-mojo">shared[local_i] = a[global_i] * b[global_i]
</code></pre>
<h3 id="phase-2-parallel-reduction-1"><a class="header" href="#phase-2-parallel-reduction-1">Phase 2: Parallel Reduction</a></h3>
<p>Tree-based reduction with layout-aware operations:</p>
<pre><code class="language-txt">Initial:  [0*0  1*1  2*2  3*3  4*4  5*5  6*6  7*7]
        = [0    1    4    9    16   25   36   49]

Step 1:   [0+16 1+25 4+36 9+49  16   25   36   49]
        = [16   26   40   58   16   25   36   49]

Step 2:   [16+40 26+58 40   58   16   25   36   49]
        = [56   84   40   58   16   25   36   49]

Step 3:   [56+84  84   40   58   16   25   36   49]
        = [140   84   40   58   16   25   36   49]
</code></pre>
<h3 id="key-implementation-features-1"><a class="header" href="#key-implementation-features-1">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Memory Management</strong>:</p>
<ul>
<li>Clean shared memory allocation with tensor builder</li>
<li>Type-safe operations with LayoutTensor</li>
<li>Automatic bounds checking</li>
<li>Layout-aware indexing</li>
</ul>
</li>
<li>
<p><strong>Thread Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> after initial multiplication</li>
<li><code>barrier()</code> between reduction steps</li>
<li>Safe thread coordination</li>
</ul>
</li>
<li>
<p><strong>Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; stride:
        shared[local_i] += shared[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
</li>
<li>
<p><strong>Performance Benefits</strong>:</p>
<ul>
<li>\(O(\log n)\) time complexity</li>
<li>Coalesced memory access</li>
<li>Minimal thread divergence</li>
<li>Efficient shared memory usage</li>
</ul>
</li>
</ol>
<p>The LayoutTensor version maintains the same efficient parallel reduction while providing:</p>
<ul>
<li>Better type safety</li>
<li>Cleaner memory management</li>
<li>Layout awareness</li>
<li>Natural indexing syntax</li>
</ul>
<h3 id="barrier-synchronization-importance-1"><a class="header" href="#barrier-synchronization-importance-1">Barrier Synchronization Importance</a></h3>
<p>The <code>barrier()</code> between reduction steps is critical for correctness. Here’s why:</p>
<p>Without <code>barrier()</code>, race conditions occur:</p>
<pre><code class="language-text">Initial shared memory: [0 1 4 9 16 25 36 49]

Step 1 (stride = 4):
Thread 0 reads: shared[0] = 0, shared[4] = 16
Thread 1 reads: shared[1] = 1, shared[5] = 25
Thread 2 reads: shared[2] = 4, shared[6] = 36
Thread 3 reads: shared[3] = 9, shared[7] = 49

Without barrier:
- Thread 0 writes: shared[0] = 0 + 16 = 16
- Thread 1 starts next step (stride = 2) before Thread 0 finishes
  and reads old value shared[0] = 0 instead of 16!
</code></pre>
<p>With <code>barrier()</code>:</p>
<pre><code class="language-text">Step 1 (stride = 4):
All threads write their sums:
[16 26 40 58 16 25 36 49]
barrier() ensures ALL threads see these values

Step 2 (stride = 2):
Now threads safely read the updated values:
Thread 0: shared[0] = 16 + 40 = 56
Thread 1: shared[1] = 26 + 58 = 84
</code></pre>
<p>The <code>barrier()</code> ensures:</p>
<ol>
<li>All writes from current step complete</li>
<li>All threads see updated values</li>
<li>No thread starts next iteration early</li>
<li>Consistent shared memory state</li>
</ol>
<p>Without these synchronization points, we could get:</p>
<ul>
<li>Memory race conditions</li>
<li>Threads reading stale values</li>
<li>Non-deterministic results</li>
<li>Incorrect final sum</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-11-1d-convolution"><a class="header" href="#puzzle-11-1d-convolution">Puzzle 11: 1D Convolution</a></h1>
<blockquote>
<h2 id="moving-to-layouttensor"><a class="header" href="#moving-to-layouttensor">Moving to LayoutTensor</a></h2>
<p>So far in our GPU puzzle journey, we’ve been exploring two parallel approaches to GPU memory management:</p>
<ol>
<li>Raw memory management with direct pointer manipulation using <a href="https://docs.modular.com/mojo/stdlib/memory/unsafe_pointer/UnsafePointer/">UnsafePointer</a></li>
<li>The more structured <a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a> and its related abstractions such as <a href="https://docs.modular.com/mojo/stdlib/layout/tensor_builder/LayoutTensorBuild/">LayoutTensorBuild</a></li>
</ol>
<p>Starting from this puzzle, we’re transitioning exclusively to using <code>LayoutTensor</code>. This abstraction provides several benefits:</p>
<ul>
<li>Type-safe memory access patterns</li>
<li>Clear representation of data layouts</li>
<li>Better code maintainability</li>
<li>Reduced chance of memory-related bugs</li>
<li>More expressive code that better represents the underlying computations</li>
<li>A lot more … that we’ll uncover gradually!</li>
</ul>
<p>This transition aligns with best practices in modern GPU programming in Mojo 🔥, where higher-level abstractions help manage complexity without sacrificing performance.</p>
</blockquote>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>In signal processing and image analysis, convolution is a fundamental operation that combines two sequences to produce a third sequence. This puzzle challenges you to implement a 1D convolution on the GPU, where each output element is computed by sliding a kernel over an input array.</p>
<p>Implement a kernel that computes a 1D convolution between vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code> using the <code>LayoutTensor</code> abstraction.</p>
<p><strong>Note:</strong> <em>You need to handle the general case. You only need 2 global reads and 1 global write per thread.</em></p>
<p><img src="puzzle_11/./media/videos/720p30/puzzle_11_viz.gif" alt="1D Convolution" /></p>
<p>For those new to convolution, think of it as a weighted sliding window operation. At each position, we multiply the kernel values with the corresponding input values and sum the results. In mathematical notation, this is often written as:</p>
<p>\[\Large out[i] = \sum_{j=0}^{\text{CONV}-1} a[i+j] \cdot b[j] \]</p>
<p>In pseudocode, 1D convolution is:</p>
<pre><code class="language-python">for i in range(SIZE):
    for j in range(CONV):
        if i + j &lt; SIZE:
            ret[i] += a_host[i + j] * b_host[j]
</code></pre>
<p>This puzzle is split into two parts to help you build understanding progressively:</p>
<ul>
<li>
<p><a href="puzzle_11/./simple.html">Simple version: Single block</a>
Start here to learn the basics of implementing convolution with shared memory in a single block using LayoutTensor.</p>
</li>
<li>
<p><a href="puzzle_11/./complete.html">Complete version: Block boundary</a>
Then tackle the more challenging case where data needs to be shared across block boundaries, leveraging LayoutTensor’s capabilities.</p>
</li>
</ul>
<p>Each version presents unique challenges in terms of memory access patterns and thread coordination. The simple version helps you understand the basic convolution operation, while the complete version tests your ability to handle more complex scenarios that arise in real-world GPU programming.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-version-single-block"><a class="header" href="#simple-version-single-block">Simple Version: Single Block</a></h1>
<h2 id="key-concepts-20"><a class="header" href="#key-concepts-20">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Implementing sliding window operations on GPUs</li>
<li>Managing data dependencies across threads</li>
<li>Using shared memory for overlapping regions</li>
</ul>
<p>The key insight is understanding how to efficiently access overlapping elements while maintaining correct boundary conditions.</p>
<h2 id="configuration-8"><a class="header" href="#configuration-8">Configuration</a></h2>
<ul>
<li>Input array size: <code>SIZE = 6</code> elements</li>
<li>Kernel size: <code>CONV = 3</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Shared memory: Two arrays of size <code>SIZE</code> and <code>CONV</code></li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Data loading</strong>: Each thread loads one element from input and kernel</li>
<li><strong>Memory pattern</strong>: Shared arrays for input and convolution kernel</li>
<li><strong>Thread sync</strong>: Coordination before computation</li>
</ul>
<h2 id="code-to-complete-16"><a class="header" href="#code-to-complete-16">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias MAX_CONV = 4
alias TPB = 8
alias SIZE = 6
alias CONV = 3
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias in_layout = Layout.row_major(SIZE)
alias out_layout = Layout.row_major(SIZE)
alias conv_layout = Layout.row_major(CONV)


fn conv_1d_simple[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
    a_size: Int,
    b_size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 13 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p11/p11.mojo" class="filename">View full file: problems/p11/p11.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>tb[dtype]().row_major[SIZE]().shared().alloc()</code> for shared memory allocation</li>
<li>Load input to <code>shared_a[local_i]</code> and kernel to <code>shared_b[local_i]</code></li>
<li>Call <code>barrier()</code> after loading</li>
<li>Sum products within bounds: <code>if local_i + j &lt; SIZE</code></li>
<li>Write result if <code>global_i &lt; a_size</code></li>
</ol>
</div>
</details>
<h3 id="running-the-code-17"><a class="header" href="#running-the-code-17">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p11 --simple
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([5.0, 8.0, 11.0, 14.0, 5.0, 0.0])
</code></pre>
<h2 id="solution-16"><a class="header" href="#solution-16">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn conv_1d_simple[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
    a_size: Int,
    b_size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared_a = tb[dtype]().row_major[SIZE]().shared().alloc()
    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()
    if global_i &lt; a_size:
        shared_a[local_i] = a[global_i]

    if global_i &lt; b_size:
        shared_b[local_i] = b[global_i]

    barrier()

    # Note: this is unsafe as it enforces no guard so could access `shared_a` beyond its bounds
    # local_sum = Scalar[dtype](0)
    # for j in range(CONV):
    #     if local_i + j &lt; SIZE:
    #         local_sum += shared_a[local_i + j] * shared_b[j]

    # if global_i &lt; a_size:
    #     out[global_i] = local_sum

    # Safe and correct:
    if global_i &lt; a_size:
        # Note: using `var` allows us to include the type in the type inference
        # `out.element_type` is available in LayoutTensor
        var local_sum: out.element_type = 0

        # Note: `@parameter` decorator unrolls the loop at compile time given `CONV` is a compile-time constant
        # See: https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-for-statement
        @parameter
        for j in range(CONV):
            # Bonus: do we need this check for this specific example with fixed SIZE, CONV
            if local_i + j &lt; SIZE:
                local_sum += shared_a[local_i + j] * shared_b[j]

        out[global_i] = local_sum


</code></pre>
<div class="solution-explanation">
<p>The solution implements a 1D convolution using shared memory for efficient access to overlapping elements. Here’s a detailed breakdown:</p>
<h3 id="memory-layout"><a class="header" href="#memory-layout">Memory Layout</a></h3>
<pre><code class="language-txt">Input array a:   [0  1  2  3  4  5]
Kernel b:        [0  1  2]
</code></pre>
<h3 id="computation-steps"><a class="header" href="#computation-steps">Computation Steps</a></h3>
<ol>
<li>
<p><strong>Data Loading</strong>:</p>
<pre><code class="language-txt">shared_a: [0  1  2  3  4  5]  // Input array
shared_b: [0  1  2]           // Convolution kernel
</code></pre>
</li>
<li>
<p><strong>Convolution Process</strong> for each position i:</p>
<pre><code class="language-txt">out[0] = a[0]*b[0] + a[1]*b[1] + a[2]*b[2] = 0*0 + 1*1 + 2*2 = 5
out[1] = a[1]*b[0] + a[2]*b[1] + a[3]*b[2] = 1*0 + 2*1 + 3*2 = 8
out[2] = a[2]*b[0] + a[3]*b[1] + a[4]*b[2] = 2*0 + 3*1 + 4*2 = 11
out[3] = a[3]*b[0] + a[4]*b[1] + a[5]*b[2] = 3*0 + 4*1 + 5*2 = 14
out[4] = a[4]*b[0] + a[5]*b[1] + 0*b[2]    = 4*0 + 5*1 + 0*2 = 5
out[5] = a[5]*b[0] + 0*b[1]   + 0*b[2]     = 5*0 + 0*1 + 0*2 = 0
</code></pre>
</li>
</ol>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<ol>
<li>
<p><strong>Memory Safety Considerations</strong>:</p>
<ul>
<li>
<p>The naive approach without proper bounds checking could be unsafe:</p>
<pre><code class="language-mojo"># Unsafe version - could access shared_a beyond its bounds
local_sum = Scalar[dtype](0)
for j in range(CONV):
    if local_i + j &lt; SIZE:
        local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
</li>
<li>
<p>The safe and correct implementation:</p>
<pre><code class="language-mojo">if global_i &lt; a_size:
    var local_sum: out.element_type = 0  # Using var allows type inference
    @parameter  # Unrolls loop at compile time since CONV is constant
    for j in range(CONV):
        if local_i + j &lt; SIZE:
            local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Key Implementation Features</strong>:</p>
<ul>
<li>Uses <code>var</code> for proper type inference with <code>out.element_type</code></li>
<li>Employs <code>@parameter</code> decorator to unroll the convolution loop at compile time</li>
<li>Maintains strict bounds checking for memory safety</li>
<li>Leverages LayoutTensor’s type system for better code safety</li>
</ul>
</li>
<li>
<p><strong>Memory Management</strong>:</p>
<ul>
<li>Uses shared memory for both input array and kernel</li>
<li>Single load per thread from global memory</li>
<li>Efficient reuse of loaded data</li>
</ul>
</li>
<li>
<p><strong>Thread Coordination</strong>:</p>
<ul>
<li><code>barrier()</code> ensures all data is loaded before computation</li>
<li>Each thread computes one output element</li>
<li>Maintains coalesced memory access pattern</li>
</ul>
</li>
<li>
<p><strong>Performance Optimizations</strong>:</p>
<ul>
<li>Minimizes global memory access</li>
<li>Uses shared memory for fast data access</li>
<li>Avoids thread divergence in main computation loop</li>
<li>Loop unrolling through <code>@parameter</code> decorator</li>
</ul>
</li>
</ol>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="complete-version-block-boundary-case"><a class="header" href="#complete-version-block-boundary-case">Complete version: Block Boundary Case</a></h1>
<h2 id="configuration-9"><a class="header" href="#configuration-9">Configuration</a></h2>
<ul>
<li>Input array size: <code>SIZE_2 = 15</code> elements</li>
<li>Kernel size: <code>CONV_2 = 4</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB + CONV_2 - 1</code> elements for input</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Extended loading</strong>: Account for boundary overlap</li>
<li><strong>Block edges</strong>: Handle data across block boundaries</li>
<li><strong>Memory layout</strong>: Efficient shared memory usage</li>
<li><strong>Synchronization</strong>: Proper thread coordination</li>
</ul>
<h2 id="code-to-complete-17"><a class="header" href="#code-to-complete-17">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE_2 = 15
alias CONV_2 = 4
alias BLOCKS_PER_GRID_2 = (2, 1)
alias THREADS_PER_BLOCK_2 = (TPB, 1)


fn conv_1d_block_boundary[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
    a_size: Int,
    b_size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 18 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p11/p11.mojo" class="filename">View full file: problems/p11/p11.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()</code> for shared memory</li>
<li>Load main data: <code>shared_a[local_i] = a[global_i]</code></li>
<li>Load boundary: <code>if local_i &lt; CONV_2 - 1</code> handle next block data</li>
<li>Load kernel: <code>shared_b[local_i] = b[local_i]</code></li>
<li>Sum within extended bounds: <code>if local_i + j &lt; TPB + CONV_2 - 1</code></li>
</ol>
</div>
</details>
<h3 id="running-the-code-18"><a class="header" href="#running-the-code-18">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p11 --block-boundary
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([5.0, 8.0, 11.0, 14.0, 5.0, 0.0])
</code></pre>
<h2 id="solution-17"><a class="header" href="#solution-17">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn conv_1d_block_boundary[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
    a_size: Int,
    b_size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # first: need to account for padding
    shared_a = tb[dtype]().row_major[TPB + CONV - 1]().shared().alloc()
    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()
    if global_i &lt; a_size:
        shared_a[local_i] = a[global_i]

    # second: load elements needed for convolution at block boundary
    if local_i &lt; CONV - 1:
        # indices from next block
        next_idx = global_i + TPB
        if next_idx &lt; a_size:
            shared_a[TPB + local_i] = a[next_idx]

    if local_i &lt; b_size:
        shared_b[local_i] = b[local_i]

    barrier()

    if global_i &lt; a_size:
        var local_sum: out.element_type = 0

        @parameter
        for j in range(CONV):
            if local_i + j &lt; TPB + CONV - 1:
                local_sum += shared_a[local_i + j] * shared_b[j]

        out[global_i] = local_sum


</code></pre>
<div class="solution-explanation">
<p>The solution handles block boundary cases in 1D convolution using extended shared memory. Here’s a detailed analysis:</p>
<h3 id="memory-layout-and-sizing"><a class="header" href="#memory-layout-and-sizing">Memory Layout and Sizing</a></h3>
<pre><code class="language-txt">Test Configuration:
- Full array size: SIZE_2 = 15 elements
- Grid: 2 blocks × 8 threads
- Convolution kernel: CONV_2 = 4 elements

Block 0 shared memory:  [0 1 2 3 4 5 6 7|8 9 10]  // TPB(8) + (CONV_2-1)(3) padding
Block 1 shared memory:  [8 9 10 11 12 13 14|0 0]  // Second block with padding

Size calculation:
- Main data: TPB elements (8)
- Overlap: CONV_2 - 1 elements (4 - 1 = 3)
- Total: TPB + CONV_2 - 1 = 8 + 4 - 1 = 11 elements
</code></pre>
<h3 id="implementation-details-1"><a class="header" href="#implementation-details-1">Implementation Details</a></h3>
<ol>
<li>
<p><strong>Shared Memory Allocation</strong>:</p>
<pre><code class="language-mojo"># First: account for padding needed for convolution window
shared_a = tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()
shared_b = tb[dtype]().row_major[CONV_2]().shared().alloc()
</code></pre>
<p>This allocation pattern ensures we have enough space for both the block’s data and the overlap region.</p>
</li>
<li>
<p><strong>Data Loading Strategy</strong>:</p>
<pre><code class="language-mojo"># Main block data
if global_i &lt; a_size:
    shared_a[local_i] = a[global_i]

# Boundary data from next block
if local_i &lt; CONV_2 - 1:
    next_idx = global_i + TPB
    if next_idx &lt; a_size:
        shared_a[TPB + local_i] = a[next_idx]
</code></pre>
<ul>
<li>Only threads with <code>local_i &lt; CONV_2 - 1</code> load boundary data</li>
<li>Prevents unnecessary thread divergence</li>
<li>Maintains memory coalescing for main data load</li>
</ul>
</li>
<li>
<p><strong>Kernel Loading</strong>:</p>
<pre><code class="language-mojo">if local_i &lt; b_size:
    shared_b[local_i] = b[local_i]
</code></pre>
<ul>
<li>Single load per thread</li>
<li>Bounded by kernel size</li>
</ul>
</li>
<li>
<p><strong>Convolution Computation</strong>:</p>
<pre><code class="language-mojo">if global_i &lt; a_size:
    var local_sum: out.element_type = 0
    @parameter
    for j in range(CONV_2):
        if local_i + j &lt; TPB + CONV_2 - 1:
            local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
<ul>
<li>Uses <code>@parameter</code> for compile-time loop unrolling</li>
<li>Proper type inference with <code>out.element_type</code></li>
<li>Extended bounds check for overlap region</li>
</ul>
</li>
</ol>
<h3 id="memory-access-pattern-analysis"><a class="header" href="#memory-access-pattern-analysis">Memory Access Pattern Analysis</a></h3>
<ol>
<li>
<p><strong>Block 0 Access Pattern</strong>:</p>
<pre><code class="language-txt">Thread 0: [0 1 2 3] × [0 1 2 3]
Thread 1: [1 2 3 4] × [0 1 2 3]
Thread 2: [2 3 4 5] × [0 1 2 3]
...
Thread 7: [7 8 9 10] × [0 1 2 3]  // Uses overlap data
</code></pre>
</li>
<li>
<p><strong>Block 1 Access Pattern</strong>:</p>
<pre><code class="language-txt">Thread 0: [8 9 10 11] × [0 1 2 3]
Thread 1: [9 10 11 12] × [0 1 2 3]
...
Thread 7: [14 0 0 0] × [0 1 2 3]  // Zero padding at end
</code></pre>
</li>
</ol>
<h3 id="performance-optimizations"><a class="header" href="#performance-optimizations">Performance Optimizations</a></h3>
<ol>
<li>
<p><strong>Memory Coalescing</strong>:</p>
<ul>
<li>Main data load: Consecutive threads access consecutive memory</li>
<li>Boundary data: Only necessary threads participate</li>
<li>Single barrier synchronization point</li>
</ul>
</li>
<li>
<p><strong>Thread Divergence Minimization</strong>:</p>
<ul>
<li>Clean separation of main and boundary loading</li>
<li>Uniform computation pattern within warps</li>
<li>Efficient bounds checking</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Usage</strong>:</p>
<ul>
<li>Optimal sizing to handle block boundaries</li>
<li>No bank conflicts in access pattern</li>
<li>Efficient reuse of loaded data</li>
</ul>
</li>
<li>
<p><strong>Boundary Handling</strong>:</p>
<ul>
<li>Implicit zero padding at array end</li>
<li>Seamless block transition</li>
<li>Proper handling of edge cases</li>
</ul>
</li>
</ol>
<p>This implementation achieves efficient cross-block convolution while maintaining:</p>
<ul>
<li>Memory safety through proper bounds checking</li>
<li>High performance through optimized memory access</li>
<li>Clean code structure using LayoutTensor abstractions</li>
<li>Minimal synchronization overhead</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-12-prefix-sum"><a class="header" href="#puzzle-12-prefix-sum">Puzzle 12: Prefix Sum</a></h1>
<h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Prefix sum (also known as <em>scan</em>) is a fundamental parallel algorithm that computes running totals of a sequence. Found at the heart of many parallel applications - from sorting algorithms to scientific simulations - it transforms a sequence of numbers into their running totals. While simple to compute sequentially, making this efficient on a GPU requires clever parallel thinking!</p>
<p>Implement a kernel that computes a prefix-sum over vector <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>If the size of <code>a</code> is greater than the block size, only store the sum of each block.</em></p>
<p><img src="puzzle_12/./media/videos/720p30/puzzle_12_viz.gif" alt="Prefix sum" /></p>
<h2 id="key-concepts-21"><a class="header" href="#key-concepts-21">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Parallel algorithms with logarithmic complexity</li>
<li>Shared memory coordination patterns</li>
<li>Multi-phase computation strategies</li>
</ul>
<p>The key insight is understanding how to transform a sequential operation into an efficient parallel algorithm using shared memory.</p>
<p>For example, given an input sequence \([3, 1, 4, 1, 5, 9]\), the prefix sum would produce:</p>
<ul>
<li>\([3]\) (just the first element)</li>
<li>\([3, 4]\) (3 + 1)</li>
<li>\([3, 4, 8]\) (previous sum + 4)</li>
<li>\([3, 4, 8, 9]\) (previous sum + 1)</li>
<li>\([3, 4, 8, 9, 14]\) (previous sum + 5)</li>
<li>\([3, 4, 8, 9, 14, 23]\) (previous sum + 9)</li>
</ul>
<p>Mathematically, for a sequence \([x_0, x_1, …, x_n]\), the prefix sum produces:
\[ [x_0, x_0+x_1, x_0+x_1+x_2, …, \sum_{i=0}^n x_i] \]</p>
<p>While a sequential algorithm would need \(O(n)\) steps, our parallel approach will use a clever two-phase algorithm that completes in \(O(\log n)\) steps! Here’s a visualization of this process:</p>
<p>This puzzle is split into two parts to help you master the concept:</p>
<ul>
<li>
<p><a href="puzzle_12/./simple.html">Simple Version</a>
Start with a single block implementation where all data fits in shared memory. This helps understand the core parallel algorithm.</p>
</li>
<li>
<p><a href="puzzle_12/./complete.html">Complete Version</a>
Then tackle the more challenging case of handling larger arrays that span multiple blocks, requiring coordination between blocks.</p>
</li>
</ul>
<p>Each version builds on the previous one, helping you develop a deep understanding of parallel prefix sum computation. The simple version establishes the fundamental algorithm, while the complete version shows how to scale it to larger datasets - a common requirement in real-world GPU applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-version"><a class="header" href="#simple-version">Simple Version</a></h1>
<h2 id="configuration-10"><a class="header" href="#configuration-10">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Data loading</strong>: Each thread loads one element using LayoutTensor access</li>
<li><strong>Memory pattern</strong>: Shared memory for intermediate results using <code>LayoutTensorBuild</code></li>
<li><strong>Thread sync</strong>: Coordination between computation phases</li>
<li><strong>Access pattern</strong>: Stride-based parallel computation</li>
<li><strong>Type safety</strong>: Leveraging LayoutTensor’s type system</li>
</ul>
<h2 id="code-to-complete-18"><a class="header" href="#code-to-complete-18">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb
from math import log2


alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn prefix_sum_simple[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 12 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p12/p12.mojo" class="filename">View full file: problems/p12/p12.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load data into <code>shared[local_i]</code></li>
<li>Use <code>offset = 1</code> and double it each step</li>
<li>Add elements where <code>local_i &gt;= offset</code></li>
<li>Call <code>barrier()</code> between steps</li>
</ol>
</div>
</details>
<h3 id="running-the-code-19"><a class="header" href="#running-the-code-19">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p12 --simple
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: DeviceBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])
</code></pre>
<h2 id="solution-18"><a class="header" href="#solution-18">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn prefix_sum_simple[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared = tb[dtype]().row_major[TPB]().shared().alloc()
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    offset = 1
    for i in range(Int(log2(Scalar[dtype](TPB)))):
        if local_i &gt;= offset and local_i &lt; size:
            shared[local_i] += shared[local_i - offset]

        barrier()
        offset *= 2

    if global_i &lt; size:
        out[global_i] = shared[local_i]


</code></pre>
<div class="solution-explanation">
<p>The parallel (inclusive) prefix-sum algorithm works as follows:</p>
<h3 id="setup--configuration"><a class="header" href="#setup--configuration">Setup &amp; Configuration</a></h3>
<ul>
<li><code>TPB</code> (Threads Per Block) = 8</li>
<li><code>SIZE</code> (Array Size) = 8</li>
</ul>
<h3 id="thread-mapping"><a class="header" href="#thread-mapping">Thread Mapping</a></h3>
<ul>
<li><code>thread_idx.x</code>: \([0, 1, 2, 3, 4, 5, 6, 7]\) (<code>local_i</code>)</li>
<li><code>block_idx.x</code>: \([0, 0, 0, 0, 0, 0, 0, 0]\)</li>
<li><code>global_i</code>: \([0, 1, 2, 3, 4, 5, 6, 7]\) (<code>block_idx.x * TPB + thread_idx.x</code>)</li>
</ul>
<h3 id="initial-load-to-shared-memory"><a class="header" href="#initial-load-to-shared-memory">Initial Load to Shared Memory</a></h3>
<pre><code class="language-txt">Threads:      T₀   T₁   T₂   T₃   T₄   T₅   T₆   T₇
Input array:  [0    1    2    3    4    5    6    7]
shared:       [0    1    2    3    4    5    6    7]
               ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑
              T₀   T₁   T₂   T₃   T₄   T₅   T₆   T₇
</code></pre>
<h3 id="offset--1-first-parallel-step"><a class="header" href="#offset--1-first-parallel-step">Offset = 1: First Parallel Step</a></h3>
<p>Active threads: \(T_1 \ldots T_7\) (where <code>local_i ≥ 1</code>)</p>
<pre><code class="language-txt">Before:      [0    1    2    3    4    5    6    7]
Add:              +0   +1   +2   +3   +4   +5   +6
                   |    |    |    |    |    |    |
Result:      [0    1    3    6    7    9    11   13]
                   ↑    ↑    ↑    ↑    ↑    ↑    ↑
                  T₁   T₂   T₃   T₄   T₅   T₆   T₇
</code></pre>
<h3 id="offset--2-second-parallel-step"><a class="header" href="#offset--2-second-parallel-step">Offset = 2: Second Parallel Step</a></h3>
<p>Active threads: \(T_2 \ldots T_7\) (where <code>local_i ≥ 2</code>)</p>
<pre><code class="language-txt">Before:      [0    1    3    6    7    9    11   13]
Add:                   +0   +1   +3   +6   +7   +9
                        |    |    |    |    |    |
Result:      [0    1    3    7    10   15   18   22]
                        ↑    ↑    ↑    ↑    ↑    ↑
                       T₂   T₃   T₄   T₅   T₆   T₇
</code></pre>
<h3 id="offset--4-third-parallel-step"><a class="header" href="#offset--4-third-parallel-step">Offset = 4: Third Parallel Step</a></h3>
<p>Active threads: \(T_4 \ldots T_7\) (where <code>local_i ≥ 4</code>)</p>
<pre><code class="language-txt">Before:      [0    1    3    7    10   15   18   22]
Add:                              +0   +1   +3   +7
                                  |    |    |    |
Result:      [0    1    3    7    10   16   21   28]
                                  ↑    ↑    ↑    ↑
                                  T₄   T₅   T₆   T₇
</code></pre>
<h3 id="final-write-to-output"><a class="header" href="#final-write-to-output">Final Write to Output</a></h3>
<pre><code class="language-txt">Threads:      T₀   T₁   T₂   T₃   T₄   T₅   T₆   T₇
global_i:     0    1    2    3    4    5    6    7
out[]:       [0    1    3    7    10   16   21   28]
              ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑
              T₀   T₁   T₂   T₃   T₄   T₅   T₆   T₇
</code></pre>
<h3 id="thread-by-thread-execution"><a class="header" href="#thread-by-thread-execution">Thread-by-Thread Execution</a></h3>
<p><strong>\(T_0\) (<code>local_i=0</code>):</strong></p>
<ul>
<li>Loads <code>shared[0] = 0</code></li>
<li>Never adds (<code>local_i &lt; offset</code> always)</li>
<li>Writes <code>out[0] = 0</code></li>
</ul>
<p><strong>\(T_1\) (<code>local_i=1</code>):</strong></p>
<ul>
<li>Loads <code>shared[1] = 1</code></li>
<li><code>offset=1</code>: adds <code>shared[0]</code> → 1</li>
<li><code>offset=2,4</code>: no action (<code>local_i &lt; offset</code>)</li>
<li>Writes <code>out[1] = 1</code></li>
</ul>
<p><strong>\(T_2\) (<code>local_i=2</code>):</strong></p>
<ul>
<li>Loads <code>shared[2] = 2</code></li>
<li><code>offset=1</code>: adds <code>shared[1]</code> → 3</li>
<li><code>offset=2</code>: adds <code>shared[0]</code> → 3</li>
<li><code>offset=4</code>: no action</li>
<li>Writes <code>out[2] = 3</code></li>
</ul>
<p><strong>\(T_3\) (<code>local_i=3</code>):</strong></p>
<ul>
<li>Loads <code>shared[3] = 3</code></li>
<li><code>offset=1</code>: adds <code>shared[2]</code> → 6</li>
<li><code>offset=2</code>: adds <code>shared[1]</code> → 7</li>
<li><code>offset=4</code>: no action</li>
<li>Writes <code>out[3] = 7</code></li>
</ul>
<p><strong>\(T_4\) (<code>local_i=4</code>):</strong></p>
<ul>
<li>Loads <code>shared[4] = 4</code></li>
<li><code>offset=1</code>: adds <code>shared[3]</code> → 7</li>
<li><code>offset=2</code>: adds <code>shared[2]</code> → 10</li>
<li><code>offset=4</code>: adds <code>shared[0]</code> → 10</li>
<li>Writes <code>out[4] = 10</code></li>
</ul>
<p><strong>\(T_5\) (<code>local_i=5</code>):</strong></p>
<ul>
<li>Loads <code>shared[5] = 5</code></li>
<li><code>offset=1</code>: adds <code>shared[4]</code> → 9</li>
<li><code>offset=2</code>: adds <code>shared[3]</code> → 15</li>
<li><code>offset=4</code>: adds <code>shared[1]</code> → 16</li>
<li>Writes <code>out[5] = 16</code></li>
</ul>
<p><strong>\(T_6\) (<code>local_i=6</code>):</strong></p>
<ul>
<li>Loads <code>shared[6] = 6</code></li>
<li><code>offset=1</code>: adds <code>shared[5]</code> → 11</li>
<li><code>offset=2</code>: adds <code>shared[4]</code> → 18</li>
<li><code>offset=4</code>: adds <code>shared[2]</code> → 21</li>
<li>Writes <code>out[6] = 21</code></li>
</ul>
<p><strong>\(T_7\) (<code>local_i=7</code>):</strong></p>
<ul>
<li>Loads <code>shared[7] = 7</code></li>
<li><code>offset=1</code>: adds <code>shared[6]</code> → 13</li>
<li><code>offset=2</code>: adds <code>shared[5]</code> → 22</li>
<li><code>offset=4</code>: adds <code>shared[3]</code> → 28</li>
<li>Writes <code>out[7] = 28</code></li>
</ul>
<p>The solution ensures correct synchronization between phases using <code>barrier()</code> and handles array bounds checking with <code>if global_i &lt; size</code>. The final result produces the inclusive prefix sum where each element \(i\) contains \(\sum_{j=0}^{i} a[j]\).</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="complete-version"><a class="header" href="#complete-version">Complete Version</a></h1>
<h2 id="configuration-11"><a class="header" href="#configuration-11">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE_2 = 15</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Block handling</strong>: Multiple blocks process array segments</li>
<li><strong>Partial blocks</strong>: Last block may not be full</li>
<li><strong>Block sums</strong>: Store running totals between blocks</li>
<li><strong>Global result</strong>: Combine local and block sums</li>
<li><strong>Layout safety</strong>: Consistent layout handling through LayoutTensor</li>
</ul>
<h2 id="code-to-complete-19"><a class="header" href="#code-to-complete-19">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE_2 = 15
alias BLOCKS_PER_GRID_2 = (2, 1)
alias THREADS_PER_BLOCK_2 = (TPB, 1)


fn prefix_sum[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 19 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p12/p12.mojo" class="filename">View full file: problems/p12/p12.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Compute local prefix sums like in <a href="puzzle_12/./simple.html">Simple Version</a></li>
<li>Last thread stores block sum at <code>TPB * (block_idx.x + 1)</code></li>
<li>Add previous block’s sum to current block</li>
<li>Handle array bounds for all operations</li>
</ol>
</div>
</details>
<h3 id="running-the-code-20"><a class="header" href="#running-the-code-20">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p12 --complete
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: DeviceBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])
</code></pre>
<h2 id="solution-19"><a class="header" href="#solution-19">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn prefix_sum[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared = tb[dtype]().row_major[TPB]().shared().alloc()
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    # Idea: two passes
    # SIZE=15, TPB=8, BLOCKS=2
    # buffer: [0,1,2,...,7 | 8,...,14]

    # Step 1: Each block computes local prefix sum
    # Block 0: [0,1,2,3,4,5,6,7] → [0,1,3,6,10,15,21,28]
    # Block 1: [8,9,10,11,12,13,14] → [8,17,27,38,50,63,77]

    # Step 2: Store block sums
    # Block 0's sum (28) → position 8

    # Step 3: Add previous block's sum
    # Block 1: Each element += 28
    # [8,17,27,38,50,63,77] → [36,45,55,66,78,91,105]

    # Final result combines both blocks:
    # [0,1,3,6,10,15,21,28, 36,45,55,66,78,91,105]

    # local prefix-sum for each block
    offset = 1
    for i in range(Int(log2(Scalar[dtype](TPB)))):
        if local_i &gt;= offset and local_i &lt; size:
            shared[local_i] += shared[local_i - offset]

        barrier()
        offset *= 2

    # store block results
    if global_i &lt; size:
        out[global_i] = shared[local_i]

    # store block sum in first element of next block:
    # - Only last thread (local_i == 7) in each block except last block executes
    # - Block 0: Thread 7 stores 28 (sum of 0-7) at position 8 (start of Block 1)
    # - Calculation: TPB * (block_idx.x + 1)
    # Block 0: 8 * (0 + 1) = position 8
    # Block 1: No action (last block)
    # Memory state:
    # [0,1,3,6,10,15,21,28 | 28,45,55,66,78,91,105]
    #                        ↑
    #                       Block 0's sum stored here
    if local_i == TPB - 1 and block_idx.x &lt; size // TPB - 1:
        out[TPB * (block_idx.x + 1)] = shared[local_i]

    # wait for all blocks to store their sums
    barrier()

    # second pass: add previous block's sum which becomes:
    # Before: [8,9,10,11,12,13,14]
    # Add 28: [36,37,38,39,40,41,42]
    if block_idx.x &gt; 0 and global_i &lt; size:
        shared[local_i] += out[block_idx.x * TPB - 1]

    # final result
    if global_i &lt; size:
        out[global_i] = shared[local_i]


</code></pre>
<div class="solution-explanation">
This solution handles multi-block prefix sum in three main phases:
<ol>
<li>
<p>Local prefix sum (per block):</p>
<pre><code>Block 0 (8 elements):    [0,1,2,3,4,5,6,7]
After local prefix sum:  [0,1,3,7,10,16,21,28]

Block 1 (7 elements):    [8,9,10,11,12,13,14]
After local prefix sum:  [8,17,27,38,50,63,77]
</code></pre>
</li>
<li>
<p>Block sum communication:</p>
<ul>
<li>Last thread (local_i == TPB-1) in each non-final block</li>
<li>Stores its block’s sum at next block’s start:</li>
</ul>
<pre><code class="language-mojo">if local_i == TPB - 1 and block_idx.x &lt; size // TPB - 1:
    out[TPB * (block_idx.x + 1)] = shared[local_i]
</code></pre>
<ul>
<li>Block 0’s sum (28) stored at position 8</li>
<li>Memory layout: <code>[0,1,3,7,10,16,21,28 | 28,17,27,38,50,63,77]</code>
↑
Block 0’s sum</li>
</ul>
</li>
<li>
<p>Final adjustment:</p>
<ul>
<li>Each block after first adds previous block’s sum</li>
</ul>
<pre><code class="language-mojo">if block_idx.x &gt; 0 and global_i &lt; size:
    shared[local_i] += out[block_idx.x * TPB - 1]
</code></pre>
<ul>
<li>Block 1: Each element += 28</li>
<li>Final result: <code>[0,1,3,7,10,16,21,28, 36,45,55,66,78,91,105]</code></li>
</ul>
</li>
</ol>
<p>Key implementation details:</p>
<ul>
<li>Uses <code>barrier()</code> after shared memory operations</li>
<li>Handles partial blocks (last block size &lt; TPB)</li>
<li>Guards all operations with proper bounds checking</li>
<li>Maintains correct thread and block synchronization</li>
<li>Achieves \(O(\log n)\) complexity per block</li>
</ul>
<p>The solution scales to arbitrary-sized inputs by combining local prefix sums with efficient block-to-block communication.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-13-axis-sum"><a class="header" href="#puzzle-13-axis-sum">Puzzle 13: Axis Sum</a></h1>
<h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>Implement a kernel that computes a sum over each row of 2D matrix <code>a</code> and stores it in <code>out</code> using LayoutTensor.</p>
<p><img src="puzzle_13/./media/videos/720p30/puzzle_13_viz.gif" alt="Axis Sum visualization" /></p>
<h2 id="key-concepts-22"><a class="header" href="#key-concepts-22">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Parallel reduction along matrix dimensions using LayoutTensor</li>
<li>Using block coordinates for data partitioning</li>
<li>Efficient shared memory reduction patterns</li>
<li>Working with multi-dimensional tensor layouts</li>
</ul>
<p>The key insight is understanding how to map thread blocks to matrix rows and perform efficient parallel reduction within each block while leveraging LayoutTensor’s dimensional indexing.</p>
<h2 id="configuration-12"><a class="header" href="#configuration-12">Configuration</a></h2>
<ul>
<li>Matrix dimensions: \(\text{BATCH} \times \text{SIZE} = 4 \times 6\)</li>
<li>Threads per block: \(\text{TPB} = 8\)</li>
<li>Grid dimensions: \(1 \times \text{BATCH}\)</li>
<li>Shared memory: \(\text{TPB}\) elements per block</li>
<li>Input layout: <code>Layout.row_major(BATCH, SIZE)</code></li>
<li>Output layout: <code>Layout.row_major(BATCH, 1)</code></li>
</ul>
<p>Matrix visualization:</p>
<pre><code class="language-txt">Row 0: [0, 1, 2, 3, 4, 5]       → Block(0,0)
Row 1: [6, 7, 8, 9, 10, 11]     → Block(0,1)
Row 2: [12, 13, 14, 15, 16, 17] → Block(0,2)
Row 3: [18, 19, 20, 21, 22, 23] → Block(0,3)
</code></pre>
<h2 id="code-to-complete-20"><a class="header" href="#code-to-complete-20">Code to Complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 8
alias BATCH = 4
alias SIZE = 6
alias BLOCKS_PER_GRID = (1, BATCH)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias in_layout = Layout.row_major(BATCH, SIZE)
alias out_layout = Layout.row_major(BATCH, 1)


fn axis_sum[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    batch = block_idx.y
    # FILL ME IN (roughly 15 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p13/p13.mojo" class="filename">View full file: problems/p13/p13.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>batch = block_idx.y</code> to select row</li>
<li>Load elements: <code>cache[local_i] = a[batch * size + local_i]</code></li>
<li>Perform parallel reduction with halving stride</li>
<li>Thread 0 writes final sum to <code>out[batch]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-21"><a class="header" href="#running-the-code-21">Running the Code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p13
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: DeviceBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([15.0, 51.0, 87.0, 123.0])
</code></pre>
<h2 id="solution-20"><a class="header" href="#solution-20">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn axis_sum[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    batch = block_idx.y
    cache = tb[dtype]().row_major[TPB]().shared().alloc()

    # Visualize:
    # Block(0,0): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 0: [0,1,2,3,4,5]
    # Block(0,1): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 1: [6,7,8,9,10,11]
    # Block(0,2): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 2: [12,13,14,15,16,17]
    # Block(0,3): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 3: [18,19,20,21,22,23]

    # each row is handled by each block bc we have grid_dim=(1, BATCH)

    if local_i &lt; size:
        cache[local_i] = a[batch, local_i]
    else:
        # Add zero-initialize padding elements for later reduction
        cache[local_i] = 0

    barrier()

    # do reduction sum per each block
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            cache[local_i] += cache[local_i + stride]

        barrier()
        stride //= 2

    # writing with local thread = 0 that has the sum for each batch
    if local_i == 0:
        out[batch, 0] = cache[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel row-wise sum reduction for a 2D matrix using LayoutTensor. Here’s a comprehensive breakdown:</p>
<h3 id="matrix-layout-and-block-mapping"><a class="header" href="#matrix-layout-and-block-mapping">Matrix Layout and Block Mapping</a></h3>
<pre><code class="language-txt">Input Matrix (4×6) with LayoutTensor:                Block Assignment:
[[ a[0,0]  a[0,1]  a[0,2]  a[0,3]  a[0,4]  a[0,5] ] → Block(0,0)
 [ a[1,0]  a[1,1]  a[1,2]  a[1,3]  a[1,4]  a[1,5] ] → Block(0,1)
 [ a[2,0]  a[2,1]  a[2,2]  a[2,3]  a[2,4]  a[2,5] ] → Block(0,2)
 [ a[3,0]  a[3,1]  a[3,2]  a[3,3]  a[3,4]  a[3,5] ] → Block(0,3)
</code></pre>
<h3 id="parallel-reduction-process"><a class="header" href="#parallel-reduction-process">Parallel Reduction Process</a></h3>
<ol>
<li>
<p><strong>Initial Data Loading</strong>:</p>
<pre><code class="language-txt">Block(0,0): cache = [a[0,0] a[0,1] a[0,2] a[0,3] a[0,4] a[0,5] * *]  // * = padding
Block(0,1): cache = [a[1,0] a[1,1] a[1,2] a[1,3] a[1,4] a[1,5] * *]
Block(0,2): cache = [a[2,0] a[2,1] a[2,2] a[2,3] a[2,4] a[2,5] * *]
Block(0,3): cache = [a[3,0] a[3,1] a[3,2] a[3,3] a[3,4] a[3,5] * *]
</code></pre>
</li>
<li>
<p><strong>Reduction Steps</strong> (for Block 0,0):</p>
<pre><code class="language-txt">Initial:  [0  1  2  3  4  5  *  *]
Stride 4: [4  5  6  7  4  5  *  *]
Stride 2: [10 12 6  7  4  5  *  *]
Stride 1: [15 12 6  7  4  5  *  *]
</code></pre>
</li>
</ol>
<h3 id="key-implementation-features-2"><a class="header" href="#key-implementation-features-2">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Layout Configuration</strong>:</p>
<ul>
<li>Input: row-major layout (BATCH × SIZE)</li>
<li>Output: row-major layout (BATCH × 1)</li>
<li>Each block processes one complete row</li>
</ul>
</li>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>LayoutTensor 2D indexing for input: <code>a[batch, local_i]</code></li>
<li>Shared memory for efficient reduction</li>
<li>LayoutTensor 2D indexing for output: <code>out[batch, 0]</code></li>
</ul>
</li>
<li>
<p><strong>Parallel Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; size:
        cache[local_i] += cache[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
</li>
<li>
<p><strong>Output Writing</strong>:</p>
<pre><code class="language-mojo">if local_i == 0:
    out[batch, 0] = cache[0]  --&gt; One result per batch
</code></pre>
</li>
</ol>
<h3 id="performance-optimizations-1"><a class="header" href="#performance-optimizations-1">Performance Optimizations:</a></h3>
<ol>
<li>
<p><strong>Memory Efficiency</strong>:</p>
<ul>
<li>Coalesced memory access through LayoutTensor</li>
<li>Shared memory for fast reduction</li>
<li>Single write per row result</li>
</ul>
</li>
<li>
<p><strong>Thread Utilization</strong>:</p>
<ul>
<li>Perfect load balancing across rows</li>
<li>No thread divergence in main computation</li>
<li>Efficient parallel reduction pattern</li>
</ul>
</li>
<li>
<p><strong>Synchronization</strong>:</p>
<ul>
<li>Minimal barriers (only during reduction)</li>
<li>Independent processing between rows</li>
<li>No inter-block communication needed</li>
</ul>
</li>
</ol>
<h3 id="complexity-analysis"><a class="header" href="#complexity-analysis">Complexity Analysis:</a></h3>
<ul>
<li>Time: \(O(\log n)\) per row, where n is row length</li>
<li>Space: \(O(TPB)\) shared memory per block</li>
<li>Total parallel time: \(O(\log n)\) with sufficient threads</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-14-matrix-multiplication-matmul"><a class="header" href="#puzzle-14-matrix-multiplication-matmul">Puzzle 14: Matrix Multiplication (MatMul)</a></h1>
<h2 id="overview-16"><a class="header" href="#overview-16">Overview</a></h2>
<p>Matrix multiplication is a fundamental operation in scientific computing, machine learning, and graphics. Given two matrices \(A\) and \(B\), we want to compute their product \(C = A \times B.\)</p>
<p>For matrices \(A_{m\times k}\) and \(B_{k\times n}\), each element of the result \(C_{m\times n}\) is computed as:</p>
<p>\[\Large C_{ij} = \sum_{l=0}^{k-1} A_{il} \cdot B_{lj} \]</p>
<p><img src="puzzle_14/./media/videos/720p30/puzzle_14_viz.gif" alt="Matrix Multiply visualization" /></p>
<p>This puzzle explores different approaches to implementing matrix multiplication on GPUs, each with its own performance characteristics:</p>
<ul>
<li>
<p><a href="puzzle_14/./naive.html">Naive Version</a>
The straightforward implementation where each thread computes one element of the output matrix. While simple to understand, this approach makes many redundant memory accesses.</p>
</li>
<li>
<p><a href="puzzle_14/./shared_memory.html">Shared Memory Version</a>
Improves performance by loading blocks of input matrices into fast shared memory, reducing global memory accesses. Each thread still computes one output element but reads from shared memory.</p>
</li>
<li>
<p><a href="puzzle_14/./tiled.html">Tiled Version</a>
Further optimizes by dividing the computation into tiles, allowing threads to cooperate on loading and computing blocks of the output matrix. This approach better utilizes memory hierarchy and thread cooperation.</p>
</li>
</ul>
<p>Each version builds upon the previous one, introducing new optimization techniques common in GPU programming. You’ll learn how different memory access patterns and thread cooperation strategies affect performance.</p>
<p>The progression illustrates a common pattern in GPU optimization:</p>
<ol>
<li>Start with a correct but naive implementation</li>
<li>Reduce global memory access with shared memory</li>
<li>Improve data locality and thread cooperation with tiling</li>
<li>Use high-level abstractions while maintaining performance</li>
</ol>
<p>Choose a version to begin your matrix multiplication journey!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="naive-matrix-multiplication"><a class="header" href="#naive-matrix-multiplication">Naive Matrix Multiplication</a></h1>
<h2 id="overview-17"><a class="header" href="#overview-17">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(\text{transpose}(A)\) and stores the result in \(\text{out}\).
This is the most straightforward implementation where each thread computes one element of the output matrix.</p>
<h2 id="key-concepts-23"><a class="header" href="#key-concepts-23">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>2D thread organization for matrix operations</li>
<li>Global memory access patterns</li>
<li>Matrix indexing in row-major layout</li>
<li>Thread-to-output element mapping</li>
</ul>
<p>The key insight is understanding how to map 2D thread indices to matrix elements and compute dot products in parallel.</p>
<h2 id="configuration-13"><a class="header" href="#configuration-13">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE} \times \text{SIZE} = 2 \times 2\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(1 \times 1\)</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Input B: <code>Layout.row_major(SIZE, SIZE)</code> (transpose of A)</li>
<li>Output: <code>Layout.row_major(SIZE, SIZE)</code></li>
</ul>
<p>Memory layout (LayoutTensor):</p>
<ul>
<li>Matrix A: <code>a[i, j]</code> directly indexes position (i,j)</li>
<li>Matrix B: <code>b[i, j]</code> is the transpose of A</li>
<li>Output C: <code>out[i, j]</code> stores result at position (i,j)</li>
</ul>
<h2 id="code-to-complete-21"><a class="header" href="#code-to-complete-21">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 3
alias SIZE = 2
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, TPB)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE, SIZE)


fn naive_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    # FILL ME IN (roughly 6 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate <code>global_i</code> and <code>global_j</code> from thread indices</li>
<li>Check if indices are within <code>size</code></li>
<li>Accumulate products in a local variable</li>
<li>Write final sum to correct output position</li>
</ol>
</div>
</details>
<h2 id="running-the-code-22"><a class="header" href="#running-the-code-22">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --naive
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([1.0, 3.0, 3.0, 13.0])
</code></pre>
<h2 id="solution-21"><a class="header" href="#solution-21">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn naive_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y

    if global_i &lt; size and global_j &lt; size:
        var acc: out.element_type = 0

        @parameter
        for k in range(size):
            acc += a[global_i, k] * b[k, global_j]

        out[global_i, global_j] = acc


</code></pre>
<div class="solution-explanation">
<p>The naive matrix multiplication using LayoutTensor demonstrates the basic approach:</p>
<h3 id="matrix-layout-22-example"><a class="header" href="#matrix-layout-22-example">Matrix Layout (2×2 example)</a></h3>
<pre><code class="language-txt">Matrix A:        Matrix B (transpose of A):    Output C:
 [a[0,0] a[0,1]]  [b[0,0] b[0,1]]             [c[0,0] c[0,1]]
 [a[1,0] a[1,1]]  [b[1,0] b[1,1]]             [c[1,0] c[1,1]]
</code></pre>
<h3 id="implementation-details-2"><a class="header" href="#implementation-details-2">Implementation Details:</a></h3>
<ol>
<li>
<p><strong>Thread Mapping</strong>:</p>
<pre><code class="language-mojo">global_i = block_dim.x * block_idx.x + thread_idx.x
global_j = block_dim.y * block_idx.y + thread_idx.y
</code></pre>
</li>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>Direct 2D indexing: <code>a[global_i, k]</code></li>
<li>Transposed access: <code>b[k, global_j]</code></li>
<li>Output writing: <code>out[global_i, global_j]</code></li>
</ul>
</li>
<li>
<p><strong>Computation Flow</strong>:</p>
<pre><code class="language-mojo"># Use var for mutable accumulator with tensor's element type
var acc: out.element_type = 0

# @parameter for compile-time loop unrolling
@parameter
for k in range(size):
    acc += a[global_i, k] * b[k, global_j]
</code></pre>
</li>
</ol>
<h3 id="key-language-features"><a class="header" href="#key-language-features">Key Language Features:</a></h3>
<ol>
<li>
<p><strong>Variable Declaration</strong>:</p>
<ul>
<li>The use of <code>var</code> in <code>var acc: out.element_type = 0</code> allows for type inference with <code>out.element_type</code> ensures type compatibility with the output tensor</li>
<li>Initialized to zero before accumulation</li>
</ul>
</li>
<li>
<p><strong>Loop Optimization</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-for-statement"><code>@parameter</code></a> decorator unrolls the loop at compile time</li>
<li>Improves performance for small, known matrix sizes</li>
<li>Enables better instruction scheduling</li>
</ul>
</li>
</ol>
<h3 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics:</a></h3>
<ol>
<li>
<p><strong>Memory Access</strong>:</p>
<ul>
<li>Each thread makes <code>2 x SIZE</code> global memory reads</li>
<li>One global memory write per thread</li>
<li>No data reuse between threads</li>
</ul>
</li>
<li>
<p><strong>Computational Efficiency</strong>:</p>
<ul>
<li>Simple implementation but suboptimal performance</li>
<li>Many redundant global memory accesses</li>
<li>No use of fast shared memory</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>High global memory bandwidth usage</li>
<li>Poor data locality</li>
<li>Limited scalability for large matrices</li>
</ul>
</li>
</ol>
<p>This naive implementation serves as a baseline for understanding matrix multiplication on GPUs, highlighting the need for optimization in memory access patterns.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-memory-matrix-multiplication"><a class="header" href="#shared-memory-matrix-multiplication">Shared Memory Matrix Multiplication</a></h1>
<h2 id="overview-18"><a class="header" href="#overview-18">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(\text{transpose}(A)\) and stores the result in \(\text{out}\), using shared memory to improve memory access efficiency. This version loads matrix blocks into shared memory before computation.</p>
<h2 id="key-concepts-24"><a class="header" href="#key-concepts-24">Key concepts</a></h2>
<p>In this puzzle, you’ll learn about:</p>
<ul>
<li>Block-local memory management with LayoutTensor</li>
<li>Thread synchronization patterns</li>
<li>Memory access optimization using shared memory</li>
<li>Collaborative data loading with 2D indexing</li>
<li>Efficient use of LayoutTensor for matrix operations</li>
</ul>
<p>The key insight is understanding how to use fast shared memory with LayoutTensor to reduce expensive global memory operations.</p>
<h2 id="configuration-14"><a class="header" href="#configuration-14">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE} \times \text{SIZE} = 2 \times 2\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(1 \times 1\)</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Input B: <code>Layout.row_major(SIZE, SIZE)</code> (transpose of A)</li>
<li>Output: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Shared Memory: Two <code>TPB × TPB</code> LayoutTensors</li>
</ul>
<p>Memory organization:</p>
<pre><code class="language-txt">Global Memory (LayoutTensor):          Shared Memory (LayoutTensor):
A[i,j]: Direct access                  a_shared[local_i, local_j]
B[i,j]: Transposed access              b_shared[local_i, local_j]
</code></pre>
<h2 id="code-to-complete-22"><a class="header" href="#code-to-complete-22">Code to complete</a></h2>
<pre><code class="language-mojo">fn single_block_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    local_i = thread_idx.x
    local_j = thread_idx.y
    # FILL ME IN (roughly 12 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load matrices to shared memory using global and local indices</li>
<li>Call <code>barrier()</code> after loading</li>
<li>Compute dot product using shared memory indices</li>
<li>Check array bounds for all operations</li>
</ol>
</div>
</details>
<h2 id="running-the-code-23"><a class="header" href="#running-the-code-23">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --single-block
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([1.0, 3.0, 3.0, 13.0])
</code></pre>
<h2 id="solution-22"><a class="header" href="#solution-22">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn single_block_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    global_j = block_dim.y * block_idx.y + thread_idx.y
    local_i = thread_idx.x
    local_j = thread_idx.y
    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    if global_i &lt; size and global_j &lt; size:
        a_shared[local_i, local_j] = a[global_i, global_j]
        b_shared[local_i, local_j] = b[global_i, global_j]

    barrier()

    if global_i &lt; size and global_j &lt; size:
        var acc: out.element_type = 0

        @parameter
        for k in range(size):
            acc += a_shared[local_i, k] * b_shared[k, local_j]

        out[global_i, global_j] = acc


</code></pre>
<div class="solution-explanation">
<p>The shared memory implementation with LayoutTensor improves performance through efficient memory access patterns:</p>
<h3 id="memory-organization"><a class="header" href="#memory-organization">Memory Organization</a></h3>
<pre><code class="language-txt">Input Tensors (2×2):                Shared Memory (3×3):
Matrix A:                           a_shared:
 [a[0,0] a[0,1]]                    [s[0,0] s[0,1] s[0,2]]
 [a[1,0] a[1,1]]                    [s[1,0] s[1,1] s[1,2]]
                                    [s[2,0] s[2,1] s[2,2]]
Matrix B (transpose):               b_shared: (similar layout)
 [b[0,0] b[0,1]]                    [t[0,0] t[0,1] t[0,2]]
 [b[1,0] b[1,1]]                    [t[1,0] t[1,1] t[1,2]]
                                    [t[2,0] t[2,1] t[2,2]]
</code></pre>
<h3 id="implementation-phases"><a class="header" href="#implementation-phases">Implementation Phases:</a></h3>
<ol>
<li>
<p><strong>Shared Memory Setup</strong>:</p>
<pre><code class="language-mojo"># Create 2D shared memory tensors using TensorBuilder
a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
</code></pre>
</li>
<li>
<p><strong>Thread Indexing</strong>:</p>
<pre><code class="language-mojo"># Global indices for matrix access
global_i = block_dim.x * block_idx.x + thread_idx.x
global_j = block_dim.y * block_idx.y + thread_idx.y

# Local indices for shared memory
local_i = thread_idx.x
local_j = thread_idx.y
</code></pre>
</li>
<li>
<p><strong>Data Loading</strong>:</p>
<pre><code class="language-mojo"># Load data into shared memory using LayoutTensor indexing
if global_i &lt; size and global_j &lt; size:
    a_shared[local_i, local_j] = a[global_i, global_j]
    b_shared[local_i, local_j] = b[global_i, global_j]
</code></pre>
</li>
<li>
<p><strong>Computation with Shared Memory</strong>:</p>
<pre><code class="language-mojo"># Guard ensures we only compute for valid matrix elements
if global_i &lt; size and global_j &lt; size:
    # Initialize accumulator with output tensor's type
    var acc: out.element_type = 0

    # Compile-time unrolled loop for matrix multiplication
    @parameter
    for k in range(size):
        acc += a_shared[local_i, k] * b_shared[k, local_j]

    # Write result only for threads within matrix bounds
    out[global_i, global_j] = acc
</code></pre>
<p>Key aspects:</p>
<ul>
<li>
<p><strong>Boundary Check</strong>: <code>if global_i &lt; size and global_j &lt; size</code></p>
<ul>
<li>Prevents out-of-bounds computation</li>
<li>Only valid threads perform work</li>
<li>Essential because TPB (3×3) &gt; SIZE (2×2)</li>
</ul>
</li>
<li>
<p><strong>Accumulator Type</strong>: <code>var acc: out.element_type</code></p>
<ul>
<li>Uses output tensor’s element type for type safety</li>
<li>Ensures consistent numeric precision</li>
<li>Initialized to zero before accumulation</li>
</ul>
</li>
<li>
<p><strong>Loop Optimization</strong>: <code>@parameter for k in range(size)</code></p>
<ul>
<li>Unrolls the loop at compile time</li>
<li>Enables better instruction scheduling</li>
<li>Efficient for small, known matrix sizes</li>
</ul>
</li>
<li>
<p><strong>Result Writing</strong>: <code>out[global_i, global_j] = acc</code></p>
<ul>
<li>Protected by the same guard condition</li>
<li>Only valid threads write results</li>
<li>Maintains matrix bounds safety</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="thread-safety-and-synchronization"><a class="header" href="#thread-safety-and-synchronization">Thread Safety and Synchronization:</a></h3>
<ol>
<li>
<p><strong>Guard Conditions</strong>:</p>
<ul>
<li>Input Loading: <code>if global_i &lt; size and global_j &lt; size</code></li>
<li>Computation: Same guard ensures thread safety</li>
<li>Output Writing: Protected by the same condition</li>
<li>Prevents invalid memory access and race conditions</li>
</ul>
</li>
<li>
<p><strong>Memory Access Safety</strong>:</p>
<ul>
<li>Shared memory: Accessed only within TPB bounds</li>
<li>Global memory: Protected by size checks</li>
<li>Output: Guarded writes prevent corruption</li>
</ul>
</li>
</ol>
<h3 id="key-language-features-1"><a class="header" href="#key-language-features-1">Key Language Features:</a></h3>
<ol>
<li>
<p><strong>LayoutTensor Benefits</strong>:</p>
<ul>
<li>Direct 2D indexing simplifies code</li>
<li>Type safety through <code>element_type</code></li>
<li>Efficient memory layout handling</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Allocation</strong>:</p>
<ul>
<li>TensorBuilder for structured allocation</li>
<li>Row-major layout matching input tensors</li>
<li>Proper alignment for efficient access</li>
</ul>
</li>
<li>
<p><strong>Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> ensures shared memory consistency</li>
<li>Proper synchronization between load and compute</li>
<li>Thread cooperation within block</li>
</ul>
</li>
</ol>
<h3 id="performance-optimizations-2"><a class="header" href="#performance-optimizations-2">Performance Optimizations:</a></h3>
<ol>
<li>
<p><strong>Memory Access Efficiency</strong>:</p>
<ul>
<li>Single global memory load per element</li>
<li>Multiple reuse through shared memory</li>
<li>Coalesced memory access patterns</li>
</ul>
</li>
<li>
<p><strong>Thread Cooperation</strong>:</p>
<ul>
<li>Collaborative data loading</li>
<li>Shared data reuse</li>
<li>Efficient thread synchronization</li>
</ul>
</li>
<li>
<p><strong>Computational Benefits</strong>:</p>
<ul>
<li>Reduced global memory traffic</li>
<li>Better cache utilization</li>
<li>Improved instruction throughput</li>
</ul>
</li>
</ol>
<p>This implementation significantly improves performance over the naive version by:</p>
<ul>
<li>Reducing global memory accesses</li>
<li>Enabling data reuse through shared memory</li>
<li>Using efficient 2D indexing with LayoutTensor</li>
<li>Maintaining proper thread synchronization</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tiled-matrix-multiplication"><a class="header" href="#tiled-matrix-multiplication">Tiled Matrix Multiplication</a></h1>
<h2 id="overview-19"><a class="header" href="#overview-19">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(\text{transpose}(A)\) using tiled matrix multiplication with LayoutTensor. This approach handles large matrices by processing them in smaller chunks (tiles).</p>
<h2 id="key-concepts-25"><a class="header" href="#key-concepts-25">Key concepts</a></h2>
<ul>
<li>Matrix tiling with LayoutTensor for large-scale computation</li>
<li>Multi-block coordination with proper layouts</li>
<li>Efficient shared memory usage through TensorBuilder</li>
<li>Boundary handling for tiles with LayoutTensor indexing</li>
</ul>
<h2 id="configuration-15"><a class="header" href="#configuration-15">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE_TILED} = 8\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(3 \times 3\) blocks</li>
<li>Shared memory: Two \(\text{TPB} \times \text{TPB}\) LayoutTensors per block</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code></li>
<li>Input B: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code> (transpose of A)</li>
<li>Output: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code></li>
<li>Shared Memory: Two <code>TPB × TPB</code> LayoutTensors using TensorBuilder</li>
</ul>
<h2 id="tiling-strategy"><a class="header" href="#tiling-strategy">Tiling strategy</a></h2>
<h3 id="block-organization"><a class="header" href="#block-organization">Block organization</a></h3>
<pre><code class="language-txt">Grid Layout (3×3):           Thread Layout per Block (3×3):
[B00][B01][B02]               [T00 T01 T02]
[B10][B11][B12]               [T10 T11 T12]
[B20][B21][B22]               [T20 T21 T22]

Each block processes a tile using LayoutTensor indexing
</code></pre>
<h3 id="tile-processing-steps"><a class="header" href="#tile-processing-steps">Tile processing steps</a></h3>
<ol>
<li>Load tile from matrix A into shared memory using LayoutTensor indexing</li>
<li>Load corresponding tile from matrix B into shared memory</li>
<li>Compute partial products using shared memory</li>
<li>Accumulate results in registers</li>
<li>Move to next tile</li>
<li>Repeat until all tiles are processed</li>
</ol>
<h3 id="memory-access-pattern-1"><a class="header" href="#memory-access-pattern-1">Memory access pattern</a></h3>
<pre><code class="language-txt">For each tile using LayoutTensor:
  Input Tiles:                Output Computation:
    A[i:i+TPB, k:k+TPB]   ×    Result tile
    B[k:k+TPB, j:j+TPB]   →    C[i:i+TPB, j:j+TPB]
</code></pre>
<h2 id="code-to-complete-23"><a class="header" href="#code-to-complete-23">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE_TILED = 8
alias BLOCKS_PER_GRID_TILED = (3, 3)  # each block convers 3x3 elements
alias THREADS_PER_BLOCK_TILED = (TPB, TPB)
alias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)


fn matmul_tiled[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    local_row = thread_idx.x
    local_col = thread_idx.y
    global_row = block_idx.x * TPB + local_row
    global_col = block_idx.y * TPB + local_col
    # FILL ME IN (roughly 20 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global thread positions from block and thread indices</li>
<li>Clear shared memory before loading new tiles</li>
<li>Load tiles with proper bounds checking</li>
<li>Accumulate results across tiles with proper synchronization</li>
</ol>
</div>
</details>
<h2 id="running-the-code-24"><a class="header" href="#running-the-code-24">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --tiled
</code></pre>
<p>Your output will look like this if the puzzle isn’t solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([140.0, 364.0, 588.0, 812.0, 1036.0, 1260.0, 1484.0, 1708.0, 364.0, 1100.0, 1836.0, 2572.0, 3308.0, 4044.0, 4780.0, 5516.0, 588.0, 1836.0, 3084.0, 4332.0, 5580.0, 6828.0, 8076.0, 9324.0, 812.0, 2572.0, 4332.0, 6092.0, 7852.0, 9612.0, 11372.0, 13132.0, 1036.0, 3308.0, 5580.0, 7852.0, 10124.0, 12396.0, 14668.0, 16940.0, 1260.0, 4044.0, 6828.0, 9612.0, 12396.0, 15180.0, 17964.0, 20748.0, 1484.0, 4780.0, 8076.0, 11372.0, 14668.0, 17964.0, 21260.0, 24556.0, 1708.0, 5516.0, 9324.0, 13132.0, 16940.0, 20748.0, 24556.0, 28364.0])
</code></pre>
<h2 id="solution-23"><a class="header" href="#solution-23">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn matmul_tiled[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    local_row = thread_idx.x
    local_col = thread_idx.y
    global_row = block_idx.x * TPB + local_row
    global_col = block_idx.y * TPB + local_col

    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    var acc: out.element_type = 0

    # Iterate over tiles to compute matrix product
    @parameter
    for tile in range((size + TPB - 1) // TPB):
        # Reset shared memory tiles
        if local_row &lt; TPB and local_col &lt; TPB:
            a_shared[local_row, local_col] = 0
            b_shared[local_row, local_col] = 0

        barrier()

        # Load A tile - global row stays the same, col determined by tile
        if global_row &lt; size and (tile * TPB + local_col) &lt; size:
            a_shared[local_row, local_col] = a[
                global_row, tile * TPB + local_col
            ]

        # Load B tile - row determined by tile, global col stays the same
        if (tile * TPB + local_row) &lt; size and global_col &lt; size:
            b_shared[local_row, local_col] = b[
                tile * TPB + local_row, global_col
            ]

        barrier()

        # Matrix multiplication within the tile
        if global_row &lt; size and global_col &lt; size:

            @parameter
            for k in range(min(TPB, size - tile * TPB)):
                acc += a_shared[local_row, k] * b_shared[k, local_col]

        barrier()

    # Write out final result
    if global_row &lt; size and global_col &lt; size:
        out[global_row, global_col] = acc


</code></pre>
<div class="solution-explanation">
<p>The tiled implementation with LayoutTensor handles large matrices efficiently by processing them in blocks. Here’s a comprehensive analysis:</p>
<h3 id="implementation-architecture"><a class="header" href="#implementation-architecture">Implementation Architecture</a></h3>
<ol>
<li>
<p><strong>Layout Configuration</strong>:</p>
<pre><code class="language-mojo">alias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)
</code></pre>
<ul>
<li>Defines row-major layout for all tensors</li>
<li>Ensures consistent memory access patterns</li>
<li>Enables efficient 2D indexing</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Setup</strong>:</p>
<pre><code class="language-mojo">a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
</code></pre>
<ul>
<li>Uses TensorBuilder for structured allocation</li>
<li>Maintains row-major layout for consistency</li>
<li>Enables efficient tile processing</li>
</ul>
</li>
<li>
<p><strong>Thread and Block Organization</strong>:</p>
<pre><code class="language-mojo">local_row = thread_idx.x
local_col = thread_idx.y
global_row = block_idx.x * TPB + local_row
global_col = block_idx.y * TPB + local_col
</code></pre>
<ul>
<li>Maps threads to matrix elements</li>
<li>Handles 2D indexing efficiently</li>
<li>Maintains proper boundary checks</li>
</ul>
</li>
</ol>
<h3 id="tile-processing-pipeline"><a class="header" href="#tile-processing-pipeline">Tile Processing Pipeline</a></h3>
<ol>
<li>
<p><strong>Tile Iteration</strong>:</p>
<pre><code class="language-mojo">@parameter
for tile in range((size + TPB - 1) // TPB):
</code></pre>
<ul>
<li>Compile-time unrolled loop</li>
<li>Handles matrix size not divisible by TPB</li>
<li>Processes matrix in TPB×TPB tiles</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Reset</strong>:</p>
<pre><code class="language-mojo">if local_row &lt; TPB and local_col &lt; TPB:
    a_shared[local_row, local_col] = 0
    b_shared[local_row, local_col] = 0
</code></pre>
<ul>
<li>Clears previous tile data</li>
<li>Ensures clean state for new tile</li>
<li>Prevents data corruption</li>
</ul>
</li>
<li>
<p><strong>Tile Loading</strong>:</p>
<pre><code class="language-mojo"># Load A tile
if global_row &lt; size and (tile * TPB + local_col) &lt; size:
    a_shared[local_row, local_col] = a[global_row, tile * TPB + local_col]

# Load B tile
if (tile * TPB + local_row) &lt; size and global_col &lt; size:
    b_shared[local_row, local_col] = b[tile * TPB + local_row, global_col]
</code></pre>
<ul>
<li>Handles boundary conditions</li>
<li>Uses LayoutTensor indexing</li>
<li>Maintains memory coalescing</li>
</ul>
</li>
<li>
<p><strong>Computation</strong>:</p>
<pre><code class="language-mojo">@parameter
for k in range(min(TPB, size - tile * TPB)):
    acc += a_shared[local_row, k] * b_shared[k, local_col]
</code></pre>
<ul>
<li>Processes current tile</li>
<li>Uses shared memory for efficiency</li>
<li>Handles partial tiles correctly</li>
</ul>
</li>
</ol>
<h3 id="memory-access-optimization"><a class="header" href="#memory-access-optimization">Memory Access Optimization</a></h3>
<ol>
<li>
<p><strong>Global Memory Pattern</strong>:</p>
<pre><code class="language-txt">A[global_row, tile * TPB + local_col] → Coalesced reads
B[tile * TPB + local_row, global_col] → Transposed access
</code></pre>
<ul>
<li>Maximizes memory coalescing</li>
<li>Minimizes bank conflicts</li>
<li>Efficient for transposed access</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Usage</strong>:</p>
<pre><code class="language-txt">a_shared[local_row, k] → Row-wise access
b_shared[k, local_col] → Column-wise access
</code></pre>
<ul>
<li>Optimized for matrix multiplication</li>
<li>Reduces bank conflicts</li>
<li>Enables data reuse</li>
</ul>
</li>
</ol>
<h3 id="synchronization-and-safety"><a class="header" href="#synchronization-and-safety">Synchronization and Safety</a></h3>
<ol>
<li>
<p><strong>Barrier Points</strong>:</p>
<pre><code class="language-mojo">barrier()  # After shared memory reset
barrier()  # After tile loading
barrier()  # After computation
</code></pre>
<ul>
<li>Ensures shared memory consistency</li>
<li>Prevents race conditions</li>
<li>Maintains thread cooperation</li>
</ul>
</li>
<li>
<p><strong>Boundary Handling</strong>:</p>
<pre><code class="language-mojo">if global_row &lt; size and global_col &lt; size:
    out[global_row, global_col] = acc
</code></pre>
<ul>
<li>Prevents out-of-bounds access</li>
<li>Handles matrix edges</li>
<li>Safe result writing</li>
</ul>
</li>
</ol>
<h3 id="performance-characteristics-1"><a class="header" href="#performance-characteristics-1">Performance Characteristics</a></h3>
<ol>
<li>
<p><strong>Memory Efficiency</strong>:</p>
<ul>
<li>Reduced global memory traffic through tiling</li>
<li>Efficient shared memory reuse</li>
<li>Coalesced memory access patterns</li>
</ul>
</li>
<li>
<p><strong>Computational Throughput</strong>:</p>
<ul>
<li>High data locality in shared memory</li>
<li>Efficient thread utilization</li>
<li>Minimal thread divergence</li>
</ul>
</li>
<li>
<p><strong>Scalability</strong>:</p>
<ul>
<li>Handles arbitrary matrix sizes</li>
<li>Efficient for large matrices</li>
<li>Good thread occupancy</li>
</ul>
</li>
</ol>
<h3 id="key-optimizations"><a class="header" href="#key-optimizations">Key Optimizations</a></h3>
<ol>
<li>
<p><strong>Layout Optimization</strong>:</p>
<ul>
<li>Row-major layout for all tensors</li>
<li>Efficient 2D indexing</li>
<li>Proper alignment</li>
</ul>
</li>
<li>
<p><strong>Memory Access</strong>:</p>
<ul>
<li>Coalesced global memory loads</li>
<li>Efficient shared memory usage</li>
<li>Minimal bank conflicts</li>
</ul>
</li>
<li>
<p><strong>Computation</strong>:</p>
<ul>
<li>Register-based accumulation</li>
<li>Compile-time loop unrolling</li>
<li>Efficient tile processing</li>
</ul>
</li>
</ol>
<p>This implementation achieves high performance through:</p>
<ul>
<li>Efficient use of LayoutTensor for memory access</li>
<li>Optimal tiling strategy</li>
<li>Proper thread synchronization</li>
<li>Careful boundary handling</li>
</ul>
</div>
</details>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/mojolang.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/solution.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
