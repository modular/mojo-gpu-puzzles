<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ðŸ”° Simple Embedding Kernel - Mojo ðŸ”¥ GPU Puzzles</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta property="og:image" content="..//puzzles_images/puzzle-mark.svg">
        <meta property="og:url" content="https://builds.modular.com/puzzles">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="MojoðŸ”¥ GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in MojoðŸ”¥ Through Interactive Puzzles">
        <meta name="twitter:image" content="..//puzzles_images/puzzle-mark.svg">
        <link rel="icon" type="image/png" href="..//puzzles_images/puzzle-mark.svg">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/css/custom.css">
        <link rel="stylesheet" href="../theme/css/highlight.css">
        <link rel="stylesheet" href="../theme/css/tabs.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="../print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="simple-embedding-kernel"><a class="header" href="#simple-embedding-kernel">Simple embedding kernel</a></h1>
<p>In this puzzle, youâ€™ll implement two different GPU kernels for embedding operations that produce identical results but use different memory access patterns, demonstrating the critical importance of memory coalescing in GPU performance.</p>
<h2 id="1d-coalesced-kernel-optimized-approach"><a class="header" href="#1d-coalesced-kernel-optimized-approach">1D coalesced kernel (optimized approach)</a></h2>
<p>This kernel uses a simple 1D grid where each thread processes exactly one output element. The key insight is that consecutive threads will access consecutive memory locations, leading to optimal memory coalescing.</p>
<p><strong>Thread organization:</strong></p>
<ul>
<li><strong>Grid configuration</strong>: <code>[total_elements // 256]</code> blocks, <code>256</code> threads per block</li>
<li><strong>Thread mapping</strong>: Each thread handles one <code>(batch, seq, embed)</code> position</li>
<li><strong>Memory pattern</strong>: Consecutive threads access consecutive embedding dimensions</li>
</ul>
<p><strong>What you need to implement:</strong></p>
<ol>
<li>Calculate the global thread index from block and thread indices</li>
<li>Convert the flat index to 3D coordinates <code>(batch_idx, seq_idx, embed_idx)</code></li>
<li>Look up the token index from the indices tensor</li>
<li>Copy the appropriate embedding vector element to the output</li>
</ol>
<h3 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h3>
<p>You need to complete the missing parts in both embedding kernels:</p>
<pre><code class="language-mojo">alias THREADS_PER_BLOCK = 256


fn embedding_kernel_coalesced[
    indices_layout: Layout,
    weights_layout: Layout,
    out_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    vocab_size: Int,
    embed_dim: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    indices: LayoutTensor[mut=True, DType.int32, indices_layout],
    weights: LayoutTensor[mut=True, dtype, weights_layout],
):
    """
    Memory-coalescing focused embedding kernel.

    Key insight: The bottleneck is memory access patterns, not computation.
    - Each thread handles one (batch, seq, embed) position
    - Simple 1D grid for maximum simplicity and correctness
    - Focus on getting memory access right first
    """

    # Simple 1D indexing - each thread = one output element
    global_idx = block_idx.x * block_dim.x + thread_idx.x
    total_elements = batch_size * seq_len * embed_dim

    if global_idx &gt;= total_elements:
        return

    # Convert to (batch, seq, embed) coordinates
    # FILL IN roughly 4 lines

    # Get token index
    # FILL IN 1 line

    # Simple, correct assignment
    # FILL IN 4 lines


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p19/op/embedding.mojo" class="filename">View full file: problems/p19/op/embedding.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ul>
<li>Start with <code>global_idx = block_idx.x * block_dim.x + thread_idx.x</code></li>
<li>Convert to 3D coordinates using division and modulo: <code>batch_idx = global_idx // (seq_len * embed_dim)</code></li>
<li>Use <code>remaining = global_idx % (seq_len * embed_dim)</code> to simplify further calculations</li>
<li>Always check bounds: <code>if global_idx &gt;= total_elements: return</code></li>
<li>Handle invalid token indices by setting output to 0</li>
<li>The embedding lookup is: <code>output[batch_idx, seq_idx, embed_idx] = weights[token_idx, embed_idx]</code></li>
</ul>
</div>
</details>
<h2 id="2d-non-coalesced-kernel-comparison-approach"><a class="header" href="#2d-non-coalesced-kernel-comparison-approach">2D non-coalesced kernel (comparison approach)</a></h2>
<p>This kernel uses a 2D grid where the X dimension spans <code>(batch Ã— seq)</code> positions and the Y dimension spans embedding dimensions. This can lead to non-coalesced memory access patterns.</p>
<p><strong>Thread organization:</strong></p>
<ul>
<li><strong>Grid configuration</strong>: <code>[batch x seq // 16, embed_dim // 16]</code> blocks, <code>16 x 16</code> threads per block</li>
<li><strong>Thread mapping</strong>: <code>thread_idx.x</code> maps to batch/sequence, <code>thread_idx.y</code> maps to embedding dimension</li>
<li><strong>Memory pattern</strong>: Threads in a warp may access scattered memory locations</li>
</ul>
<p><strong>What you need to implement:</strong></p>
<ol>
<li>Calculate both X and Y coordinates from the 2D grid</li>
<li>Convert the X coordinate to separate batch and sequence indices</li>
<li>Use the Y coordinate directly as the embedding dimension</li>
<li>Perform the same embedding lookup with bounds checking</li>
</ol>
<h3 id="code-to-complete-1"><a class="header" href="#code-to-complete-1">Code to complete</a></h3>
<p>You need to complete the missing parts in both embedding kernels:</p>
<pre><code class="language-mojo">fn embedding_kernel_2d[
    indices_layout: Layout,
    weights_layout: Layout,
    out_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    vocab_size: Int,
    embed_dim: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    indices: LayoutTensor[mut=True, DType.int32, indices_layout],
    weights: LayoutTensor[mut=True, dtype, weights_layout],
):
    """
    2D grid non-coalesced embedding kernel.

    Non-optimal approach for comparison:
    - 2D grid: (batch*seq, embed_dim)
    - More complex indexing
    - Potentially worse memory access patterns
    """

    # 2D grid indexing
    batch_seq_idx = block_idx.x * block_dim.x + thread_idx.x
    embed_idx = block_idx.y * block_dim.y + thread_idx.y
    total_positions = batch_size * seq_len

    if batch_seq_idx &gt;= total_positions or embed_idx &gt;= embed_dim:
        return

    # Convert to (batch, seq) coordinates
    # FILL IN 2 lines

    # Get token index
    # FILL IN 1 line

    # Assignment with 2D grid pattern
    # FILL IN 4 lines


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p19/op/embedding.mojo" class="filename">View full file: problems/p19/op/embedding.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ul>
<li>Use both X and Y thread coordinates: <code>batch_seq_idx = block_idx.x * block_dim.x + thread_idx.x</code></li>
<li>And: <code>embed_idx = block_idx.y * block_dim.y + thread_idx.y</code></li>
<li>Convert <code>batch_seq_idx</code> to separate batch and sequence indices: <code>batch_idx = batch_seq_idx // seq_len</code></li>
<li>Remember to check bounds for both dimensions: <code>if batch_seq_idx &gt;= total_positions or embed_idx &gt;= embed_dim</code></li>
<li>The token lookup is the same as 1D, but youâ€™re only handling one embedding dimension per thread</li>
<li>This kernel processes one embedding dimension per thread instead of entire vectors</li>
</ul>
</div>
</details>
<h2 id="custom-ops-registration"><a class="header" href="#custom-ops-registration">Custom ops registration</a></h2>
<p>The kernels are wrapped in PyTorch custom operations for easy integration. The registration pattern is the same as MAX custom ops explained in <a href="../puzzle_15/puzzle_15.html#understanding-max-graph-custom-ops">Understanding MAX Graph custom ops</a>:</p>
<h3 id="1d-coalesced-operation"><a class="header" href="#1d-coalesced-operation">1D coalesced operation</a></h3>
<p>This operation registers the optimized 1D embedding kernel as <code>"embedding"</code>:</p>
<pre><code class="language-mojo">import compiler
from runtime.asyncrt import DeviceContextPtr
from tensor import InputTensor, OutputTensor
from memory import UnsafePointer
from gpu.host import DeviceBuffer


@compiler.register("embedding")
struct EmbeddingCustomOp:
    @staticmethod
    fn execute[
        target: StaticString,
        batch_size: Int,
        seq_len: Int,
        vocab_size: Int,
        embed_dim: Int,
    ](
        output: OutputTensor[
            dtype = DType.float32, rank=3
        ],  # [batch_size, seq_len, embed_dim]
        indices: InputTensor[
            dtype = DType.int32, rank=2
        ],  # [batch_size, seq_len]
        weights: InputTensor[
            dtype = output.dtype, rank=2
        ],  # [vocab_size, embed_dim]
        ctx: DeviceContextPtr,
    ) raises:
        output_tensor = output.to_layout_tensor()
        indices_tensor = indices.to_layout_tensor()
        weights_tensor = weights.to_layout_tensor()

        alias indices_layout = indices_tensor.layout
        alias weights_layout = weights_tensor.layout
        alias out_layout = output_tensor.layout

        @parameter
        if target == "gpu":
            gpu_ctx = ctx.get_device_context()

            # Zero out output tensor
            gpu_ctx.enqueue_memset(
                DeviceBuffer[output.dtype](
                    gpu_ctx,
                    rebind[UnsafePointer[Scalar[output.dtype]]](
                        output_tensor.ptr
                    ),
                    batch_size * seq_len * embed_dim,
                    owning=False,
                ),
                0,
            )

            # Calculate 1D grid dimensions (matching kernel's flat indexing)
            total_elements = batch_size * seq_len * embed_dim
            blocks = max(1, ceildiv(total_elements, THREADS_PER_BLOCK))

            # Compile and launch optimized kernel
            compiled_kernel = gpu_ctx.compile_function[
                embedding_kernel_coalesced[
                    indices_layout,
                    weights_layout,
                    out_layout,
                    batch_size,
                    seq_len,
                    vocab_size,
                    embed_dim,
                    output.dtype,
                ]
            ]()

            gpu_ctx.enqueue_function(
                compiled_kernel,
                output_tensor,
                indices_tensor,
                weights_tensor,
                grid_dim=(blocks,),
                block_dim=(THREADS_PER_BLOCK,),
            )

        elif target == "cpu":
            for batch in range(batch_size):
                for seq in range(seq_len):
                    token_idx_val = Int(indices_tensor[batch, seq])
                    if token_idx_val &gt;= 0 and token_idx_val &lt; vocab_size:
                        for emb in range(embed_dim):
                            output_tensor[batch, seq, emb] = weights_tensor[
                                token_idx_val, emb
                            ]
        else:
            raise Error("Unsupported target: " + target)


</code></pre>
<p><strong>Key aspects of this registration:</strong></p>
<ul>
<li><strong>Simple grid configuration</strong>: Uses a straightforward 1D grid with <code>ceildiv(total_elements, THREADS_PER_BLOCK)</code> blocks</li>
<li><strong>Memory optimization</strong>: Single <code>enqueue_memset</code> call to zero the output buffer efficiently</li>
<li><strong>Compile-time parameters</strong>: All tensor dimensions passed as compile-time parameters for optimal performance</li>
<li><strong>Device abstraction</strong>: Handles both GPU execution and CPU fallback seamlessly</li>
</ul>
<h3 id="2d-non-coalesced-operation"><a class="header" href="#2d-non-coalesced-operation">2D non-coalesced operation</a></h3>
<p>This operation registers the comparison 2D embedding kernel as <code>"embedding_2d"</code>:</p>
<pre><code class="language-mojo">@compiler.register("embedding_2d")
struct Embedding2DCustomOp:
    @staticmethod
    fn execute[
        target: StaticString,
        batch_size: Int,
        seq_len: Int,
        vocab_size: Int,
        embed_dim: Int,
    ](
        output: OutputTensor[
            dtype = DType.float32, rank=3
        ],  # [batch_size, seq_len, embed_dim]
        indices: InputTensor[
            dtype = DType.int32, rank=2
        ],  # [batch_size, seq_len]
        weights: InputTensor[
            dtype = output.dtype, rank=2
        ],  # [vocab_size, embed_dim]
        ctx: DeviceContextPtr,
    ) raises:
        output_tensor = output.to_layout_tensor()
        indices_tensor = indices.to_layout_tensor()
        weights_tensor = weights.to_layout_tensor()

        alias indices_layout = indices_tensor.layout
        alias weights_layout = weights_tensor.layout
        alias out_layout = output_tensor.layout

        @parameter
        if target == "gpu":
            gpu_ctx = ctx.get_device_context()

            # Zero out output tensor
            gpu_ctx.enqueue_memset(
                DeviceBuffer[output.dtype](
                    gpu_ctx,
                    rebind[UnsafePointer[Scalar[output.dtype]]](
                        output_tensor.ptr
                    ),
                    batch_size * seq_len * embed_dim,
                    owning=False,
                ),
                0,
            )

            # Calculate 2D grid dimensions for non-coalesced access
            total_positions = batch_size * seq_len
            alias BLOCK_X = 16  # batch*seq dimension
            alias BLOCK_Y = 16  # embed dimension
            blocks_x = max(1, ceildiv(total_positions, BLOCK_X))
            blocks_y = max(1, ceildiv(embed_dim, BLOCK_Y))

            # Compile and launch 2D kernel
            compiled_kernel = gpu_ctx.compile_function[
                embedding_kernel_2d[
                    indices_layout,
                    weights_layout,
                    out_layout,
                    batch_size,
                    seq_len,
                    vocab_size,
                    embed_dim,
                    output.dtype,
                ]
            ]()

            gpu_ctx.enqueue_function(
                compiled_kernel,
                output_tensor,
                indices_tensor,
                weights_tensor,
                grid_dim=(blocks_x, blocks_y),
                block_dim=(BLOCK_X, BLOCK_Y),
            )

        elif target == "cpu":
            # Same CPU fallback as 1D version
            for batch in range(batch_size):
                for seq in range(seq_len):
                    token_idx_val = Int(indices_tensor[batch, seq])
                    if token_idx_val &gt;= 0 and token_idx_val &lt; vocab_size:
                        for emb in range(embed_dim):
                            output_tensor[batch, seq, emb] = weights_tensor[
                                token_idx_val, emb
                            ]
        else:
            raise Error("Unsupported target: " + target)


</code></pre>
<p><strong>Key differences from the 1D operation:</strong></p>
<ul>
<li><strong>Complex grid configuration</strong>: Uses a 2D grid with separate calculations for <code>blocks_x</code> and <code>blocks_y</code></li>
<li><strong>Fixed block dimensions</strong>: Hard-coded <code>BLOCK_X = 16</code> and <code>BLOCK_Y = 16</code> for 2D thread organization</li>
<li><strong>Same memory management</strong>: Identical memory initialization and CPU fallback logic</li>
<li><strong>Different kernel call</strong>: Passes 2D grid dimensions <code>(blocks_x, blocks_y)</code> and block dimensions <code>(BLOCK_X, BLOCK_Y)</code></li>
</ul>
<h3 id="common-wrapper-functionality"><a class="header" href="#common-wrapper-functionality">Common wrapper functionality</a></h3>
<p>Both custom operations provide essential infrastructure:</p>
<ol>
<li>
<p><strong>Memory management</strong>:</p>
<ul>
<li>Zero-initialization of output tensors with <code>enqueue_memset</code></li>
<li>Proper buffer creation and memory layout handling</li>
<li>Automatic cleanup and resource management</li>
</ul>
</li>
<li>
<p><strong>Device abstraction</strong>:</p>
<ul>
<li>GPU execution with optimized kernels</li>
<li>CPU fallback for compatibility and debugging</li>
<li>Consistent interface regardless of execution target</li>
</ul>
</li>
<li>
<p><strong>Parameter passing</strong>:</p>
<ul>
<li>Compile-time tensor dimensions for kernel optimization</li>
<li>Runtime tensor data through layout tensor conversion</li>
<li>Type-safe parameter validation</li>
</ul>
</li>
<li>
<p><strong>Grid configuration</strong>:</p>
<ul>
<li>Automatic calculation of optimal grid dimensions</li>
<li>Different strategies optimized for each kernelâ€™s access pattern</li>
<li>Proper block dimension management</li>
</ul>
</li>
</ol>
<h3 id="integration-with-pytorch"><a class="header" href="#integration-with-pytorch">Integration with PyTorch</a></h3>
<p>These registered operations can be called from Python using the <a href="https://docs.modular.com/max/api/python/torch/CustomOpLibrary/">CustomOpLibrary</a>:</p>
<pre><code class="language-python"># Load the custom operations
ops = CustomOpLibrary(mojo_kernels)

# Call the 1D coalesced version
result_1d = ops.embedding[{"batch_size": B, "seq_len": L, "vocab_size": V, "embed_dim": E}](
    indices, weights
)

# Call the 2D non-coalesced version
result_2d = ops.embedding_2d[{"batch_size": B, "seq_len": L, "vocab_size": V, "embed_dim": E}](
    indices, weights
)
</code></pre>
<p>The power of this approach is that the same kernel implementations can be used across different Python frameworks while maintaining optimal performance characteristics.</p>
<h2 id="run-the-code"><a class="header" href="#run-the-code">Run the code</a></h2>
<p>You can run the puzzle with:</p>
<div class="code-tabs" data-tab-group="package-manager">
  <div class="tab-buttons">
    <button class="tab-button">uv</button>
    <button class="tab-button">pixi</button>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">uv run poe p19
</code></pre>
  </div>
  <div class="tab-content">
<pre><code class="language-bash">pixi run p19
</code></pre>
  </div>
</div>
<p>When successful, you should see output similar to:</p>
<pre><code>Puzzle 19: Mojo Embedding Kernel Comparison
======================================================================
Configuration: B=8, L=512, V=10000, E=512
------------------------------------------------------------

Testing Correctness...
   1D Coalesced - Max difference: 1.19e-07
   2D Non-coalesced - Max difference: 1.19e-07
   âœ… Both implementations CORRECT

Benchmarking Mojo Kernels...

Performance Results:
   1D Coalesced:     2.145 ms
   2D Non-coalesced: 3.867 ms
   1D is 1.80x faster than 2D

Key Learning Points:
â€¢ Compare different GPU kernel implementations
â€¢ 1D vs 2D grid patterns have different memory access
â€¢ Coalesced memory access should be faster
â€¢ Grid configuration affects GPU utilization
</code></pre>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<p>The solution involves implementing the coordinate transformations and memory operations for both kernels:</p>
<h2 id="1d-coalesced-kernel"><a class="header" href="#1d-coalesced-kernel">1D Coalesced Kernel</a></h2>
<pre><code class="language-mojo">fn embedding_kernel_coalesced[
    indices_layout: Layout,
    weights_layout: Layout,
    out_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    vocab_size: Int,
    embed_dim: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    indices: LayoutTensor[mut=True, DType.int32, indices_layout],
    weights: LayoutTensor[mut=True, dtype, weights_layout],
):
    """
    Memory-coalescing focused embedding kernel.

    Key insight: The bottleneck is memory access patterns, not computation.
    - Each thread handles one (batch, seq, embed) position
    - Simple 1D grid for maximum simplicity and correctness
    - Focus on getting memory access right first
    """

    # Simple 1D indexing - each thread = one output element
    global_idx = block_idx.x * block_dim.x + thread_idx.x
    total_elements = batch_size * seq_len * embed_dim

    if global_idx &gt;= total_elements:
        return

    # Convert to (batch, seq, embed) coordinates
    batch_idx = global_idx // (seq_len * embed_dim)
    remaining = global_idx % (seq_len * embed_dim)
    seq_idx = remaining // embed_dim
    embed_idx = remaining % embed_dim

    # Get token index
    token_idx_val = Int(indices[batch_idx, seq_idx])

    # Simple, correct assignment
    if token_idx_val &gt;= 0 and token_idx_val &lt; vocab_size:
        output[batch_idx, seq_idx, embed_idx] = weights[
            token_idx_val, embed_idx
        ]
    else:
        output[batch_idx, seq_idx, embed_idx] = 0


</code></pre>
<h2 id="2d-non-coalesced-kernel"><a class="header" href="#2d-non-coalesced-kernel">2D Non-Coalesced Kernel</a></h2>
<pre><code class="language-mojo">fn embedding_kernel_2d[
    indices_layout: Layout,
    weights_layout: Layout,
    out_layout: Layout,
    batch_size: Int,
    seq_len: Int,
    vocab_size: Int,
    embed_dim: Int,
    dtype: DType = DType.float32,
](
    output: LayoutTensor[mut=True, dtype, out_layout],
    indices: LayoutTensor[mut=True, DType.int32, indices_layout],
    weights: LayoutTensor[mut=True, dtype, weights_layout],
):
    """
    2D grid non-coalesced embedding kernel.

    Non-optimal approach for comparison:
    - 2D grid: (batch*seq, embed_dim)
    - More complex indexing
    - Potentially worse memory access patterns
    """

    # 2D grid indexing
    batch_seq_idx = block_idx.x * block_dim.x + thread_idx.x
    embed_idx = block_idx.y * block_dim.y + thread_idx.y

    total_positions = batch_size * seq_len

    # Bounds check
    if batch_seq_idx &gt;= total_positions or embed_idx &gt;= embed_dim:
        return

    # Convert to (batch, seq) coordinates
    batch_idx = batch_seq_idx // seq_len
    seq_idx = batch_seq_idx % seq_len

    # Get token index
    token_idx_val = Int(indices[batch_idx, seq_idx])

    # Assignment with 2D grid pattern
    if token_idx_val &gt;= 0 and token_idx_val &lt; vocab_size:
        output[batch_idx, seq_idx, embed_idx] = weights[
            token_idx_val, embed_idx
        ]
    else:
        output[batch_idx, seq_idx, embed_idx] = 0


</code></pre>
<div class="solution-explanation">
<p>Both solutions implement the same embedding lookup logic but with different thread organizations:</p>
<h3 id="key-differences"><a class="header" href="#key-differences">Key differences</a></h3>
<ol>
<li>
<p><strong>Thread mapping</strong>:</p>
<ul>
<li><strong>1D kernel</strong>: One thread per output element, simple flat indexing</li>
<li><strong>2D kernel</strong>: 2D grid mapping to (batchÃ—seq, embed_dim) coordinates</li>
</ul>
</li>
<li>
<p><strong>Memory access patterns</strong>:</p>
<ul>
<li><strong>1D kernel</strong>: Consecutive threads access consecutive embedding dimensions â†’ coalesced</li>
<li><strong>2D kernel</strong>: Thread access pattern depends on block configuration â†’ potentially non-coalesced</li>
</ul>
</li>
<li>
<p><strong>Indexing complexity</strong>:</p>
<ul>
<li><strong>1D kernel</strong>: Single division/modulo chain to get 3D coordinates</li>
<li><strong>2D kernel</strong>: Separate X/Y coordinate calculations</li>
</ul>
</li>
</ol>
<h3 id="performance-implications"><a class="header" href="#performance-implications">Performance implications</a></h3>
<p>The 1D kernel typically performs better because:</p>
<ul>
<li><strong>Memory coalescing</strong>: Consecutive threads access consecutive memory addresses</li>
<li><strong>Simple indexing</strong>: Lower computational overhead for coordinate calculations</li>
<li><strong>Better cache utilization</strong>: Predictable memory access patterns</li>
</ul>
<p>The 2D kernel may perform worse due to:</p>
<ul>
<li><strong>Scattered memory accesses</strong>: Threads within a warp may access different embedding vectors</li>
<li><strong>Complex grid configuration</strong>: 16Ã—16 blocks may not align optimally with memory layout</li>
<li><strong>Warp divergence</strong>: Different threads may follow different execution paths</li>
</ul>
</div>
</details>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Concept</th><th>1D Coalesced</th><th>2D Non-coalesced</th></tr></thead><tbody>
<tr><td><strong>Thread organization</strong></td><td>1D flat indexing</td><td>2D grid (batchÃ—seq, embed)</td></tr>
<tr><td><strong>Memory access</strong></td><td>Consecutive addresses</td><td>Potentially scattered</td></tr>
<tr><td><strong>Grid configuration</strong></td><td>Simple: <code>[total_elements // 256]</code></td><td>Complex: <code>[batchÃ—seq // 16, embed // 16]</code></td></tr>
<tr><td><strong>Performance</strong></td><td>Optimized for memory bandwidth</td><td>Suboptimal memory pattern</td></tr>
<tr><td><strong>Use case</strong></td><td>Production kernels</td><td>Educational comparison</td></tr>
</tbody></table>
</div>
<p>The core lesson: <strong>memory coalescing</strong> can lead to 2-3x performance differences for memory-bound operations like embeddings.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../puzzle_19/puzzle_19.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../puzzle_19/performance.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../puzzle_19/puzzle_19.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../puzzle_19/performance.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../theme/mojolang.js"></script>
        <script src="../theme/sidebar.js"></script>
        <script src="../theme/solution.js"></script>
        <script src="../theme/init-amplitude.js"></script>
        <script src="../theme/tabs.js"></script>


    </div>
    </body>
</html>
