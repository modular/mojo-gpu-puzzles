<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mojo üî• GPU Puzzles</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Mojoüî• GPU Puzzles">
        <meta property="og:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta property="og:image" content="/images/puzzle-mark.svg">
        <meta property="og:url" content="https://builds.modular.com/puzzles">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="Mojo GPU Puzzles Logo">
        <meta name="twitter:title" content="Mojoüî• GPU Puzzles">
        <meta name="twitter:description" content="Learn GPU Programming in Mojoüî• Through Interactive Puzzles">
        <meta name="twitter:image" content="/images/puzzle-mark.svg">
        <link rel="icon" type="image/png" href="/images/puzzle-mark.svg">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromPuzzles');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" ref="/">Puzzles</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="menu-links">
                    <li><a href="https://docs.modular.com/max/get-started/" target="_blank"><div>Product</div></a></li>
                    <li><a href="https://www.modular.com/max/solutions/agent" target="_blank">Solutions</a></li>
                    <li><a href="https://builds.modular.com/?category=featured" target="_blank">Resources</a></li>
                    <li><a href="https://www.modular.com/company/about" target="_blank">Company</a></li>
                    <li><a href="https://www.modular.com/pricing" target="_blank">Pricing</a></li>
                    <li><a href="https://docs.modular.com" target="_blank">Docs</a></li>
                    <li><a href="https://www.modular.com/blog" target="_blank">Blog</a></li>
                </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/mojo-gpu-puzzles" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                    <button class="secondary-btn log-in">Log in</button>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <p align="center">
  <img src="images/puzzle-mark.svg" alt="Mojo GPU Puzzles Logo" width="150" class="puzzle-image">
</p>
<p align="center">
  <h1 align="center">Mojoüî• GPU Puzzles</h1>
</p>
<p align="center" class="social-buttons" style="display: flex; justify-content: center; gap: 8px;">
  <a href="https://github.com/modular/mojo-gpu-puzzles">
    <img src="https://img.shields.io/badge/GitHub-Repository-181717?logo=github" alt="GitHub Repository">
  </a>
  <a href="https://docs.modular.com/mojo">
    <img src="https://img.shields.io/badge/Powered%20by-Mojo-FF5F1F" alt="Powered by Mojo">
  </a>
  <a href="https://docs.modular.com/max/get-started/#stay-in-touch">
    <img src="https://img.shields.io/badge/Subscribe-Updates-00B5AD?logo=mail.ru" alt="Subscribe for Updates">
  </a>
  <a href="https://forum.modular.com/c/">
    <img src="https://img.shields.io/badge/Modular-Forum-9B59B6?logo=discourse" alt="Modular Forum">
  </a>
  <a href="https://discord.com/channels/1087530497313357884/1098713601386233997">
    <img src="https://img.shields.io/badge/Discord-Join_Chat-5865F2?logo=discord" alt="Discord">
  </a>
</p>
<blockquote>
<p>üöß This book is a work in progress! Some sections may be incomplete or subject to change. üöß</p>
</blockquote>
<blockquote>
<p><em>‚ÄúFor the things we have to learn before we can do them, we learn by doing them.‚Äù</em>
Aristotle, (Nicomachean Ethics)</p>
</blockquote>
<p>Welcome to <strong>Mojo üî• GPU Puzzles</strong>, a hands-on guide to mastering GPU programming using <a href="https://docs.modular.com/mojo/manual/">Mojo</a> üî• ‚Äî the innovative programming language that combines Pythonic syntax with systems-level performance. GPU programming remains one of the most powerful skills in modern computing, driving advances in artificial intelligence, scientific simulation, and high-performance computing.</p>
<p>This book takes a unique approach to teaching GPU programming: learning by solving increasingly challenging puzzles. Rather than traditional textbook learning, you‚Äôll immediately start writing real GPU code and seeing the results.</p>
<p>The early chapters of this book are heavily inspired by <a href="https://github.com/srush/GPU-Puzzles">GPU Puzzles</a>, an interactive CUDA learning project by Sasha Rush. This adaptation reimplements these concepts using Mojo‚Äôs powerful abstractions and performance capabilities, while expanding on advanced topics with Mojo-specific optimizations.</p>
<h2 id="why-mojo--for-gpu-programming"><a class="header" href="#why-mojo--for-gpu-programming">Why Mojo üî• for GPU Programming?</a></h2>
<p>The computing industry has reached a critical inflection point. We can no longer rely on new CPU generations to automatically increase application performance through higher clock speeds. As power and heat constraints have plateaued CPU speeds, hardware manufacturers have shifted toward increasing the number of physical cores. This multi-core revolution has reached its zenith in modern GPUs, which contain thousands of cores operating in parallel. The NVIDIA H100, for example, can run an astonishing 16,896 threads simultaneously in a single clock cycle, with over 270,000 threads queued and ready for execution.</p>
<p>Mojo represents a fresh approach to GPU programming, making this massive parallelism more accessible and productive:</p>
<ul>
<li><strong>Python-like Syntax</strong> with systems programming capabilities that feels familiar to the largest programming community</li>
<li><strong>Zero-cost Abstractions</strong> that compile to efficient machine code without sacrificing performance</li>
<li><strong>Strong Type System</strong> that catches errors at compile time while maintaining expressiveness</li>
<li><strong>Built-in Tensor Support</strong> with hardware-aware optimizations specifically designed for GPU computation</li>
<li><strong>Direct Access</strong> to low-level CPU and GPU intrinsics for systems-level programming</li>
<li><strong>Cross-Hardware Portability</strong> allowing you to write code that can run on both CPUs and GPUs</li>
<li><strong>Ergonomic and Safety Improvements</strong> over traditional C/C++ GPU programming</li>
<li><strong>Lower Barrier to Entry</strong> enabling more programmers to harness GPU power effectively</li>
</ul>
<blockquote>
<p><strong>Mojo üî• aims to fuel innovation by democratizing GPU programming.</strong>
<strong>By expanding on Python‚Äôs familiar syntax while adding direct GPU access, Mojo empowers programmers with minimal specialized knowledge to build high-performance, heterogeneous (CPU, GPU-enabled) applications.</strong></p>
</blockquote>
<h2 id="the-gpu-programming-mindset"><a class="header" href="#the-gpu-programming-mindset">The GPU Programming Mindset</a></h2>
<p>Effective GPU programming requires a fundamental shift in how we think about computation. Here are some key mental models that will guide your journey:</p>
<h3 id="from-sequential-to-parallel-eliminating-loops-with-threads"><a class="header" href="#from-sequential-to-parallel-eliminating-loops-with-threads">From Sequential to Parallel: Eliminating Loops with Threads</a></h3>
<p>In traditional CPU programming, we process data sequentially through loops:</p>
<pre><code class="language-python"># CPU approach
for i in range(data_size):
    result[i] = process(data[i])
</code></pre>
<p>With GPUs, we flip this model entirely. Instead of moving sequentially through data, we map thousands of parallel threads directly onto the data:</p>
<pre><code class="language-mojo"># GPU approach (conceptual)
thread_id = get_global_id()
if thread_id &lt; data_size:
    result[thread_id] = process(data[thread_id])
</code></pre>
<p>Each thread becomes responsible for computing a single element, eliminating the need for explicit loops. This mental shift‚Äîfrom ‚Äústepping through data‚Äù to ‚Äúblanketing data with compute‚Äù‚Äîis central to GPU programming.</p>
<h3 id="fitting-a-mesh-of-compute-over-data"><a class="header" href="#fitting-a-mesh-of-compute-over-data">Fitting a Mesh of Compute Over Data</a></h3>
<p>Imagine your data as a grid, and GPU threads as another grid that overlays it. Your task is to design this ‚Äúcompute mesh‚Äù to efficiently cover your data:</p>
<ul>
<li><strong>Threads</strong>: Individual compute units that process single data elements</li>
<li><strong>Blocks</strong>: Organized groups of threads that share fast memory</li>
<li><strong>Grid</strong>: The entire collection of blocks that covers your dataset</li>
</ul>
<p>The art of GPU programming lies in crafting this mesh to maximize parallelism while respecting memory and synchronization constraints.</p>
<h3 id="data-movement-vs-computation"><a class="header" href="#data-movement-vs-computation">Data Movement vs. Computation</a></h3>
<p>In GPU programming, data movement is often more expensive than computation:</p>
<ul>
<li>Moving data between CPU and GPU is slow</li>
<li>Moving data between global and shared memory is faster</li>
<li>Operating on data already in registers or shared memory is extremely fast</li>
</ul>
<p>This inverts another common assumption in programming: computation is no longer the bottleneck‚Äîdata movement is.</p>
<p>Through the puzzles in this book, you‚Äôll develop an intuitive understanding of these principles, transforming how you approach computational problems.</p>
<h2 id="what-you-will-learn"><a class="header" href="#what-you-will-learn">What You Will Learn</a></h2>
<p>This book takes you on a journey from first principles to advanced GPU programming techniques. Rather than treating the GPU as a mysterious black box, we‚Äôll build your understanding layer by layer‚Äîstarting with how individual threads operate and culminating in sophisticated parallel algorithms. By mastering both low-level memory management and high-level tensor abstractions, you‚Äôll gain the versatility to tackle any GPU programming challenge.</p>
<p>Your learning path includes:</p>
<ul>
<li><strong>GPU Programming Fundamentals</strong>: Thread organization, memory hierarchies, and kernel execution models</li>
<li><strong>Dual Implementation Paths</strong>: Beginning with raw memory approaches using pointers, then transitioning to LayoutTensor abstractions</li>
<li><strong>Memory Management</strong>: Working with global, shared, and thread-local memory for optimal performance</li>
<li><strong>Low-level to High-level Progression</strong>: Understanding the foundation with UnsafePointer before leveraging LayoutTensor‚Äôs elegant abstractions</li>
<li><strong>Layout Tensors</strong>: Mastering Mojo‚Äôs powerful tensor abstractions for simplified, efficient GPU computation</li>
<li><strong>Parallel Algorithms</strong>: Implementing and optimizing parallel reductions, convolutions, matrix operations, and more</li>
<li><strong>Performance Optimization</strong>: Advanced techniques for memory coalescing, tiling, bank conflict avoidance, and minimizing thread divergence</li>
<li><strong>Real-world Applications</strong>: Applying these concepts to machine learning, signal processing, and computational tasks</li>
</ul>
<p>The book uniquely challenges the status quo approach by first building understanding with low-level memory manipulation, then gradually transitioning to Mojo‚Äôs powerful LayoutTensor abstractions. This gives you both deep understanding of GPU memory patterns and practical knowledge of modern tensor-based approaches.</p>
<h2 id="-prizes-and-rewards-"><a class="header" href="#-prizes-and-rewards-">üèÜ Prizes and Rewards üéâ</a></h2>
<p>Have you completed the available puzzles? We‚Äôre giving away free sticker packs to celebrate your achievement!</p>
<p>To claim your free stickers:</p>
<ol>
<li>Fork the GitHub repository <a href="https://github.com/modular/mojo-gpu-puzzles">https://github.com/modular/mojo-gpu-puzzles</a></li>
<li>Add your solutions to the available puzzles</li>
<li>Submit your solutions through <a href="https://forms.gle/bchQpB3GanHMNY3x9">this form</a> and we‚Äôll send you exclusive Modular stickers!</li>
</ol>
<p><em>Note: More puzzles are being added regularly - complete the currently available ones to claim your reward!</em></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="how-to-use-this-book"><a class="header" href="#how-to-use-this-book">How to Use This Book</a></h2>
<p>Each puzzle follows a consistent format designed to progressively build your skills:</p>
<ul>
<li><strong>Overview</strong>: Clear problem statement and key concepts introduced in each puzzle</li>
<li><strong>Configuration</strong>: Setup parameters and memory organization specific to each challenge</li>
<li><strong>Code to Complete</strong>: Skeleton code with specific sections for you to implement</li>
<li><strong>Tips</strong>: Optional hints if you get stuck, without giving away complete solutions</li>
<li><strong>Solution</strong>: Detailed explanations of the implementation, performance considerations, and underlying concepts</li>
</ul>
<p>The puzzles gradually increase in complexity, introducing new concepts while reinforcing fundamentals. We recommend solving them in order, as later puzzles build on skills developed in earlier ones.</p>
<h2 id="running-the-code"><a class="header" href="#running-the-code">Running the code</a></h2>
<p>All puzzles are designed to be run with the provided testing framework that verifies your implementation against expected results. Each puzzle includes instructions for running the code and validating your solution.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<h3 id="compatible-gpu"><a class="header" href="#compatible-gpu">Compatible GPU</a></h3>
<p>You‚Äôll need a <a href="https://docs.modular.com/max/faq#gpu-requirements">compatible GPU</a> to run the examples.</p>
<h3 id="setting-up-your-environment"><a class="header" href="#setting-up-your-environment">Setting up your environment</a></h3>
<p><a href="https://github.com/modular/mojo-gpu-puzzles">Clone the GitHub repository</a> and make sure you have the <code>magic</code> CLI installed to be able to run the Mojo programs:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/modular/mojo-gpu-puzzles
cd mojo-gpu-puzzles

# Install magic CLI (if not already installed)
curl -ssL https://magic.modular.com/ | bash

# Or update if already installed
magic self-update
</code></pre>
<h3 id="knowledge-prerequisites"><a class="header" href="#knowledge-prerequisites">Knowledge prerequisites</a></h3>
<p>Basic knowledge of:</p>
<ul>
<li>Programming fundamentals (variables, loops, conditionals, functions)</li>
<li>Parallel computing concepts (threads, synchronization, race conditions)</li>
<li>Basic familiarity with <a href="https://docs.modular.com/mojo/manual/">Mojo</a> (language basics parts and <a href="https://docs.modular.com/mojo/manual/pointers/">intro to pointers</a> section)</li>
<li><a href="https://docs.modular.com/mojo/manual/gpu/basics">A tour of GPU basics in Mojo</a> is helpful</li>
</ul>
<p>No prior GPU programming experience is necessary! We‚Äôll build that knowledge through the puzzles.</p>
<p>Let‚Äôs begin our journey into the exciting world of GPU computing with Mojo üî•!</p>
<h2 id="join-the-community"><a class="header" href="#join-the-community">Join the community</a></h2>
<p align="center" style="display: flex; justify-content: center; gap: 10px;">
  <a href="https://www.modular.com/company/talk-to-us">
    <img src="https://img.shields.io/badge/Subscribe-Updates-00B5AD?logo=mail.ru" alt="Subscribe for Updates">
  </a>
  <a href="https://forum.modular.com/c/">
    <img src="https://img.shields.io/badge/Modular-Forum-9B59B6?logo=discourse" alt="Modular Forum">
  </a>
  <a href="https://discord.com/channels/1087530497313357884/1098713601386233997">
    <img src="https://img.shields.io/badge/Discord-Join_Chat-5865F2?logo=discord" alt="Discord">
  </a>
</p>
<p>Join our vibrant community to discuss GPU programming, share solutions, and get help!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-1-map"><a class="header" href="#puzzle-1-map">Puzzle 1: Map</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>GPU programming is all about parallelism. In this puzzle, each thread will process a single element of the input array independently.
Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position.</em></p>
<p><img src="puzzle_01/./media/videos/720p30/puzzle_01_viz.gif" alt="Map" /></p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key concepts</a></h2>
<ul>
<li>Basic GPU kernel structure</li>
<li>One-to-one thread to data mapping</li>
<li>Memory access patterns</li>
<li>Array operations on GPU</li>
</ul>
<p>For each position \(i\):
\[\Large out[i] = a[i] + 10\]</p>
<h2 id="what-we-cover"><a class="header" href="#what-we-cover">What we cover</a></h2>
<h3 id="-raw-memory-approach"><a class="header" href="#-raw-memory-approach"><a href="puzzle_01/./raw.html">üî∞ Raw Memory Approach</a></a></h3>
<p>Start with direct memory manipulation to understand GPU fundamentals.</p>
<h3 id="-preview-modern-approach-with-layouttensor"><a class="header" href="#-preview-modern-approach-with-layouttensor"><a href="puzzle_01/./layout_tensor_preview.html">üí° Preview: Modern Approach with LayoutTensor</a></a></h3>
<p>See how LayoutTensor simplifies GPU programming with safer, cleaner code.</p>
<p>üí° <strong>Tip</strong>: Understanding both approaches helps you better appreciate modern GPU programming patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>
<p>Basic GPU kernel structure</p>
</li>
<li>
<p>Thread indexing with <code>thread_idx.x</code></p>
</li>
<li>
<p>Simple parallel operations</p>
</li>
<li>
<p><strong>Parallelism</strong>: Each thread executes independently</p>
</li>
<li>
<p><strong>Thread indexing</strong>: Access element at position <code>i = thread_idx.x</code></p>
</li>
<li>
<p><strong>Memory access</strong>: Read from <code>a[i]</code> and write to <code>out[i]</code></p>
</li>
<li>
<p><strong>Data independence</strong>: Each output depends only on its corresponding input</p>
</li>
</ul>
<h2 id="code-to-complete"><a class="header" href="#code-to-complete">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = SIZE
alias dtype = DType.float32


fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):
    i = thread_idx.x
    # FILL ME IN (roughly 1 line)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p01/p01.mojo" class="filename">View full file: problems/p01/p01.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>i</code></li>
<li>Add 10 to <code>a[i]</code></li>
<li>Store result in <code>out[i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-1"><a class="header" href="#running-the-code-1">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p01
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):
    i = thread_idx.x
    out[i] = a[i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>i = thread_idx.x</code></li>
<li>Adds 10 to input value: <code>out[i] = a[i] + 10.0</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="why-consider-layouttensor"><a class="header" href="#why-consider-layouttensor">Why Consider LayoutTensor?</a></h2>
<p>Looking at our traditional implementation above, you might notice some potential issues:</p>
<h3 id="current-approach"><a class="header" href="#current-approach">Current approach</a></h3>
<pre><code class="language-mojo">i = thread_idx.x
out[i] = a[i] + 10.0
</code></pre>
<p>This works for 1D arrays, but what happens when we need to:</p>
<ul>
<li>Handle 2D or 3D data?</li>
<li>Deal with different memory layouts?</li>
<li>Ensure coalesced memory access?</li>
</ul>
<h3 id="preview-of-future-challenges"><a class="header" href="#preview-of-future-challenges">Preview of future challenges</a></h3>
<p>As we progress through the puzzles, array indexing will become more complex:</p>
<pre><code class="language-mojo"># 2D indexing coming in later puzzles
idx = row * WIDTH + col

# 3D indexing
idx = (batch * HEIGHT + row) * WIDTH + col

# With padding
idx = (batch * padded_height + row) * padded_width + col
</code></pre>
<h3 id="layouttensor-preview"><a class="header" href="#layouttensor-preview">LayoutTensor preview</a></h3>
<p><a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a> will help us handle these cases more elegantly:</p>
<pre><code class="language-mojo"># Future preview - don't worry about this syntax yet!
out[i, j] = a[i, j] + 10.0  # 2D indexing
out[b, i, j] = a[b, i, j] + 10.0  # 3D indexing
</code></pre>
<p>We‚Äôll learn about LayoutTensor in detail in Puzzle 4, where these concepts become essential. For now, focus on understanding:</p>
<ul>
<li>Basic thread indexing</li>
<li>Simple memory access patterns</li>
<li>One-to-one mapping of threads to data</li>
</ul>
<p>üí° <strong>Key Takeaway</strong>: While direct indexing works for simple cases, we‚Äôll soon need more sophisticated tools for complex GPU programming patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-2-zip"><a class="header" href="#puzzle-2-zip">Puzzle 2: Zip</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Implement a kernel that adds together each position of vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position.</em></p>
<p><img src="puzzle_02/./media/videos/720p30/puzzle_02_viz.gif" alt="Zip" /></p>
<h2 id="key-concepts-2"><a class="header" href="#key-concepts-2">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Processing multiple input arrays in parallel</li>
<li>Element-wise operations with multiple inputs</li>
<li>Thread-to-data mapping across arrays</li>
<li>Memory access patterns with multiple arrays</li>
</ul>
<p>For each thread \(i\): \[\Large out[i] = a[i] + b[i]\]</p>
<h3 id="memory-access-pattern"><a class="header" href="#memory-access-pattern">Memory access pattern</a></h3>
<pre><code class="language-txt">Thread 0:  a[0] + b[0] ‚Üí out[0]
Thread 1:  a[1] + b[1] ‚Üí out[1]
Thread 2:  a[2] + b[2] ‚Üí out[2]
...
</code></pre>
<p>üí° <strong>Note</strong>: Notice how we‚Äôre now managing three arrays (<code>a</code>, <code>b</code>, <code>out</code>) in our kernel. As we progress to more complex operations, managing multiple array accesses will become increasingly challenging.</p>
<h2 id="code-to-complete-1"><a class="header" href="#code-to-complete-1">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = SIZE
alias dtype = DType.float32


fn add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
):
    i = thread_idx.x
    # FILL ME IN (roughly 1 line)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p02/p02.mojo" class="filename">View full file: problems/p02/p02.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>i</code></li>
<li>Add <code>a[i]</code> and <code>b[i]</code></li>
<li>Store result in <code>out[i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-2"><a class="header" href="#running-the-code-2">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p02
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 2.0, 4.0, 6.0])
</code></pre>
<h2 id="solution-1"><a class="header" href="#solution-1">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
):
    i = thread_idx.x
    out[i] = a[i] + b[i]


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>i = thread_idx.x</code></li>
<li>Adds values from both arrays: <code>out[i] = a[i] + b[i]</code></li>
</ul>
</div>
</details>
<h3 id="looking-ahead"><a class="header" href="#looking-ahead">Looking ahead</a></h3>
<p>While this direct indexing works for simple element-wise operations, consider:</p>
<ul>
<li>What if arrays have different layouts?</li>
<li>What if we need to broadcast one array to another?</li>
<li>How to ensure coalesced access across multiple arrays?</li>
</ul>
<p>These questions will be addressed when we <a href="puzzle_02/../puzzle_04/">introduce LayoutTensor in Puzzle 4</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-3-guards"><a class="header" href="#puzzle-3-guards">Puzzle 3: Guards</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note</strong>: <em>You have more threads than positions. This means you need to protect against out-of-bounds memory access.</em></p>
<p><img src="puzzle_03/./media/videos/720p30/puzzle_03_viz.gif" alt="Guard" /></p>
<h2 id="key-concepts-3"><a class="header" href="#key-concepts-3">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Handling thread/data size mismatches</li>
<li>Preventing out-of-bounds memory access</li>
<li>Using conditional execution in GPU kernels</li>
<li>Safe memory access patterns</li>
</ul>
<h3 id="mathematical-description"><a class="header" href="#mathematical-description">Mathematical Description</a></h3>
<p>For each thread \(i\):
\[\Large \text{if}\ i &lt; \text{size}: out[i] = a[i] + 10\]</p>
<h3 id="memory-safety-pattern"><a class="header" href="#memory-safety-pattern">Memory Safety Pattern</a></h3>
<pre><code class="language-txt">Thread 0 (i=0):  if 0 &lt; size:  out[0] = a[0] + 10  ‚úì Valid
Thread 1 (i=1):  if 1 &lt; size:  out[1] = a[1] + 10  ‚úì Valid
Thread 2 (i=2):  if 2 &lt; size:  out[2] = a[2] + 10  ‚úì Valid
Thread 3 (i=3):  if 3 &lt; size:  out[3] = a[3] + 10  ‚úì Valid
Thread 4 (i=4):  if 4 &lt; size:  ‚ùå Skip (out of bounds)
Thread 5 (i=5):  if 5 &lt; size:  ‚ùå Skip (out of bounds)
</code></pre>
<p>üí° <strong>Note</strong>: Boundary checking becomes increasingly complex with:</p>
<ul>
<li>Multi-dimensional arrays</li>
<li>Different array shapes</li>
<li>Complex access patterns</li>
</ul>
<h2 id="code-to-complete-2"><a class="header" href="#code-to-complete-2">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 4
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (8, 1)
alias dtype = DType.float32


fn add_10_guard(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    i = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p03/p03.mojo" class="filename">View full file: problems/p03/p03.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>thread_idx.x</code> in <code>i</code></li>
<li>Add guard: <code>if i &lt; size</code></li>
<li>Inside guard: <code>out[i] = a[i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-3"><a class="header" href="#running-the-code-3">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p03
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-2"><a class="header" href="#solution-2">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_guard(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    i = thread_idx.x
    if i &lt; size:
        out[i] = a[i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets thread index with <code>i = thread_idx.x</code></li>
<li>Guards against out-of-bounds access with <code>if i &lt; size</code></li>
<li>Inside guard: adds 10 to input value</li>
</ul>
</div>
</details>
<h3 id="looking-ahead-1"><a class="header" href="#looking-ahead-1">Looking ahead</a></h3>
<p>While simple boundary checks work here, consider these challenges:</p>
<ul>
<li>What about 2D/3D array boundaries?</li>
<li>How to handle different shapes efficiently?</li>
<li>What if we need padding or edge handling?</li>
</ul>
<p>Example of growing complexity:</p>
<pre><code class="language-mojo"># Current: 1D bounds check
if i &lt; size: ...

# Coming soon: 2D bounds check
if i &lt; height and j &lt; width: ...

# Later: 3D with padding
if i &lt; height and j &lt; width and k &lt; depth and
   i &gt;= padding and j &gt;= padding: ...
</code></pre>
<p>These boundary handling patterns will become more elegant when we <a href="puzzle_03/../puzzle_04/">learn about LayoutTensor in Puzzle 4</a>, which provides built-in boundary checking and shape management.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-4-2d-map"><a class="header" href="#puzzle-4-2d-map">Puzzle 4: 2D Map</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D square matrix <code>a</code> and stores it in 2D square matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions</em>.</p>
<p><img src="puzzle_04/./media/videos/720p30/puzzle_04_viz.gif" alt="2D Matrix Mapping" /></p>
<h2 id="key-concepts-4"><a class="header" href="#key-concepts-4">Key concepts</a></h2>
<ul>
<li>2D thread indexing</li>
<li>Matrix operations on GPU</li>
<li>Handling excess threads</li>
<li>Memory layout patterns</li>
</ul>
<p>For each position \((i,j)\):
\[\Large out[i,j] = a[i,j] + 10\]</p>
<blockquote>
<h2 id="thread-indexing-convention"><a class="header" href="#thread-indexing-convention">Thread indexing convention</a></h2>
<p>When working with 2D matrices in GPU programming, we follow a natural mapping between thread indices and matrix coordinates:</p>
<ul>
<li><code>thread_idx.y</code> corresponds to the row index</li>
<li><code>thread_idx.x</code> corresponds to the column index</li>
</ul>
<p><img src="puzzle_04/./media/videos/720p30/thread_indexing_viz.gif" alt="2D thread indexing" /></p>
<p>This convention aligns with:</p>
<ol>
<li>The standard mathematical notation where matrix positions are specified as (row, column)</li>
<li>The visual representation of matrices where rows go top-to-bottom (y-axis) and columns go left-to-right (x-axis)</li>
<li>Common GPU programming patterns where thread blocks are organized in a 2D grid matching the matrix structure</li>
</ol>
<h3 id="historical-origins"><a class="header" href="#historical-origins">Historical origins</a></h3>
<p>While graphics and image processing typically use \((x,y)\) coordinates, matrix operations in computing have historically used (row, column) indexing. This comes from how early computers stored and processed 2D data: line by line, top to bottom, with each line read left to right. This row-major memory layout proved efficient for both CPUs and GPUs, as it matches how they access memory sequentially. When GPU programming adopted thread blocks for parallel processing, it was natural to map <code>thread_idx.y</code> to rows and <code>thread_idx.x</code> to columns, maintaining consistency with established matrix indexing conventions.</p>
</blockquote>
<h2 id="implementation-approaches"><a class="header" href="#implementation-approaches">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-1"><a class="header" href="#-raw-memory-approach-1"><a href="puzzle_04/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how 2D indexing works with manual memory management.</p>
<h3 id="-learn-about-layouttensor"><a class="header" href="#-learn-about-layouttensor"><a href="puzzle_04/./introduction_layout_tensor.html">üìö Learn about LayoutTensor</a></a></h3>
<p>Discover a powerful abstraction that simplifies multi-dimensional array operations and memory management on GPU.</p>
<h3 id="-modern-2d-operations"><a class="header" href="#-modern-2d-operations"><a href="puzzle_04/./layout_tensor.html">üöÄ Modern 2D operations</a></a></h3>
<p>Put LayoutTensor into practice with natural 2D indexing and automatic bounds checking.</p>
<p>üí° <strong>Note</strong>: From this puzzle onward, we‚Äôll primarily use LayoutTensor for cleaner, safer GPU code.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D square matrix <code>a</code> and stores it in 2D square matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions</em>.</p>
<h2 id="key-concepts-5"><a class="header" href="#key-concepts-5">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Working with 2D thread indices (<code>thread_idx.x</code>, <code>thread_idx.y</code>)</li>
<li>Converting 2D coordinates to 1D memory indices</li>
<li>Handling boundary checks in two dimensions</li>
</ul>
<p>The key insight is understanding how to map from 2D thread coordinates \((i,j)\) to elements in a row-major matrix of size \(n \times n\), while ensuring thread indices are within bounds.</p>
<ul>
<li><strong>2D indexing</strong>: Each thread has a unique \((i,j)\) position</li>
<li><strong>Memory layout</strong>: Row-major ordering maps 2D to 1D memory</li>
<li><strong>Guard condition</strong>: Need bounds checking in both dimensions</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than matrix elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-3"><a class="header" href="#code-to-complete-3">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn add_10_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p04/p04.mojo" class="filename">View full file: problems/p04/p04.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard add 10 in row-major way!</li>
</ol>
</div>
</details>
<h2 id="running-the-code-4"><a class="header" href="#running-the-code-4">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p04
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-3"><a class="header" href="#solution-3">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    if row &lt; size and col &lt; size:
        out[row * size + col] = a[row * size + col] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ol>
<li>Get 2D indices:  <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard: <code>out[row * size + col] = a[row * size + col] + 10.0</code></li>
</ol>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-layouttensor"><a class="header" href="#introduction-to-layouttensor">Introduction to LayoutTensor</a></h1>
<p>Let‚Äôs take a quick break from solving puzzles to preview a powerful abstraction that will make our GPU programming journey more enjoyable:
ü•Å ‚Ä¶ the <strong><a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a></strong>.</p>
<blockquote>
<p>üí° <em>This is a motivational overview of LayoutTensor‚Äôs capabilities. Don‚Äôt worry about understanding everything now - we‚Äôll explore each feature in depth as we progress through the puzzles</em>.</p>
</blockquote>
<h2 id="the-challenge-growing-complexity"><a class="header" href="#the-challenge-growing-complexity">The challenge: Growing complexity</a></h2>
<p>Let‚Äôs look at the challenges we‚Äôve faced so far:</p>
<pre><code class="language-mojo"># Puzzle 1: Simple indexing
out[i] = a[i] + 10.0

# Puzzle 2: Multiple array management
out[i] = a[i] + b[i]

# Puzzle 3: Bounds checking
if i &lt; size:
    out[i] = a[i] + 10.0
</code></pre>
<p>As dimensions grow, code becomes more complex:</p>
<pre><code class="language-mojo"># Traditional 2D indexing for row-major 2D matrix
idx = row * WIDTH + col
if row &lt; height and col &lt; width:
    out[idx] = a[idx] + 10.0
</code></pre>
<h2 id="the-solution-a-peek-at-layouttensor"><a class="header" href="#the-solution-a-peek-at-layouttensor">The solution: A peek at LayoutTensor</a></h2>
<p>LayoutTensor will help us tackle these challenges with elegant solutions. Here‚Äôs a glimpse of what‚Äôs coming:</p>
<ol>
<li><strong>Natural Indexing</strong>: Use <code>tensor[i, j]</code> instead of manual offset calculations</li>
<li><strong>Automatic Bounds Checking</strong>: Built-in protection against out-of-bounds access</li>
<li><strong>Flexible Memory Layouts</strong>: Support for row-major, column-major, and tiled organizations</li>
<li><strong>Performance Optimization</strong>: Efficient memory access patterns for GPU</li>
</ol>
<h2 id="a-taste-of-whats-ahead"><a class="header" href="#a-taste-of-whats-ahead">A taste of what‚Äôs ahead</a></h2>
<p>Let‚Äôs look at a few examples of what LayoutTensor can do. Don‚Äôt worry about understanding all the details now - we‚Äôll cover each feature thoroughly in upcoming puzzles.</p>
<h3 id="basic-usage-example"><a class="header" href="#basic-usage-example">Basic usage example</a></h3>
<pre><code class="language-mojo">from layout import Layout, LayoutTensor

# Define layout
alias HEIGHT = 2
alias WIDTH = 3
alias layout = Layout.row_major(HEIGHT, WIDTH)

# Create tensor
tensor = LayoutTensor[dtype, layout](buffer.unsafe_ptr())

# Access elements naturally
tensor[0, 0] = 1.0  # First element
tensor[1, 2] = 2.0  # Last element
</code></pre>
<h3 id="preview-of-advanced-features"><a class="header" href="#preview-of-advanced-features">Preview of advanced features</a></h3>
<p>As we progress through the puzzles, you‚Äôll learn about:</p>
<ul>
<li>Shared memory optimizations</li>
<li>Efficient tiling strategies</li>
<li>Vectorized operations</li>
<li>Hardware acceleration</li>
<li>Optimized memory access patterns</li>
</ul>
<pre><code class="language-mojo"># Column-major layout
layout_col = Layout.col_major(HEIGHT, WIDTH)

# Tiled layout (for better cache utilization)
layout_tiled = tensor.tiled[4, 4](HEIGHT, WIDTH)
</code></pre>
<p>Each layout has its advantages:</p>
<ul>
<li>
<p><strong>Row-major</strong>: Elements in a row are contiguous</p>
<pre><code class="language-mojo"># [1 2 3]
# [4 5 6] -&gt; [1 2 3 4 5 6]
layout_row = Layout.row_major(2, 3)
</code></pre>
</li>
<li>
<p><strong>Column-major</strong>: Elements in a column are contiguous</p>
<pre><code class="language-mojo"># [1 2 3]
# [4 5 6] -&gt; [1 4 2 5 3 6]
layout_col = Layout.col_major(2, 3)
</code></pre>
</li>
<li>
<p><strong>Tiled</strong>: Elements grouped in tiles for cache efficiency</p>
<pre><code class="language-mojo"># [[1 2] [3 4]] in 2x2 tiles
layout_tiled = Layout.tiled[2, 2](4, 4)
</code></pre>
</li>
</ul>
<h3 id="advanced-gpu-optimizations"><a class="header" href="#advanced-gpu-optimizations">Advanced GPU optimizations</a></h3>
<p>As you progress, you‚Äôll discover LayoutTensor‚Äôs powerful features for GPU programming:</p>
<ol>
<li><strong>Memory hierarchy management</strong></li>
</ol>
<pre><code class="language-mojo"># Shared memory allocation
shared_mem = tb[dtype]().row_major[BM, BK]().shared().alloc()

# Register allocation
reg_tile = tb[dtype]().row_major[TM, TN]().local().alloc()
</code></pre>
<ol start="2">
<li><strong>Tiling strategies</strong></li>
</ol>
<pre><code class="language-mojo"># Block tiling
block_tile = tensor.tile[BM, BN](block_idx.y, block_idx.x)

# Register tiling
reg_tile = block_tile.tile[TM, TN](thread_row, thread_col)
</code></pre>
<ol start="3">
<li><strong>Memory access patterns</strong></li>
</ol>
<pre><code class="language-mojo"># Vectorized access
vec_tensor = tensor.vectorize[1, simd_width]()

# Asynchronous transfers
copy_dram_to_sram_async[thread_layout=layout](dst, src)
</code></pre>
<ol start="4">
<li><strong>Hardware acceleration</strong></li>
</ol>
<pre><code class="language-mojo"># Tensor Core operations (coming in later puzzles)
mma_op = TensorCore[dtype, out_type, Index(M, N, K)]()
result = mma_op.mma_op(a_reg, b_reg, c_reg)
</code></pre>
<p>üí° <strong>Looking ahead</strong>: Through these puzzles, you‚Äôll learn to:</p>
<ul>
<li>Optimize data access with shared memory</li>
<li>Implement efficient tiling strategies</li>
<li>Leverage vectorized operations</li>
<li>Utilize hardware accelerators</li>
<li>Master memory access patterns</li>
</ul>
<p>Each concept builds on the last, gradually taking you from basic tensor operations to advanced GPU programming. Ready to begin? Let‚Äôs start with the fundamentals!</p>
<h2 id="quick-example"><a class="header" href="#quick-example">Quick example</a></h2>
<p>Let‚Äôs put everything together with a simple example that demonstrates the basics of LayoutTensor:</p>
<pre><code class="language-mojo">from gpu.host import DeviceContext
from layout import Layout, LayoutTensor

alias HEIGHT = 2
alias WIDTH = 3
alias dtype = DType.float32
alias layout = Layout.row_major(HEIGHT, WIDTH)

fn kernel[dtype: DType, layout: Layout](tensor: LayoutTensor[mut=True, dtype, layout]):
    print("Before:")
    print(tensor)
    tensor[0, 0] += 1
    print("After:")
    print(tensor)

def main():
    ctx = DeviceContext()

    a = ctx.enqueue_create_buffer[dtype](HEIGHT * WIDTH).enqueue_fill(0)
    tensor = LayoutTensor[mut=True, dtype, layout](a.unsafe_ptr())
    # Note: since `tensor` is a device tensor we can't print it without the kernel wrapper
    ctx.enqueue_function[kernel[dtype, layout]](tensor, grid_dim=1, block_dim=1)

    ctx.synchronize()
</code></pre>
<p>When we run this code with <code>magic run layout_tensor_intro</code>, we see:</p>
<pre><code class="language-txt">Before:
0.0 0.0 0.0
0.0 0.0 0.0
After:
1.0 0.0 0.0
0.0 0.0 0.0
</code></pre>
<p>Let‚Äôs break down what‚Äôs happening:</p>
<ol>
<li>We create a <code>2 x 3</code> tensor with row-major layout</li>
<li>Initially, all elements are zero</li>
<li>Using natural indexing, we modify a single element</li>
<li>The change is reflected in our output</li>
</ol>
<p>This simple example demonstrates key LayoutTensor benefits:</p>
<ul>
<li>Clean syntax for tensor creation and access</li>
<li>Automatic memory layout handling</li>
<li>Built-in bounds checking</li>
<li>Natural multi-dimensional indexing</li>
</ul>
<p>While this example is straightforward, the same patterns will scale to complex GPU operations in upcoming puzzles. You‚Äôll see how these basic concepts extend to:</p>
<ul>
<li>Multi-threaded GPU operations</li>
<li>Shared memory optimizations</li>
<li>Complex tiling strategies</li>
<li>Hardware-accelerated computations</li>
</ul>
<p>Ready to start your GPU programming journey with LayoutTensor? Let‚Äôs dive into the puzzles!</p>
<p>üí° <strong>Tip</strong>: Keep this example in mind as we progress - we‚Äôll build upon these fundamental concepts to create increasingly sophisticated GPU programs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version"><a class="header" href="#layouttensor-version">LayoutTensor Version</a></h1>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D <em>LayoutTensor</em> <code>a</code> and stores it in 2D <em>LayoutTensor</em> <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions</em>.</p>
<h2 id="key-concepts-6"><a class="header" href="#key-concepts-6">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> for 2D array access</li>
<li>Direct 2D indexing with <code>tensor[i, j]</code></li>
<li>Handling bounds checking with <code>LayoutTensor</code></li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> provides a natural 2D indexing interface, abstracting away the underlying memory layout while still requiring bounds checking.</p>
<ul>
<li><strong>2D access</strong>: Natural \((i,j)\) indexing with <code>LayoutTensor</code></li>
<li><strong>Memory abstraction</strong>: No manual row-major calculation needed</li>
<li><strong>Guard condition</strong>: Still need bounds checking in both dimensions</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than tensor elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-4"><a class="header" href="#code-to-complete-4">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE, SIZE)


fn add_10_2d(
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p04/p04_layout_tensor.mojo" class="filename">View full file: problems/p04/p04_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard add 10 to <code>a[row, col]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-5"><a class="header" href="#running-the-code-5">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p04_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0])
</code></pre>
<h2 id="solution-4"><a class="header" href="#solution-4">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_2d(
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    if col &lt; size and row &lt; size:
        out[row, col] = a[row, col] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution:</p>
<ul>
<li>Gets 2D thread indices with <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Guards against out-of-bounds with <code>if row &lt; size and col &lt; size</code></li>
<li>Uses <code>LayoutTensor</code>‚Äôs 2D indexing: <code>out[row, col] = a[row, col] + 10.0</code></li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-5-broadcast"><a class="header" href="#puzzle-5-broadcast">Puzzle 5: Broadcast</a></h1>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>Implement a kernel that broadcast adds vector <code>a</code> and vector <code>b</code> and stores it in 2D matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions.</em></p>
<p><img src="puzzle_05/./media/videos/720p30/puzzle_05_viz.gif" alt="Broadcast visualization" /></p>
<h2 id="key-concepts-7"><a class="header" href="#key-concepts-7">Key concepts</a></h2>
<ul>
<li>Broadcasting vectors to matrix</li>
<li>2D thread management</li>
<li>Mixed dimension operations</li>
<li>Memory layout patterns</li>
</ul>
<h2 id="implementation-approaches-1"><a class="header" href="#implementation-approaches-1">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-2"><a class="header" href="#-raw-memory-approach-2"><a href="puzzle_05/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how to handle broadcasting with manual memory indexing.</p>
<h3 id="-layouttensor-version"><a class="header" href="#-layouttensor-version"><a href="puzzle_05/./layout_tensor.html">üìê LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor to handle mixed-dimension operations.</p>
<p>üí° <strong>Note</strong>: Notice how LayoutTensor simplifies broadcasting compared to manual indexing.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>Implement a kernel that broadcast adds vector <code>a</code> and vector <code>b</code> and stores it in 2D matrix <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions.</em></p>
<h2 id="key-concepts-8"><a class="header" href="#key-concepts-8">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Broadcasting 1D vectors across different dimensions</li>
<li>Using 2D thread indices for broadcast operations</li>
<li>Handling boundary conditions in broadcast patterns</li>
</ul>
<p>The key insight is understanding how to map elements from two 1D vectors to create a 2D output matrix through broadcasting, while handling thread bounds correctly.</p>
<ul>
<li><strong>Broadcasting</strong>: Each element of <code>a</code> combines with each element of <code>b</code></li>
<li><strong>Thread mapping</strong>: 2D thread grid \((3 \times 3)\) for \(2 \times 2\) output</li>
<li><strong>Vector access</strong>: Different access patterns for <code>a</code> and <code>b</code></li>
<li><strong>Bounds checking</strong>: Guard against threads outside matrix dimensions</li>
</ul>
<h2 id="code-to-complete-5"><a class="header" href="#code-to-complete-5">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn broadcast_add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p05/p05.mojo" class="filename">View full file: problems/p05/p05.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard: think about how to broadcast values of <code>a</code> and <code>b</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-6"><a class="header" href="#running-the-code-6">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p05
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 1.0, 2.0])
</code></pre>
<h2 id="solution-5"><a class="header" href="#solution-5">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn broadcast_add(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    if row &lt; size and col &lt; size:
        out[row * size + col] = a[col] + b[row]


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates fundamental GPU broadcasting concepts without LayoutTensor abstraction:</p>
<ol>
<li>
<p><strong>Thread to matrix mapping</strong></p>
<ul>
<li>Uses <code>thread_idx.y</code> for row access and <code>thread_idx.x</code> for column access</li>
<li>Direct mapping from 2D thread grid to output matrix elements</li>
<li>Handles excess threads (3√ó3 grid) for 2√ó2 output matrix</li>
</ul>
</li>
<li>
<p><strong>Broadcasting mechanics</strong></p>
<ul>
<li>Vector <code>a</code> broadcasts horizontally: same <code>a[col]</code> used across each row</li>
<li>Vector <code>b</code> broadcasts vertically: same <code>b[row]</code> used across each column</li>
<li>Output combines both vectors through addition</li>
</ul>
<pre><code class="language-txt">[ a0 a1 ]  +  [ b0 ]  =  [ a0+b0  a1+b0 ]
              [ b1 ]     [ a0+b1  a1+b1 ]
</code></pre>
</li>
<li>
<p><strong>Bounds checking</strong></p>
<ul>
<li>Single guard condition <code>row &lt; size and col &lt; size</code> handles both dimensions</li>
<li>Prevents out-of-bounds access for both input vectors and output matrix</li>
<li>Required due to 3√ó3 thread grid being larger than 2√ó2 data</li>
</ul>
</li>
</ol>
<p>Compare this with the LayoutTensor version to see how the abstraction simplifies broadcasting operations while maintaining the same underlying concepts.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version-1"><a class="header" href="#layouttensor-version-1">LayoutTensor Version</a></h1>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Implement a kernel that broadcast adds 1D LayoutTensor <code>a</code> and 1D LayoutTensor <code>b</code> and stores it in 2D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have more threads than positions.</em></p>
<h2 id="key-concepts-9"><a class="header" href="#key-concepts-9">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> for broadcast operations</li>
<li>Working with different tensor shapes</li>
<li>Handling 2D indexing with <code>LayoutTensor</code></li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> allows natural broadcasting through different tensor shapes: \((1, n)\) and \((n, 1)\) to \((n,n)\), while still requiring bounds checking.</p>
<ul>
<li><strong>Tensor shapes</strong>: Input vectors have shapes \((1, n)\) and \((n, 1)\)</li>
<li><strong>Broadcasting</strong>: Output combines both dimensions to \((n,n)\)</li>
<li><strong>Guard condition</strong>: Still need bounds checking for output size</li>
<li><strong>Thread bounds</strong>: More threads \((3 \times 3)\) than tensor elements \((2 \times 2)\)</li>
</ul>
<h2 id="code-to-complete-6"><a class="header" href="#code-to-complete-6">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 2
alias BLOCKS_PER_GRID = 1
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias out_layout = Layout.row_major(SIZE, SIZE)
alias a_layout = Layout.row_major(1, SIZE)
alias b_layout = Layout.row_major(SIZE, 1)


fn broadcast_add[
    out_layout: Layout,
    a_layout: Layout,
    b_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, a_layout],
    b: LayoutTensor[mut=False, dtype, b_layout],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p05/p05_layout_tensor.mojo" class="filename">View full file: problems/p05/p05_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Get 2D indices: <code>row = thread_idx.y</code>, <code>col = thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard: think about how to broadcast values of <code>a</code> and <code>b</code> as LayoutTensors</li>
</ol>
</div>
</details>
<h2 id="running-the-code-7"><a class="header" href="#running-the-code-7">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p05_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 1.0, 2.0])
</code></pre>
<h2 id="solution-6"><a class="header" href="#solution-6">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn broadcast_add[
    out_layout: Layout,
    a_layout: Layout,
    b_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, a_layout],
    b: LayoutTensor[mut=False, dtype, b_layout],
    size: Int,
):
    row = thread_idx.y
    col = thread_idx.x
    if row &lt; size and col &lt; size:
        out[row, col] = a[0, col] + b[row, 0]


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates key concepts of LayoutTensor broadcasting and GPU thread mapping:</p>
<ol>
<li>
<p><strong>Thread to matrix mapping</strong></p>
<ul>
<li>Uses <code>thread_idx.y</code> for row access and <code>thread_idx.x</code> for column access</li>
<li>Natural 2D indexing matches the output matrix structure</li>
<li>Excess threads (3√ó3 grid) are handled by bounds checking</li>
</ul>
</li>
<li>
<p><strong>Broadcasting mechanics</strong></p>
<ul>
<li>Input <code>a</code> has shape <code>(1,n)</code>: <code>a[0,col]</code> broadcasts across rows</li>
<li>Input <code>b</code> has shape <code>(n,1)</code>: <code>b[row,0]</code> broadcasts across columns</li>
<li>Output has shape <code>(n,n)</code>: Each element is sum of corresponding broadcasts</li>
</ul>
<pre><code class="language-txt">[ a0 a1 ]  +  [ b0 ]  =  [ a0+b0  a1+b0 ]
              [ b1 ]     [ a0+b1  a1+b1 ]
</code></pre>
</li>
<li>
<p><strong>Bounds Checking</strong></p>
<ul>
<li>Guard condition <code>row &lt; size and col &lt; size</code> prevents out-of-bounds access</li>
<li>Handles both matrix bounds and excess threads efficiently</li>
<li>No need for separate checks for <code>a</code> and <code>b</code> due to broadcasting</li>
</ul>
</li>
</ol>
<p>This pattern forms the foundation for more complex tensor operations we‚Äôll explore in later puzzles.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-6-blocks"><a class="header" href="#puzzle-6-blocks">Puzzle 6: Blocks</a></h1>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of vector <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of a.</em></p>
<p><img src="puzzle_06/./media/videos/720p30/puzzle_06_viz.gif" alt="Blocks visualization" /></p>
<h2 id="key-concepts-10"><a class="header" href="#key-concepts-10">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Processing data larger than thread block size</li>
<li>Coordinating multiple blocks of threads</li>
<li>Computing global thread positions</li>
</ul>
<p>The key insight is understanding how blocks of threads work together to process data that‚Äôs larger than a single block‚Äôs capacity, while maintaining correct element-to-thread mapping.</p>
<h2 id="code-to-complete-7"><a class="header" href="#code-to-complete-7">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 9
alias BLOCKS_PER_GRID = (3, 1)
alias THREADS_PER_BLOCK = (4, 1)
alias dtype = DType.float32


fn add_10_blocks(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    i = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p06/p06.mojo" class="filename">View full file: problems/p06/p06.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global index: <code>i = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if i &lt; size</code></li>
<li>Inside guard: <code>out[i] = a[i] + 10.0</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-8"><a class="header" href="#running-the-code-8">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p06
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0])
</code></pre>
<h2 id="solution-7"><a class="header" href="#solution-7">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    i = block_dim.x * block_idx.x + thread_idx.x
    if i &lt; size:
        out[i] = a[i] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates key concepts of block-based GPU processing:</p>
<ol>
<li>
<p><strong>Global thread indexing</strong></p>
<ul>
<li>Combines block and thread indices: <code>block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Maps each thread to a unique global position</li>
<li>Example for 3 threads per block:
<pre><code class="language-txt">Block 0: [0 1 2]
Block 1: [3 4 5]
Block 2: [6 7 8]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Block coordination</strong></p>
<ul>
<li>Each block processes a contiguous chunk of data</li>
<li>Block size (3) &lt; Data size (9) requires multiple blocks</li>
<li>Automatic work distribution across blocks:
<pre><code class="language-txt">Data:    [0 1 2 3 4 5 6 7 8]
Block 0: [0 1 2]
Block 1:       [3 4 5]
Block 2:             [6 7 8]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Bounds checking</strong></p>
<ul>
<li>Guard condition <code>i &lt; size</code> handles edge cases</li>
<li>Prevents out-of-bounds access when size isn‚Äôt perfectly divisible by block size</li>
<li>Essential for handling partial blocks at the end of data</li>
</ul>
</li>
<li>
<p><strong>Memory access pattern</strong></p>
<ul>
<li>Coalesced memory access: threads in a block access contiguous memory</li>
<li>Each thread processes one element: <code>out[i] = a[i] + 10.0</code></li>
<li>Block-level parallelism enables efficient memory bandwidth utilization</li>
</ul>
</li>
</ol>
<p>This pattern forms the foundation for processing large datasets that exceed the size of a single thread block.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-7-2d-blocks"><a class="header" href="#puzzle-7-2d-blocks">Puzzle 7: 2D Blocks</a></h1>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of matrix <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code> in both directions.</em></p>
<p><img src="puzzle_07/./media/videos/720p30/puzzle_07_viz.gif" alt="Blocks 2D visualization" /></p>
<h2 id="key-concepts-11"><a class="header" href="#key-concepts-11">Key concepts</a></h2>
<ul>
<li>Block-based processing</li>
<li>Grid-block coordination</li>
<li>Multi-block indexing</li>
<li>Memory access patterns</li>
</ul>
<blockquote>
<p>üîë <strong>2D thread indexing convention</strong></p>
<p>We extend the block-based indexing from <a href="puzzle_07/../puzzle_04/puzzle_04.html">puzzle 04</a> to 2D:</p>
<pre><code class="language-txt">Global position calculation:
row = block_dim.y * block_idx.y + thread_idx.y
col = block_dim.x * block_idx.x + thread_idx.x
</code></pre>
<p>For example, with 2√ó2 blocks in a 4√ó4 grid:</p>
<pre><code class="language-txt">Block (0,0):   Block (1,0):
[0,0  0,1]     [0,2  0,3]
[1,0  1,1]     [1,2  1,3]

Block (0,1):   Block (1,1):
[2,0  2,1]     [2,2  2,3]
[3,0  3,1]     [3,2  3,3]
</code></pre>
<p>Each position shows (row, col) for that thread‚Äôs global index.
The block dimensions and indices work together to ensure:</p>
<ul>
<li>Continuous coverage of the 2D space</li>
<li>No overlap between blocks</li>
<li>Efficient memory access patterns</li>
</ul>
</blockquote>
<h2 id="implementation-approaches-2"><a class="header" href="#implementation-approaches-2">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-3"><a class="header" href="#-raw-memory-approach-3"><a href="puzzle_07/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how to handle multi-block operations with manual indexing.</p>
<h3 id="-layouttensor-version-1"><a class="header" href="#-layouttensor-version-1"><a href="puzzle_07/./layout_tensor.html">üìê LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor features to elegantly handle block-based processing.</p>
<p>üí° <strong>Note</strong>: See how LayoutTensor simplifies block coordination and memory access patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of matrix <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code> in both directions.</em></p>
<h2 id="key-concepts-12"><a class="header" href="#key-concepts-12">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Working with 2D block and thread arrangements</li>
<li>Handling matrix data larger than block size</li>
<li>Converting between 2D and linear memory access</li>
</ul>
<p>The key insight is understanding how to coordinate multiple blocks of threads to process a 2D matrix that‚Äôs larger than a single block‚Äôs dimensions.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li><strong>Matrix size</strong>: \(5 \times 5\) elements</li>
<li><strong>2D blocks</strong>: Each block processes a \(3 \times 3\) region</li>
<li><strong>Grid layout</strong>: Blocks arranged in \(2 \times 2\) grid</li>
<li><strong>Total threads</strong>: \(36\) for \(25\) elements</li>
<li><strong>Memory pattern</strong>: Row-major storage for 2D data</li>
<li><strong>Coverage</strong>: Ensuring all matrix elements are processed</li>
</ul>
<h2 id="code-to-complete-8"><a class="header" href="#code-to-complete-8">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 5
alias BLOCKS_PER_GRID = (2, 2)
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32


fn add_10_blocks_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p07/p07.mojo" class="filename">View full file: problems/p07/p07.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global indices: <code>row = block_dim.y * block_idx.y + thread_idx.y</code>, <code>col = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard: think about how to add 10 in row-major way!</li>
</ol>
</div>
</details>
<h2 id="running-the-code-9"><a class="header" href="#running-the-code-9">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p07
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, ... , 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, ... , 11.0])
</code></pre>
<h2 id="solution-8"><a class="header" href="#solution-8">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks_2d(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    if row &lt; size and col &lt; size:
        out[row * size + col] = a[row * size + col] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates key concepts of 2D block-based processing with raw memory:</p>
<ol>
<li>
<p><strong>2D thread indexing</strong></p>
<ul>
<li>Global row: <code>block_dim.y * block_idx.y + thread_idx.y</code></li>
<li>Global col: <code>block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Maps thread grid to matrix elements:
<pre><code class="language-txt">5√ó5 matrix with 3√ó3 blocks:

Block (0,0)         Block (1,0)
[(0,0) (0,1) (0,2)] [(0,3) (0,4)    *  ]
[(1,0) (1,1) (1,2)] [(1,3) (1,4)    *  ]
[(2,0) (2,1) (2,2)] [(2,3) (2,4)    *  ]

Block (0,1)         Block (1,1)
[(3,0) (3,1) (3,2)] [(3,3) (3,4)    *  ]
[(4,0) (4,1) (4,2)] [(4,3) (4,4)    *  ]
[  *     *     *  ] [  *     *      *  ]
</code></pre>
(* = thread exists but outside matrix bounds)</li>
</ul>
</li>
<li>
<p><strong>Memory layout</strong></p>
<ul>
<li>Row-major linear memory: <code>index = row * size + col</code></li>
<li>Example for 5√ó5 matrix:
<pre><code class="language-txt">2D indices:    Linear memory:
(2,1) -&gt; 11   [00 01 02 03 04]
              [05 06 07 08 09]
              [10 11 12 13 14]
              [15 16 17 18 19]
              [20 21 22 23 24]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Bounds checking</strong></p>
<ul>
<li>Guard <code>row &lt; size and col &lt; size</code> handles:
<ul>
<li>Excess threads in partial blocks</li>
<li>Edge cases at matrix boundaries</li>
<li>2√ó2 block grid with 3√ó3 threads each = 36 threads for 25 elements</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Block coordination</strong></p>
<ul>
<li>Each 3√ó3 block processes part of 5√ó5 matrix</li>
<li>2√ó2 grid of blocks ensures full coverage</li>
<li>Overlapping threads handled by bounds check</li>
<li>Efficient parallel processing across blocks</li>
</ul>
</li>
</ol>
<p>This pattern shows how to handle 2D data larger than block size while maintaining efficient memory access and thread coordination.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layouttensor-version-2"><a class="header" href="#layouttensor-version-2">LayoutTensor Version</a></h1>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of 2D LayoutTensor <code>a</code> and stores it in 2D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code> in both directions.</em></p>
<h2 id="key-concepts-13"><a class="header" href="#key-concepts-13">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using <code>LayoutTensor</code> with multiple blocks</li>
<li>Handling large matrices with 2D block organization</li>
<li>Combining block indexing with <code>LayoutTensor</code> access</li>
</ul>
<p>The key insight is that <code>LayoutTensor</code> simplifies 2D indexing while still requiring proper block coordination for large matrices.</p>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<ul>
<li><strong>Matrix size</strong>: \(5 \times 5\) elements</li>
<li><strong>Layout handling</strong>: <code>LayoutTensor</code> manages row-major organization</li>
<li><strong>Block coordination</strong>: Multiple blocks cover the full matrix</li>
<li><strong>2D indexing</strong>: Natural \((i,j)\) access with bounds checking</li>
<li><strong>Total threads</strong>: \(36\) for \(25\) elements</li>
<li><strong>Thread mapping</strong>: Each thread processes one matrix element</li>
</ul>
<h2 id="code-to-complete-9"><a class="header" href="#code-to-complete-9">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE = 5
alias BLOCKS_PER_GRID = (2, 2)
alias THREADS_PER_BLOCK = (3, 3)
alias dtype = DType.float32
alias out_layout = Layout.row_major(SIZE, SIZE)
alias a_layout = Layout.row_major(SIZE, SIZE)


fn add_10_blocks_2d[
    out_layout: Layout,
    a_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, a_layout],
    size: Int,
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p07/p07_layout_tensor.mojo" class="filename">View full file: problems/p07/p07_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global indices: <code>row = block_dim.y * block_idx.y + thread_idx.y</code>, <code>col = block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Add guard: <code>if row &lt; size and col &lt; size</code></li>
<li>Inside guard: think about how to add 10 to 2D LayoutTensor</li>
</ol>
</div>
</details>
<h2 id="running-the-code-10"><a class="header" href="#running-the-code-10">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p07_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, ... , 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, ... , 11.0])
</code></pre>
<h2 id="solution-9"><a class="header" href="#solution-9">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_blocks_2d[
    out_layout: Layout,
    a_layout: Layout,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, a_layout],
    size: Int,
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    if row &lt; size and col &lt; size:
        out[row, col] = a[row, col] + 10.0


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates how LayoutTensor simplifies 2D block-based processing:</p>
<ol>
<li>
<p><strong>2D thread indexing</strong></p>
<ul>
<li>Global row: <code>block_dim.y * block_idx.y + thread_idx.y</code></li>
<li>Global col: <code>block_dim.x * block_idx.x + thread_idx.x</code></li>
<li>Maps thread grid to tensor elements:
<pre><code class="language-txt">5√ó5 tensor with 3√ó3 blocks:

Block (0,0)         Block (1,0)
[(0,0) (0,1) (0,2)] [(0,3) (0,4)    *  ]
[(1,0) (1,1) (1,2)] [(1,3) (1,4)    *  ]
[(2,0) (2,1) (2,2)] [(2,3) (2,4)    *  ]

Block (0,1)         Block (1,1)
[(3,0) (3,1) (3,2)] [(3,3) (3,4)    *  ]
[(4,0) (4,1) (4,2)] [(4,3) (4,4)    *  ]
[  *     *     *  ] [  *     *      *  ]
</code></pre>
(* = thread exists but outside tensor bounds)</li>
</ul>
</li>
<li>
<p><strong>LayoutTensor benefits</strong></p>
<ul>
<li>Natural 2D indexing: <code>tensor[row, col]</code> instead of manual offset calculation</li>
<li>Automatic memory layout optimization</li>
<li>Example access pattern:
<pre><code class="language-txt">Raw memory:         LayoutTensor:
row * size + col    tensor[row, col]
(2,1) -&gt; 11        (2,1) -&gt; same element
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Bounds checking</strong></p>
<ul>
<li>Guard <code>row &lt; size and col &lt; size</code> handles:
<ul>
<li>Excess threads in partial blocks</li>
<li>Edge cases at tensor boundaries</li>
<li>Automatic memory layout handling by LayoutTensor</li>
<li>36 threads (2√ó2 blocks of 3√ó3) for 25 elements</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Block coordination</strong></p>
<ul>
<li>Each 3√ó3 block processes part of 5√ó5 tensor</li>
<li>LayoutTensor handles:
<ul>
<li>Memory layout optimization</li>
<li>Efficient access patterns</li>
<li>Block boundary coordination</li>
<li>Cache-friendly data access</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>This pattern shows how LayoutTensor simplifies 2D block processing while maintaining optimal memory access patterns and thread coordination.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-8-shared-memory"><a class="header" href="#puzzle-8-shared-memory">Puzzle 8: Shared Memory</a></h1>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of a vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code>.</em></p>
<p><img src="puzzle_08/./media/videos/720p30/puzzle_08_viz.gif" alt="Shared memory visualization" /></p>
<h2 id="implementation-approaches-3"><a class="header" href="#implementation-approaches-3">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-4"><a class="header" href="#-raw-memory-approach-4"><a href="puzzle_08/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how to manually manage shared memory and synchronization.</p>
<h3 id="-layouttensor-version-2"><a class="header" href="#-layouttensor-version-2"><a href="puzzle_08/./layout_tensor.html">üìê LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor‚Äôs built-in shared memory management features.</p>
<p>üí° <strong>Note</strong>: Experience how LayoutTensor simplifies shared memory operations while maintaining performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of a vector <code>a</code> and stores it in <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code>.</em></p>
<h2 id="key-concepts-14"><a class="header" href="#key-concepts-14">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using shared memory within thread blocks</li>
<li>Synchronizing threads with barriers</li>
<li>Managing block-local data storage</li>
</ul>
<p>The key insight is understanding how shared memory provides fast, block-local storage that all threads in a block can access, requiring careful coordination between threads.</p>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 4</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Shared memory</strong>: Fast storage shared by threads in a block</li>
<li><strong>Thread sync</strong>: Coordination using <code>barrier()</code></li>
<li><strong>Memory scope</strong>: Shared memory only visible within block</li>
<li><strong>Access pattern</strong>: Local vs global indexing</li>
</ul>
<blockquote>
<p><strong>Warning</strong>: Each block can only have a <em>constant</em> amount of shared memory that threads in that block can read and write to. This needs to be a literal python constant, not a variable. After writing to shared memory you need to call <a href="https://docs.modular.com/mojo/stdlib/gpu/sync/barrier/">barrier</a> to ensure that threads do not cross.</p>
</blockquote>
<h2 id="code-to-complete-10"><a class="header" href="#code-to-complete-10">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 4
alias SIZE = 8
alias BLOCKS_PER_GRID = (2, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32


fn add_10_shared(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB,
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # local data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # wait for all threads to complete
    # works within a thread block
    barrier()

    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p08/p08.mojo" class="filename">View full file: problems/p08/p08.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Wait for shared memory load with <code>barrier()</code></li>
<li>Use <code>local_i</code> to access shared memory: <code>shared[local_i]</code></li>
<li>Use <code>global_i</code> for output: <code>out[global_i]</code></li>
<li>Add guard: <code>if global_i &lt; size</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-11"><a class="header" href="#running-the-code-11">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p08
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])
</code></pre>
<h2 id="solution-10"><a class="header" href="#solution-10">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_shared(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB,
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # local data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # wait for all threads to complete
    # works within a thread block
    barrier()

    # process using shared memory
    if global_i &lt; size:
        out[global_i] = shared[local_i] + 10


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates key concepts of shared memory usage in GPU programming:</p>
<ol>
<li>
<p><strong>Memory hierarchy</strong></p>
<ul>
<li>Global memory: <code>a</code> and <code>out</code> arrays (slow, visible to all blocks)</li>
<li>Shared memory: <code>shared</code> array (fast, thread-block local)</li>
<li>Example for 8 elements with 4 threads per block:
<pre><code class="language-txt">Global array a: [1 1 1 1 | 1 1 1 1]  # Input: all ones

Block (0):      Block (1):
shared[0..3]    shared[0..3]
[1 1 1 1]       [1 1 1 1]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Thread coordination</strong></p>
<ul>
<li>Load phase:
<pre><code class="language-txt">Thread 0: shared[0] = a[0]=1    Thread 2: shared[2] = a[2]=1
Thread 1: shared[1] = a[1]=1    Thread 3: shared[3] = a[3]=1
barrier()    ‚Üì         ‚Üì        ‚Üì         ‚Üì   # Wait for all loads
</code></pre>
</li>
<li>Process phase: Each thread adds 10 to its shared memory value</li>
<li>Result: <code>out[i] = shared[local_i] + 10 = 11</code></li>
</ul>
</li>
<li>
<p><strong>Index mapping</strong></p>
<ul>
<li>Global index: <code>block_dim.x * block_idx.x + thread_idx.x</code>
<pre><code class="language-txt">Block 0 output: [11 11 11 11]
Block 1 output: [11 11 11 11]
</code></pre>
</li>
<li>Local index: <code>thread_idx.x</code> for shared memory access
<pre><code class="language-txt">Both blocks process: 1 + 10 = 11
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Memory access pattern</strong></p>
<ul>
<li>Load: Global ‚Üí Shared (coalesced reads of 1s)</li>
<li>Sync: <code>barrier()</code> ensures all loads complete</li>
<li>Process: Add 10 to shared values</li>
<li>Store: Write 11s back to global memory</li>
</ul>
</li>
</ol>
<p>This pattern shows how to use shared memory to optimize data access while maintaining thread coordination within blocks.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>Implement a kernel that adds 10 to each position of a 1D ayoutTensor <code>a</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have fewer threads per block than the size of <code>a</code>.</em></p>
<h2 id="key-concepts-15"><a class="header" href="#key-concepts-15">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using LayoutTensor‚Äôs shared memory features</li>
<li>Thread synchronization with shared memory</li>
<li>Block-local data management with tensor builder</li>
</ul>
<p>The key insight is how LayoutTensor simplifies shared memory management while maintaining the performance benefits of block-local storage.</p>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 4</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<h2 id="key-differences-from-raw-approach"><a class="header" href="#key-differences-from-raw-approach">Key differences from raw approach</a></h2>
<ol>
<li>
<p><strong>Memory allocation</strong>: We will use <a href="https://docs.modular.com/mojo/stdlib/layout/tensor_builder/LayoutTensorBuild">LayoutTensorBuild</a> instead of <a href="https://docs.modular.com/mojo/stdlib/memory/memory/stack_allocation/">stack_allocation</a></p>
<pre><code class="language-mojo"># Raw approach
shared = stack_allocation[TPB, Scalar[dtype]]()

# LayoutTensor approach
shared = LayoutTensorBuild[dtype]().row_major[TPB]().shared().alloc()
</code></pre>
</li>
<li>
<p><strong>Memory access</strong>: Same syntax</p>
<pre><code class="language-mojo"># Raw approach
shared[local_i] = a[global_i]

# LayoutTensor approach
shared[local_i] = a[global_i]
</code></pre>
</li>
<li>
<p><strong>Safety features</strong>:</p>
<ul>
<li>Type safety</li>
<li>Layout management</li>
<li>Memory alignment handling</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Note</strong>: LayoutTensor handles memory layout, but you still need to manage thread synchronization with <code>barrier()</code> when using shared memory.</p>
</blockquote>
<h2 id="code-to-complete-11"><a class="header" href="#code-to-complete-11">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 4
alias SIZE = 8
alias BLOCKS_PER_GRID = (2, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn add_10_shared_layout_tensor[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    # FILL ME IN (roughly 2 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p08/p08_layout_tensor.mojo" class="filename">View full file: problems/p08/p08_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Load data with natural indexing: <code>shared[local_i] = a[global_i]</code></li>
<li>Synchronize with <code>barrier()</code></li>
<li>Process data using shared memory indices</li>
<li>Guard against out-of-bounds access</li>
</ol>
</div>
</details>
<h2 id="running-the-code-12"><a class="header" href="#running-the-code-12">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p08_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])
</code></pre>
<h2 id="solution-11"><a class="header" href="#solution-11">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn add_10_shared_layout_tensor[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    if global_i &lt; size:
        out[global_i] = shared[local_i] + 10


</code></pre>
<div class="solution-explanation">
<p>This solution demonstrates how LayoutTensor simplifies shared memory usage while maintaining performance:</p>
<ol>
<li>
<p><strong>Memory hierarchy with LayoutTensor</strong></p>
<ul>
<li>Global tensors: <code>a</code> and <code>out</code> (slow, visible to all blocks)</li>
<li>Shared tensor: <code>shared</code> (fast, thread-block local)</li>
<li>Example for 8 elements with 4 threads per block:
<pre><code class="language-txt">Global tensor a: [1 1 1 1 | 1 1 1 1]  # Input: all ones

Block (0):         Block (1):
shared[0..3]       shared[0..3]
[1 1 1 1]          [1 1 1 1]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Thread coordination</strong></p>
<ul>
<li>Load phase with natural indexing:
<pre><code class="language-txt">Thread 0: shared[0] = a[0]=1    Thread 2: shared[2] = a[2]=1
Thread 1: shared[1] = a[1]=1    Thread 3: shared[3] = a[3]=1
barrier()    ‚Üì         ‚Üì        ‚Üì         ‚Üì   # Wait for all loads
</code></pre>
</li>
<li>Process phase: Each thread adds 10 to its shared tensor value</li>
<li>Result: <code>out[global_i] = shared[local_i] + 10 = 11</code></li>
</ul>
</li>
<li>
<p><strong>LayoutTensor benefits</strong></p>
<ul>
<li>Shared memory allocation:
<pre><code class="language-txt"># Clean tensor builder API
shared = tb[dtype]().row_major[TPB]().shared().alloc()
</code></pre>
</li>
<li>Natural indexing for both global and shared:
<pre><code class="language-txt">Block 0 output: [11 11 11 11]
Block 1 output: [11 11 11 11]
</code></pre>
</li>
<li>Built-in layout management and type safety</li>
</ul>
</li>
<li>
<p><strong>Memory access pattern</strong></p>
<ul>
<li>Load: Global tensor ‚Üí Shared tensor (optimized)</li>
<li>Sync: Same <code>barrier()</code> requirement as raw version</li>
<li>Process: Add 10 to shared values</li>
<li>Store: Write 11s back to global tensor</li>
</ul>
</li>
</ol>
<p>This pattern shows how LayoutTensor maintains the performance benefits of shared memory while providing a more ergonomic API and built-in features.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-9-pooling"><a class="header" href="#puzzle-9-pooling">Puzzle 9: Pooling</a></h1>
<h2 id="overview-16"><a class="header" href="#overview-16">Overview</a></h2>
<p>Implement a kernel that compute the running sum of the last 3 positions of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 1 global read and 1 global write per thread.</em></p>
<p><img src="puzzle_09/./media/videos/720p30/puzzle_09_viz.gif" alt="Pooling visualization" /></p>
<h2 id="implementation-approaches-4"><a class="header" href="#implementation-approaches-4">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-5"><a class="header" href="#-raw-memory-approach-5"><a href="puzzle_09/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how to implement sliding window operations with manual memory management and synchronization.</p>
<h3 id="-layouttensor-version-3"><a class="header" href="#-layouttensor-version-3"><a href="puzzle_09/./layout_tensor.html">üìê LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor‚Äôs features for efficient window-based operations and shared memory management.</p>
<p>üí° <strong>Note</strong>: See how LayoutTensor simplifies sliding window operations while maintaining efficient memory access patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-17"><a class="header" href="#overview-17">Overview</a></h2>
<p>Implement a kernel that compute the running sum of the last 3 positions of vector <code>a</code> and stores it in vector <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 1 global read and 1 global write per thread.</em></p>
<h2 id="key-concepts-16"><a class="header" href="#key-concepts-16">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using shared memory for sliding window operations</li>
<li>Handling boundary conditions in pooling</li>
<li>Coordinating thread access to neighboring elements</li>
</ul>
<p>The key insight is understanding how to efficiently access a window of elements using shared memory, with special handling for the first elements in the sequence.</p>
<h2 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Window size: 3 elements</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Window access</strong>: Each output depends on up to 3 previous elements</li>
<li><strong>Edge handling</strong>: First two positions need special treatment</li>
<li><strong>Memory pattern</strong>: One shared memory load per thread</li>
<li><strong>Thread sync</strong>: Coordination before window operations</li>
</ul>
<h2 id="code-to-complete-12"><a class="header" href="#code-to-complete-12">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32


fn pooling(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB,
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 10 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p09/p09.mojo" class="filename">View full file: problems/p09/p09.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load data and call <code>barrier()</code></li>
<li>Special cases: <code>out[0] = shared[0]</code>, <code>out[1] = shared[0] + shared[1]</code></li>
<li>General case: <code>if 1 &lt; global_i &lt; size</code></li>
<li>Sum three elements: <code>shared[local_i - 2] + shared[local_i - 1] + shared[local_i]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-13"><a class="header" href="#running-the-code-13">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p09
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0])
</code></pre>
<h2 id="solution-12"><a class="header" href="#solution-12">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn pooling(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB,
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    if global_i == 0:
        out[0] = shared[0]
    elif global_i == 1:
        out[1] = shared[0] + shared[1]
    elif 1 &lt; global_i &lt; size:
        out[global_i] = (
            shared[local_i - 2] + shared[local_i - 1] + shared[local_i]
        )


</code></pre>
<div class="solution-explanation">
<p>The solution implements a sliding window sum using shared memory with these key steps:</p>
<ol>
<li>
<p><strong>Shared memory setup</strong></p>
<ul>
<li>Allocates <code>TPB</code> elements in shared memory:
<pre><code class="language-txt">Input array:  [0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0]
Block shared: [0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0]
</code></pre>
</li>
<li>Each thread loads one element from global memory</li>
<li><code>barrier()</code> ensures all data is loaded</li>
</ul>
</li>
<li>
<p><strong>Boundary cases</strong></p>
<ul>
<li>Position 0: Single element
<pre><code class="language-txt">out[0] = shared[0] = 0.0
</code></pre>
</li>
<li>Position 1: Sum of first two elements
<pre><code class="language-txt">out[1] = shared[0] + shared[1] = 0.0 + 1.0 = 1.0
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Main window operation</strong></p>
<ul>
<li>For positions 2 and beyond:
<pre><code class="language-txt">Position 2: shared[0] + shared[1] + shared[2] = 0.0 + 1.0 + 2.0 = 3.0
Position 3: shared[1] + shared[2] + shared[3] = 1.0 + 2.0 + 3.0 = 6.0
Position 4: shared[2] + shared[3] + shared[4] = 2.0 + 3.0 + 4.0 = 9.0
...
</code></pre>
</li>
<li>Window calculation using local indices:
<pre><code class="language-txt"># Sliding window of 3 elements
window_sum = shared[i-2] + shared[i-1] + shared[i]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Memory access pattern</strong></p>
<ul>
<li>One global read per thread into shared memory</li>
<li>One global write per thread from shared memory</li>
<li>Uses shared memory for efficient neighbor access</li>
<li>Maintains coalesced memory access pattern</li>
</ul>
</li>
</ol>
<p>This approach optimizes performance through:</p>
<ul>
<li>Minimal global memory access</li>
<li>Fast shared memory neighbor lookups</li>
<li>Clean boundary handling</li>
<li>Efficient memory coalescing</li>
</ul>
<p>The final output shows the cumulative window sums:</p>
<pre><code class="language-txt">[0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0]
</code></pre>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-18"><a class="header" href="#overview-18">Overview</a></h2>
<p>Implement a kernel that compute the running sum of the last 3 positions of 1D LayoutTensor <code>a</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 1 global read and 1 global write per thread.</em></p>
<h2 id="key-concepts-17"><a class="header" href="#key-concepts-17">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Using LayoutTensor for sliding window operations</li>
<li>Managing shared memory with <code>LayoutTensorBuilder</code> that we saw in <a href="puzzle_09/../puzzle_08/layout_tensor.html">puzzle_08</a></li>
<li>Efficient neighbor access patterns</li>
<li>Boundary condition handling</li>
</ul>
<p>The key insight is how LayoutTensor simplifies shared memory management while maintaining efficient window-based operations.</p>
<h2 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Window size: 3 elements</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Tensor builder</strong>: Use <code>LayoutTensorBuilder[dtype]().row_major[TPB]().shared().alloc()</code></li>
<li><strong>Window access</strong>: Natural indexing for 3-element windows</li>
<li><strong>Edge handling</strong>: Special cases for first two positions</li>
<li><strong>Memory pattern</strong>: One shared memory load per thread</li>
</ul>
<h2 id="code-to-complete-13"><a class="header" href="#code-to-complete-13">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn pooling[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FIX ME IN (roughly 10 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p09/p09_layout_tensor.mojo" class="filename">View full file: problems/p09/p09_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Load data with natural indexing: <code>shared[local_i] = a[global_i]</code></li>
<li>Handle special cases for first two elements</li>
<li>Use shared memory for window operations</li>
<li>Guard against out-of-bounds access</li>
</ol>
</div>
</details>
<h2 id="running-the-code-14"><a class="header" href="#running-the-code-14">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p09_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0])
</code></pre>
<h2 id="solution-13"><a class="header" href="#solution-13">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn pooling[
    layout: Layout
](
    out: LayoutTensor[mut=True, dtype, layout],
    a: LayoutTensor[mut=True, dtype, layout],
    size: Int,
):
    # Allocate shared memory using tensor builder
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    # Load data into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    # Synchronize threads within block
    barrier()

    # Handle first two special cases
    if global_i == 0:
        out[0] = shared[0]
    elif global_i == 1:
        out[1] = shared[0] + shared[1]
    # Handle general case
    elif 1 &lt; global_i &lt; size:
        out[global_i] = (
            shared[local_i - 2] + shared[local_i - 1] + shared[local_i]
        )


</code></pre>
<div class="solution-explanation">
<p>The solution implements a sliding window sum using LayoutTensor with these key steps:</p>
<ol>
<li>
<p><strong>Shared memory setup</strong></p>
<ul>
<li>Tensor builder creates block-local storage:
<pre><code class="language-txt">shared = tb[dtype]().row_major[TPB]().shared().alloc()
</code></pre>
</li>
<li>Each thread loads one element:
<pre><code class="language-txt">Input array:  [0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0]
Block shared: [0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0]
</code></pre>
</li>
<li><code>barrier()</code> ensures all data is loaded</li>
</ul>
</li>
<li>
<p><strong>Boundary cases</strong></p>
<ul>
<li>Position 0: Single element
<pre><code class="language-txt">out[0] = shared[0] = 0.0
</code></pre>
</li>
<li>Position 1: Sum of first two elements
<pre><code class="language-txt">out[1] = shared[0] + shared[1] = 0.0 + 1.0 = 1.0
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Main window operation</strong></p>
<ul>
<li>For positions 2 and beyond:
<pre><code class="language-txt">Position 2: shared[0] + shared[1] + shared[2] = 0.0 + 1.0 + 2.0 = 3.0
Position 3: shared[1] + shared[2] + shared[3] = 1.0 + 2.0 + 3.0 = 6.0
Position 4: shared[2] + shared[3] + shared[4] = 2.0 + 3.0 + 4.0 = 9.0
...
</code></pre>
</li>
<li>Natural indexing with LayoutTensor:
<pre><code class="language-txt"># Sliding window of 3 elements
window_sum = shared[i-2] + shared[i-1] + shared[i]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Memory access pattern</strong></p>
<ul>
<li>One global read per thread into shared tensor</li>
<li>Efficient neighbor access through shared memory</li>
<li>LayoutTensor benefits:
<ul>
<li>Automatic bounds checking</li>
<li>Natural window indexing</li>
<li>Layout-aware memory access</li>
<li>Type safety throughout</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>This approach combines the performance of shared memory with LayoutTensor‚Äôs safety and ergonomics:</p>
<ul>
<li>Minimizes global memory access</li>
<li>Simplifies window operations</li>
<li>Handles boundaries cleanly</li>
<li>Maintains coalesced access patterns</li>
</ul>
<p>The final output shows the cumulative window sums:</p>
<pre><code class="language-txt">[0.0, 1.0, 3.0, 6.0, 9.0, 12.0, 15.0, 18.0]
</code></pre>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-10-dot-product"><a class="header" href="#puzzle-10-dot-product">Puzzle 10: Dot Product</a></h1>
<h2 id="overview-19"><a class="header" href="#overview-19">Overview</a></h2>
<p>Implement a kernel that computes the dot-product of vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code> (single number).</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 2 global reads and 1 global write per thread.</em></p>
<p><img src="puzzle_10/./media/videos/720p30/puzzle_10_viz.gif" alt="Dot product visualization" /></p>
<h2 id="implementation-approaches-5"><a class="header" href="#implementation-approaches-5">Implementation approaches</a></h2>
<h3 id="-raw-memory-approach-6"><a class="header" href="#-raw-memory-approach-6"><a href="puzzle_10/./raw.html">üî∞ Raw memory approach</a></a></h3>
<p>Learn how to implement the reduction with manual memory management and synchronization.</p>
<h3 id="-layouttensor-version-4"><a class="header" href="#-layouttensor-version-4"><a href="puzzle_10/./layout_tensor.html">üìê LayoutTensor Version</a></a></h3>
<p>Use LayoutTensor‚Äôs features for efficient reduction and shared memory management.</p>
<p>üí° <strong>Note</strong>: See how LayoutTensor simplifies efficient memory access patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-20"><a class="header" href="#overview-20">Overview</a></h2>
<p>Implement a kernel that computes the dot-product of vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code> (single number).</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 2 global reads and 1 global write per thread.</em></p>
<h2 id="key-concepts-18"><a class="header" href="#key-concepts-18">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Implementing parallel reduction operations</li>
<li>Using shared memory for intermediate results</li>
<li>Coordinating threads for collective operations</li>
</ul>
<p>The key insight is understanding how to efficiently combine multiple values into a single result using parallel computation and shared memory.</p>
<h2 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h2>
<ul>
<li>Vector size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Output size: 1 element</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Element access</strong>: Each thread reads corresponding elements from <code>a</code> and <code>b</code></li>
<li><strong>Partial results</strong>: Computing and storing intermediate values</li>
<li><strong>Thread coordination</strong>: Synchronizing before combining results</li>
<li><strong>Final reduction</strong>: Converting partial results to scalar output</li>
</ul>
<p><em>Note: For this problem, you don‚Äôt need to worry about number of shared reads. We will
handle that challenge later.</em></p>
<h2 id="code-to-complete-14"><a class="header" href="#code-to-complete-14">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (SIZE, 1)
alias dtype = DType.float32


fn dot_product(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    # FILL ME IN (roughly 13 lines)
    ...


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p10/p10.mojo" class="filename">View full file: problems/p10/p10.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Store <code>a[global_i] * b[global_i]</code> in <code>shared[local_i]</code></li>
<li>Call <code>barrier()</code> to synchronize</li>
<li>Use thread 0 to sum all products in shared memory</li>
<li>Write final sum to <code>out[0]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-15"><a class="header" href="#running-the-code-15">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p10
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0])
expected: HostBuffer([140.0])
</code></pre>
<h2 id="solution-14"><a class="header" href="#solution-14">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn dot_product(
    out: UnsafePointer[Scalar[dtype]],
    a: UnsafePointer[Scalar[dtype]],
    b: UnsafePointer[Scalar[dtype]],
    size: Int,
):
    shared = stack_allocation[
        TPB,
        Scalar[dtype],
        address_space = AddressSpace.SHARED,
    ]()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    if global_i &lt; size:
        shared[local_i] = a[global_i] * b[global_i]

    barrier()

    # The following causes race condition: all threads writing to the same location
    # out[0] += shared[local_i]

    # Instead can do parallel reduction in shared memory as opposed to
    # global memory which has no guarantee on synchronization.
    # Loops using global memory can cause thread divergence because
    # fundamentally GPUs execute threads in warps (groups of 32 threads typically)
    # and warps can be scheduled independently.
    # However, shared memory does not have such issues as long as we use `barrier()`
    # correctly when we're in the same thread block.
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            shared[local_i] += shared[local_i + stride]

        barrier()
        stride //= 2

    # only thread 0 writes the final result
    if local_i == 0:
        out[0] = shared[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel reduction algorithm for dot product computation using shared memory. Here‚Äôs a detailed breakdown:</p>
<h3 id="phase-1-element-wise-multiplication"><a class="header" href="#phase-1-element-wise-multiplication">Phase 1: Element-wise Multiplication</a></h3>
<p>Each thread performs one multiplication:</p>
<pre><code class="language-txt">Thread i: shared[i] = a[i] * b[i]
</code></pre>
<h3 id="phase-2-parallel-reduction"><a class="header" href="#phase-2-parallel-reduction">Phase 2: Parallel Reduction</a></h3>
<p>The reduction uses a tree-based approach that halves active threads in each step:</p>
<pre><code class="language-txt">Initial:  [0*0  1*1  2*2  3*3  4*4  5*5  6*6  7*7]
        = [0    1    4    9    16   25   36   49]

Step 1:   [0+16 1+25 4+36 9+49  16   25   36   49]
        = [16   26   40   58   16   25   36   49]

Step 2:   [16+40 26+58 40   58   16   25   36   49]
        = [56   84   40   58   16   25   36   49]

Step 3:   [56+84  84   40   58   16   25   36   49]
        = [140   84   40   58   16   25   36   49]
</code></pre>
<h3 id="key-implementation-features"><a class="header" href="#key-implementation-features">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>Each thread loads exactly two values from global memory (<code>a[i]</code>, <code>b[i]</code>)</li>
<li>Uses shared memory for intermediate results</li>
<li>Final result written once to global memory</li>
</ul>
</li>
<li>
<p><strong>Thread Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> after initial multiplication</li>
<li><code>barrier()</code> after each reduction step</li>
<li>Prevents race conditions between reduction steps</li>
</ul>
</li>
<li>
<p><strong>Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; stride:
        shared[local_i] += shared[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
<ul>
<li>Halves stride in each step</li>
<li>Only active threads perform additions</li>
<li>Maintains work efficiency</li>
</ul>
</li>
<li>
<p><strong>Performance Considerations</strong>:</p>
<ul>
<li>\(\log_2(n)\) steps for \(n\) elements</li>
<li>Coalesced memory access pattern</li>
<li>Minimal thread divergence</li>
<li>Efficient use of shared memory</li>
</ul>
</li>
</ol>
<p>This implementation achieves \(O(\log n)\) time complexity compared to \(O(n)\) in sequential execution, demonstrating the power of parallel reduction algorithms.</p>
<h3 id="barrier-synchronization-importance"><a class="header" href="#barrier-synchronization-importance">Barrier Synchronization Importance</a></h3>
<p>The <code>barrier()</code> between reduction steps is critical for correctness. Here‚Äôs why:</p>
<p>Without <code>barrier()</code>, race conditions occur:</p>
<pre><code class="language-text">Initial shared memory: [0 1 4 9 16 25 36 49]

Step 1 (stride = 4):
Thread 0 reads: shared[0] = 0, shared[4] = 16
Thread 1 reads: shared[1] = 1, shared[5] = 25
Thread 2 reads: shared[2] = 4, shared[6] = 36
Thread 3 reads: shared[3] = 9, shared[7] = 49

Without barrier:
- Thread 0 writes: shared[0] = 0 + 16 = 16
- Thread 1 starts next step (stride = 2) before Thread 0 finishes
  and reads old value shared[0] = 0 instead of 16!
</code></pre>
<p>With <code>barrier()</code>:</p>
<pre><code class="language-text">Step 1 (stride = 4):
All threads write their sums:
[16 26 40 58 16 25 36 49]
barrier() ensures ALL threads see these values

Step 2 (stride = 2):
Now threads safely read the updated values:
Thread 0: shared[0] = 16 + 40 = 56
Thread 1: shared[1] = 26 + 58 = 84
</code></pre>
<p>The <code>barrier()</code> ensures:</p>
<ol>
<li>All writes from current step complete</li>
<li>All threads see updated values</li>
<li>No thread starts next iteration early</li>
<li>Consistent shared memory state</li>
</ol>
<p>Without these synchronization points, we could get:</p>
<ul>
<li>Memory race conditions</li>
<li>Threads reading stale values</li>
<li>Non-deterministic results</li>
<li>Incorrect final sum</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h2 id="overview-21"><a class="header" href="#overview-21">Overview</a></h2>
<p>Implement a kernel that computes the dot-product of 1D LayoutTensor <code>a</code> and 1D LayoutTensor <code>b</code> and stores it in 1D LayoutTensor <code>out</code> (single number).</p>
<p><strong>Note:</strong> <em>You have 1 thread per position. You only need 2 global reads and 1 global write per thread.</em></p>
<h2 id="key-concepts-19"><a class="header" href="#key-concepts-19">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Similar to the <a href="puzzle_10/../puzzle_08/layout_tensor.html">puzzle 8</a> and <a href="puzzle_10/../puzzle_09/layout_tensor.html">puzzle 9</a>, implementing parallel reduction with LayoutTensor</li>
<li>Managing shared memory using <code>LayoutTensorBuilder</code></li>
<li>Coordinating threads for collective operations</li>
<li>Using layout-aware tensor operations</li>
</ul>
<p>The key insight is how LayoutTensor simplifies memory management while maintaining efficient parallel reduction patterns.</p>
<h2 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h2>
<ul>
<li>Vector size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Output size: 1 element</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Tensor builder</strong>: Use <code>LayoutTensorBuilder[dtype]().row_major[TPB]().shared().alloc()</code></li>
<li><strong>Element access</strong>: Natural indexing with bounds checking</li>
<li><strong>Layout handling</strong>: Separate layouts for input and output</li>
<li><strong>Thread coordination</strong>: Same synchronization patterns with <code>barrier()</code></li>
</ul>
<h2 id="code-to-complete-15"><a class="header" href="#code-to-complete-15">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (SIZE, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)
alias out_layout = Layout.row_major(1)


fn dot_product[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, in_layout],
    b: LayoutTensor[mut=True, dtype, in_layout],
    size: Int,
):
    # FILL ME IN (roughly 13 lines)
    ...


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p10/p10_layout_tensor.mojo" class="filename">View full file: problems/p10/p10_layout_tensor.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Create shared memory with tensor builder</li>
<li>Store <code>a[global_i] * b[global_i]</code> in <code>shared[local_i]</code></li>
<li>Use parallel reduction pattern with <code>barrier()</code></li>
<li>Let thread 0 write final result to <code>out[0]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-16"><a class="header" href="#running-the-code-16">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p10_layout_tensor
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0])
expected: HostBuffer([140.0])
</code></pre>
<h2 id="solution-15"><a class="header" href="#solution-15">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn dot_product[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    a: LayoutTensor[mut=True, dtype, in_layout],
    b: LayoutTensor[mut=True, dtype, in_layout],
    size: Int,
):
    shared = tb[dtype]().row_major[TPB]().shared().alloc()
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x

    # Compute element-wise multiplication into shared memory
    if global_i &lt; size:
        shared[local_i] = a[global_i] * b[global_i]

    # Synchronize threads within block
    barrier()

    # Parallel reduction in shared memory
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            shared[local_i] += shared[local_i + stride]

        barrier()
        stride //= 2

    # Only thread 0 writes the final result
    if local_i == 0:
        out[0] = shared[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel reduction for dot product using LayoutTensor. Here‚Äôs the detailed breakdown:</p>
<h3 id="phase-1-element-wise-multiplication-1"><a class="header" href="#phase-1-element-wise-multiplication-1">Phase 1: Element-wise Multiplication</a></h3>
<p>Each thread performs one multiplication with natural indexing:</p>
<pre><code class="language-mojo">shared[local_i] = a[global_i] * b[global_i]
</code></pre>
<h3 id="phase-2-parallel-reduction-1"><a class="header" href="#phase-2-parallel-reduction-1">Phase 2: Parallel Reduction</a></h3>
<p>Tree-based reduction with layout-aware operations:</p>
<pre><code class="language-txt">Initial:  [0*0  1*1  2*2  3*3  4*4  5*5  6*6  7*7]
        = [0    1    4    9    16   25   36   49]

Step 1:   [0+16 1+25 4+36 9+49  16   25   36   49]
        = [16   26   40   58   16   25   36   49]

Step 2:   [16+40 26+58 40   58   16   25   36   49]
        = [56   84   40   58   16   25   36   49]

Step 3:   [56+84  84   40   58   16   25   36   49]
        = [140   84   40   58   16   25   36   49]
</code></pre>
<h3 id="key-implementation-features-1"><a class="header" href="#key-implementation-features-1">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Memory Management</strong>:</p>
<ul>
<li>Clean shared memory allocation with tensor builder</li>
<li>Type-safe operations with LayoutTensor</li>
<li>Automatic bounds checking</li>
<li>Layout-aware indexing</li>
</ul>
</li>
<li>
<p><strong>Thread Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> after initial multiplication</li>
<li><code>barrier()</code> between reduction steps</li>
<li>Safe thread coordination</li>
</ul>
</li>
<li>
<p><strong>Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; stride:
        shared[local_i] += shared[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
</li>
<li>
<p><strong>Performance Benefits</strong>:</p>
<ul>
<li>\(O(\log n)\) time complexity</li>
<li>Coalesced memory access</li>
<li>Minimal thread divergence</li>
<li>Efficient shared memory usage</li>
</ul>
</li>
</ol>
<p>The LayoutTensor version maintains the same efficient parallel reduction while providing:</p>
<ul>
<li>Better type safety</li>
<li>Cleaner memory management</li>
<li>Layout awareness</li>
<li>Natural indexing syntax</li>
</ul>
<h3 id="barrier-synchronization-importance-1"><a class="header" href="#barrier-synchronization-importance-1">Barrier Synchronization Importance</a></h3>
<p>The <code>barrier()</code> between reduction steps is critical for correctness. Here‚Äôs why:</p>
<p>Without <code>barrier()</code>, race conditions occur:</p>
<pre><code class="language-text">Initial shared memory: [0 1 4 9 16 25 36 49]

Step 1 (stride = 4):
Thread 0 reads: shared[0] = 0, shared[4] = 16
Thread 1 reads: shared[1] = 1, shared[5] = 25
Thread 2 reads: shared[2] = 4, shared[6] = 36
Thread 3 reads: shared[3] = 9, shared[7] = 49

Without barrier:
- Thread 0 writes: shared[0] = 0 + 16 = 16
- Thread 1 starts next step (stride = 2) before Thread 0 finishes
  and reads old value shared[0] = 0 instead of 16!
</code></pre>
<p>With <code>barrier()</code>:</p>
<pre><code class="language-text">Step 1 (stride = 4):
All threads write their sums:
[16 26 40 58 16 25 36 49]
barrier() ensures ALL threads see these values

Step 2 (stride = 2):
Now threads safely read the updated values:
Thread 0: shared[0] = 16 + 40 = 56
Thread 1: shared[1] = 26 + 58 = 84
</code></pre>
<p>The <code>barrier()</code> ensures:</p>
<ol>
<li>All writes from current step complete</li>
<li>All threads see updated values</li>
<li>No thread starts next iteration early</li>
<li>Consistent shared memory state</li>
</ol>
<p>Without these synchronization points, we could get:</p>
<ul>
<li>Memory race conditions</li>
<li>Threads reading stale values</li>
<li>Non-deterministic results</li>
<li>Incorrect final sum</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-11-1d-convolution"><a class="header" href="#puzzle-11-1d-convolution">Puzzle 11: 1D Convolution</a></h1>
<blockquote>
<h2 id="moving-to-layouttensor"><a class="header" href="#moving-to-layouttensor">Moving to LayoutTensor</a></h2>
<p>So far in our GPU puzzle journey, we‚Äôve been exploring two parallel approaches to GPU memory management:</p>
<ol>
<li>Raw memory management with direct pointer manipulation using <a href="https://docs.modular.com/mojo/stdlib/memory/unsafe_pointer/UnsafePointer/">UnsafePointer</a></li>
<li>The more structured <a href="https://docs.modular.com/mojo/stdlib/layout/layout_tensor/LayoutTensor/">LayoutTensor</a> and its related abstractions such as <a href="https://docs.modular.com/mojo/stdlib/layout/tensor_builder/LayoutTensorBuild/">LayoutTensorBuild</a></li>
</ol>
<p>Starting from this puzzle, we‚Äôre transitioning exclusively to using <code>LayoutTensor</code>. This abstraction provides several benefits:</p>
<ul>
<li>Type-safe memory access patterns</li>
<li>Clear representation of data layouts</li>
<li>Better code maintainability</li>
<li>Reduced chance of memory-related bugs</li>
<li>More expressive code that better represents the underlying computations</li>
<li>A lot more ‚Ä¶ that we‚Äôll uncover gradually!</li>
</ul>
<p>This transition aligns with best practices in modern GPU programming in Mojo üî•, where higher-level abstractions help manage complexity without sacrificing performance.</p>
</blockquote>
<h2 id="overview-22"><a class="header" href="#overview-22">Overview</a></h2>
<p>In signal processing and image analysis, convolution is a fundamental operation that combines two sequences to produce a third sequence. This puzzle challenges you to implement a 1D convolution on the GPU, where each output element is computed by sliding a kernel over an input array.</p>
<p>Implement a kernel that computes a 1D convolution between vector <code>a</code> and vector <code>b</code> and stores it in <code>out</code> using the <code>LayoutTensor</code> abstraction.</p>
<p><strong>Note:</strong> <em>You need to handle the general case. You only need 2 global reads and 1 global write per thread.</em></p>
<p><img src="puzzle_11/./media/videos/720p30/puzzle_11_viz.gif" alt="1D Convolution" /></p>
<p>For those new to convolution, think of it as a weighted sliding window operation. At each position, we multiply the kernel values with the corresponding input values and sum the results. In mathematical notation, this is often written as:</p>
<p>\[\Large out[i] = \sum_{j=0}^{\text{CONV}-1} a[i+j] \cdot b[j] \]</p>
<p>In pseudocode, 1D convolution is:</p>
<pre><code class="language-python">for i in range(SIZE):
    for j in range(CONV):
        if i + j &lt; SIZE:
            ret[i] += a_host[i + j] * b_host[j]
</code></pre>
<p>This puzzle is split into two parts to help you build understanding progressively:</p>
<ul>
<li>
<p><a href="puzzle_11/./simple.html">Simple Version with Single Block</a>
Start here to learn the basics of implementing convolution with shared memory in a single block using LayoutTensor.</p>
</li>
<li>
<p><a href="puzzle_11/./complete.html">Block Boundary Version</a>
Then tackle the more challenging case where data needs to be shared across block boundaries, leveraging LayoutTensor‚Äôs capabilities.</p>
</li>
</ul>
<p>Each version presents unique challenges in terms of memory access patterns and thread coordination. The simple version helps you understand the basic convolution operation, while the complete version tests your ability to handle more complex scenarios that arise in real-world GPU programming.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-case-with-single-block"><a class="header" href="#simple-case-with-single-block">Simple Case with Single Block</a></h1>
<p>Implement a kernel that computes a 1D convolution between 1D LayoutTensor <code>a</code> and 1D LayoutTensor <code>b</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You need to handle the general case. You only need 2 global reads and 1 global write per thread.</em></p>
<h2 id="key-concepts-20"><a class="header" href="#key-concepts-20">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Implementing sliding window operations on GPUs</li>
<li>Managing data dependencies across threads</li>
<li>Using shared memory for overlapping regions</li>
</ul>
<p>The key insight is understanding how to efficiently access overlapping elements while maintaining correct boundary conditions.</p>
<h2 id="configuration-8"><a class="header" href="#configuration-8">Configuration</a></h2>
<ul>
<li>Input array size: <code>SIZE = 6</code> elements</li>
<li>Kernel size: <code>CONV = 3</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Shared memory: Two arrays of size <code>SIZE</code> and <code>CONV</code></li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Data loading</strong>: Each thread loads one element from input and kernel</li>
<li><strong>Memory pattern</strong>: Shared arrays for input and convolution kernel</li>
<li><strong>Thread sync</strong>: Coordination before computation</li>
</ul>
<h2 id="code-to-complete-16"><a class="header" href="#code-to-complete-16">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 6
alias CONV = 3
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias in_layout = Layout.row_major(SIZE)
alias out_layout = Layout.row_major(SIZE)
alias conv_layout = Layout.row_major(CONV)


fn conv_1d_simple[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 14 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p11/p11.mojo" class="filename">View full file: problems/p11/p11.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>tb[dtype]().row_major[SIZE]().shared().alloc()</code> for shared memory allocation</li>
<li>Load input to <code>shared_a[local_i]</code> and kernel to <code>shared_b[local_i]</code></li>
<li>Call <code>barrier()</code> after loading</li>
<li>Sum products within bounds: <code>if local_i + j &lt; SIZE</code></li>
<li>Write result if <code>global_i &lt; a_size</code></li>
</ol>
</div>
</details>
<h3 id="running-the-code-17"><a class="header" href="#running-the-code-17">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p11 --simple
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([5.0, 8.0, 11.0, 14.0, 5.0, 0.0])
</code></pre>
<h2 id="solution-16"><a class="header" href="#solution-16">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn conv_1d_simple[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared_a = tb[dtype]().row_major[SIZE]().shared().alloc()
    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()
    if global_i &lt; SIZE:
        shared_a[local_i] = a[global_i]

    if global_i &lt; CONV:
        shared_b[local_i] = b[global_i]

    barrier()

    # Note: this is unsafe as it enforces no guard so could access `shared_a` beyond its bounds
    # local_sum = Scalar[dtype](0)
    # for j in range(CONV):
    #     if local_i + j &lt; SIZE:
    #         local_sum += shared_a[local_i + j] * shared_b[j]

    # if global_i &lt; SIZE:
    #     out[global_i] = local_sum

    # Safe and correct:
    if global_i &lt; SIZE:
        # Note: using `var` allows us to include the type in the type inference
        # `out.element_type` is available in LayoutTensor
        var local_sum: out.element_type = 0

        # Note: `@parameter` decorator unrolls the loop at compile time given `CONV` is a compile-time constant
        # See: https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-for-statement
        @parameter
        for j in range(CONV):
            # Bonus: do we need this check for this specific example with fixed SIZE, CONV
            if local_i + j &lt; SIZE:
                local_sum += shared_a[local_i + j] * shared_b[j]

        out[global_i] = local_sum


</code></pre>
<div class="solution-explanation">
<p>The solution implements a 1D convolution using shared memory for efficient access to overlapping elements. Here‚Äôs a detailed breakdown:</p>
<h3 id="memory-layout"><a class="header" href="#memory-layout">Memory Layout</a></h3>
<pre><code class="language-txt">Input array a:   [0  1  2  3  4  5]
Kernel b:        [0  1  2]
</code></pre>
<h3 id="computation-steps"><a class="header" href="#computation-steps">Computation Steps</a></h3>
<ol>
<li>
<p><strong>Data Loading</strong>:</p>
<pre><code class="language-txt">shared_a: [0  1  2  3  4  5]  // Input array
shared_b: [0  1  2]           // Convolution kernel
</code></pre>
</li>
<li>
<p><strong>Convolution Process</strong> for each position i:</p>
<pre><code class="language-txt">out[0] = a[0]*b[0] + a[1]*b[1] + a[2]*b[2] = 0*0 + 1*1 + 2*2 = 5
out[1] = a[1]*b[0] + a[2]*b[1] + a[3]*b[2] = 1*0 + 2*1 + 3*2 = 8
out[2] = a[2]*b[0] + a[3]*b[1] + a[4]*b[2] = 2*0 + 3*1 + 4*2 = 11
out[3] = a[3]*b[0] + a[4]*b[1] + a[5]*b[2] = 3*0 + 4*1 + 5*2 = 14
out[4] = a[4]*b[0] + a[5]*b[1] + 0*b[2]    = 4*0 + 5*1 + 0*2 = 5
out[5] = a[5]*b[0] + 0*b[1]   + 0*b[2]     = 5*0 + 0*1 + 0*2 = 0
</code></pre>
</li>
</ol>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<ol>
<li>
<p><strong>Memory Safety Considerations</strong>:</p>
<ul>
<li>
<p>The naive approach without proper bounds checking could be unsafe:</p>
<pre><code class="language-mojo"># Unsafe version - could access shared_a beyond its bounds
local_sum = Scalar[dtype](0)
for j in range(CONV):
    if local_i + j &lt; SIZE:
        local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
</li>
<li>
<p>The safe and correct implementation:</p>
<pre><code class="language-mojo">if global_i &lt; a_size:
    var local_sum: out.element_type = 0  # Using var allows type inference
    @parameter  # Unrolls loop at compile time since CONV is constant
    for j in range(CONV):
        if local_i + j &lt; SIZE:
            local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Key Implementation Features</strong>:</p>
<ul>
<li>Uses <code>var</code> for proper type inference with <code>out.element_type</code></li>
<li>Employs <code>@parameter</code> decorator to unroll the convolution loop at compile time</li>
<li>Maintains strict bounds checking for memory safety</li>
<li>Leverages LayoutTensor‚Äôs type system for better code safety</li>
</ul>
</li>
<li>
<p><strong>Memory Management</strong>:</p>
<ul>
<li>Uses shared memory for both input array and kernel</li>
<li>Single load per thread from global memory</li>
<li>Efficient reuse of loaded data</li>
</ul>
</li>
<li>
<p><strong>Thread Coordination</strong>:</p>
<ul>
<li><code>barrier()</code> ensures all data is loaded before computation</li>
<li>Each thread computes one output element</li>
<li>Maintains coalesced memory access pattern</li>
</ul>
</li>
<li>
<p><strong>Performance Optimizations</strong>:</p>
<ul>
<li>Minimizes global memory access</li>
<li>Uses shared memory for fast data access</li>
<li>Avoids thread divergence in main computation loop</li>
<li>Loop unrolling through <code>@parameter</code> decorator</li>
</ul>
</li>
</ol>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block-boundary-version"><a class="header" href="#block-boundary-version">Block Boundary Version</a></h1>
<p>Implement a kernel that computes a 1D convolution between 1D LayoutTensor <code>a</code> and 1D LayoutTensor <code>b</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>You need to handle the general case. You only need 2 global reads and 1 global write per thread.</em></p>
<h2 id="configuration-9"><a class="header" href="#configuration-9">Configuration</a></h2>
<ul>
<li>Input array size: <code>SIZE_2 = 15</code> elements</li>
<li>Kernel size: <code>CONV_2 = 4</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB + CONV_2 - 1</code> elements for input</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Extended loading</strong>: Account for boundary overlap</li>
<li><strong>Block edges</strong>: Handle data across block boundaries</li>
<li><strong>Memory layout</strong>: Efficient shared memory usage</li>
<li><strong>Synchronization</strong>: Proper thread coordination</li>
</ul>
<h2 id="code-to-complete-17"><a class="header" href="#code-to-complete-17">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE_2 = 15
alias CONV_2 = 4
alias BLOCKS_PER_GRID_2 = (2, 1)
alias THREADS_PER_BLOCK_2 = (TPB, 1)


fn conv_1d_block_boundary[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 18 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p11/p11.mojo" class="filename">View full file: problems/p11/p11.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()</code> for shared memory</li>
<li>Load main data: <code>shared_a[local_i] = a[global_i]</code></li>
<li>Load boundary: <code>if local_i &lt; CONV_2 - 1</code> handle next block data</li>
<li>Load kernel: <code>shared_b[local_i] = b[local_i]</code></li>
<li>Sum within extended bounds: <code>if local_i + j &lt; TPB + CONV_2 - 1</code></li>
</ol>
</div>
</details>
<h3 id="running-the-code-18"><a class="header" href="#running-the-code-18">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p11 --block-boundary
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([14.0, 20.0, 26.0, 32.0, 38.0, 44.0, 50.0, 56.0, 62.0, 68.0, 74.0, 80.0, 41.0, 14.0, 0.0])
</code></pre>
<h2 id="solution-17"><a class="header" href="#solution-17">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn conv_1d_block_boundary[
    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    b: LayoutTensor[mut=False, dtype, in_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # first: need to account for padding
    shared_a = tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()
    shared_b = tb[dtype]().row_major[CONV_2]().shared().alloc()
    if global_i &lt; SIZE_2:
        shared_a[local_i] = a[global_i]

    # second: load elements needed for convolution at block boundary
    if local_i &lt; CONV_2 - 1:
        # indices from next block
        next_idx = global_i + TPB
        if next_idx &lt; SIZE_2:
            shared_a[TPB + local_i] = a[next_idx]

    if local_i &lt; CONV_2:
        shared_b[local_i] = b[local_i]

    barrier()

    if global_i &lt; SIZE_2:
        var local_sum: out.element_type = 0

        @parameter
        for j in range(CONV_2):
            if local_i + j &lt; TPB + CONV_2 - 1:
                local_sum += shared_a[local_i + j] * shared_b[j]

        out[global_i] = local_sum


</code></pre>
<div class="solution-explanation">
<p>The solution handles block boundary cases in 1D convolution using extended shared memory. Here‚Äôs a detailed analysis:</p>
<h3 id="memory-layout-and-sizing"><a class="header" href="#memory-layout-and-sizing">Memory layout and sizing</a></h3>
<pre><code class="language-txt">Test Configuration:
- Full array size: SIZE_2 = 15 elements
- Grid: 2 blocks √ó 8 threads
- Convolution kernel: CONV_2 = 4 elements

Block 0 shared memory:  [0 1 2 3 4 5 6 7|8 9 10]  // TPB(8) + (CONV_2-1)(3) padding
Block 1 shared memory:  [8 9 10 11 12 13 14|0 0]  // Second block with padding

Size calculation:
- Main data: TPB elements (8)
- Overlap: CONV_2 - 1 elements (4 - 1 = 3)
- Total: TPB + CONV_2 - 1 = 8 + 4 - 1 = 11 elements
</code></pre>
<h3 id="implementation-details-1"><a class="header" href="#implementation-details-1">Implementation details</a></h3>
<ol>
<li>
<p><strong>Shared Memory Allocation</strong>:</p>
<pre><code class="language-mojo"># First: account for padding needed for convolution window
shared_a = tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()
shared_b = tb[dtype]().row_major[CONV_2]().shared().alloc()
</code></pre>
<p>This allocation pattern ensures we have enough space for both the block‚Äôs data and the overlap region.</p>
</li>
<li>
<p><strong>Data Loading Strategy</strong>:</p>
<pre><code class="language-mojo"># Main block data
if global_i &lt; a_size:
    shared_a[local_i] = a[global_i]

# Boundary data from next block
if local_i &lt; CONV_2 - 1:
    next_idx = global_i + TPB
    if next_idx &lt; a_size:
        shared_a[TPB + local_i] = a[next_idx]
</code></pre>
<ul>
<li>Only threads with <code>local_i &lt; CONV_2 - 1</code> load boundary data</li>
<li>Prevents unnecessary thread divergence</li>
<li>Maintains memory coalescing for main data load</li>
</ul>
</li>
<li>
<p><strong>Kernel Loading</strong>:</p>
<pre><code class="language-mojo">if local_i &lt; b_size:
    shared_b[local_i] = b[local_i]
</code></pre>
<ul>
<li>Single load per thread</li>
<li>Bounded by kernel size</li>
</ul>
</li>
<li>
<p><strong>Convolution Computation</strong>:</p>
<pre><code class="language-mojo">if global_i &lt; a_size:
    var local_sum: out.element_type = 0
    @parameter
    for j in range(CONV_2):
        if local_i + j &lt; TPB + CONV_2 - 1:
            local_sum += shared_a[local_i + j] * shared_b[j]
</code></pre>
<ul>
<li>Uses <code>@parameter</code> for compile-time loop unrolling</li>
<li>Proper type inference with <code>out.element_type</code></li>
<li>Extended bounds check for overlap region</li>
</ul>
</li>
</ol>
<h3 id="memory-access-pattern-analysis"><a class="header" href="#memory-access-pattern-analysis">Memory access pattern analysis</a></h3>
<ol>
<li>
<p><strong>Block 0 Access Pattern</strong>:</p>
<pre><code class="language-txt">Thread 0: [0 1 2 3] √ó [0 1 2 3]
Thread 1: [1 2 3 4] √ó [0 1 2 3]
Thread 2: [2 3 4 5] √ó [0 1 2 3]
...
Thread 7: [7 8 9 10] √ó [0 1 2 3]  // Uses overlap data
</code></pre>
</li>
<li>
<p><strong>Block 1 Access Pattern</strong>:</p>
<pre><code class="language-txt">Thread 0: [8 9 10 11] √ó [0 1 2 3]
Thread 1: [9 10 11 12] √ó [0 1 2 3]
...
Thread 7: [14 0 0 0] √ó [0 1 2 3]  // Zero padding at end
</code></pre>
</li>
</ol>
<h3 id="performance-optimizations"><a class="header" href="#performance-optimizations">Performance optimizations</a></h3>
<ol>
<li>
<p><strong>Memory Coalescing</strong>:</p>
<ul>
<li>Main data load: Consecutive threads access consecutive memory</li>
<li>Boundary data: Only necessary threads participate</li>
<li>Single barrier synchronization point</li>
</ul>
</li>
<li>
<p><strong>Thread Divergence Minimization</strong>:</p>
<ul>
<li>Clean separation of main and boundary loading</li>
<li>Uniform computation pattern within warps</li>
<li>Efficient bounds checking</li>
</ul>
</li>
<li>
<p><strong>Shared Memory Usage</strong>:</p>
<ul>
<li>Optimal sizing to handle block boundaries</li>
<li>No bank conflicts in access pattern</li>
<li>Efficient reuse of loaded data</li>
</ul>
</li>
<li>
<p><strong>Boundary Handling</strong>:</p>
<ul>
<li>Implicit zero padding at array end</li>
<li>Seamless block transition</li>
<li>Proper handling of edge cases</li>
</ul>
</li>
</ol>
<p>This implementation achieves efficient cross-block convolution while maintaining:</p>
<ul>
<li>Memory safety through proper bounds checking</li>
<li>High performance through optimized memory access</li>
<li>Clean code structure using LayoutTensor abstractions</li>
<li>Minimal synchronization overhead</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-12-prefix-sum"><a class="header" href="#puzzle-12-prefix-sum">Puzzle 12: Prefix Sum</a></h1>
<h2 id="overview-23"><a class="header" href="#overview-23">Overview</a></h2>
<p>Prefix sum (also known as <em>scan</em>) is a fundamental parallel algorithm that computes running totals of a sequence. Found at the heart of many parallel applications - from sorting algorithms to scientific simulations - it transforms a sequence of numbers into their running totals. While simple to compute sequentially, making this efficient on a GPU requires clever parallel thinking!</p>
<p>Implement a kernel that computes a prefix-sum over 1D LayoutTensor <code>a</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>If the size of <code>a</code> is greater than the block size, only store the sum of each block.</em></p>
<p><img src="puzzle_12/./media/videos/720p30/puzzle_12_viz.gif" alt="Prefix sum" /></p>
<h2 id="key-concepts-21"><a class="header" href="#key-concepts-21">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Parallel algorithms with logarithmic complexity</li>
<li>Shared memory coordination patterns</li>
<li>Multi-phase computation strategies</li>
</ul>
<p>The key insight is understanding how to transform a sequential operation into an efficient parallel algorithm using shared memory.</p>
<p>For example, given an input sequence \([3, 1, 4, 1, 5, 9]\), the prefix sum would produce:</p>
<ul>
<li>\([3]\) (just the first element)</li>
<li>\([3, 4]\) (3 + 1)</li>
<li>\([3, 4, 8]\) (previous sum + 4)</li>
<li>\([3, 4, 8, 9]\) (previous sum + 1)</li>
<li>\([3, 4, 8, 9, 14]\) (previous sum + 5)</li>
<li>\([3, 4, 8, 9, 14, 23]\) (previous sum + 9)</li>
</ul>
<p>Mathematically, for a sequence \([x_0, x_1, ‚Ä¶, x_n]\), the prefix sum produces:
\[ [x_0, x_0+x_1, x_0+x_1+x_2, ‚Ä¶, \sum_{i=0}^n x_i] \]</p>
<p>While a sequential algorithm would need \(O(n)\) steps, our parallel approach will use a clever two-phase algorithm that completes in \(O(\log n)\) steps! Here‚Äôs a visualization of this process:</p>
<p>This puzzle is split into two parts to help you master the concept:</p>
<ul>
<li>
<p><a href="puzzle_12/./simple.html">Simple Version</a>
Start with a single block implementation where all data fits in shared memory. This helps understand the core parallel algorithm.</p>
</li>
<li>
<p><a href="puzzle_12/./complete.html">Complete Version</a>
Then tackle the more challenging case of handling larger arrays that span multiple blocks, requiring coordination between blocks.</p>
</li>
</ul>
<p>Each version builds on the previous one, helping you develop a deep understanding of parallel prefix sum computation. The simple version establishes the fundamental algorithm, while the complete version shows how to scale it to larger datasets - a common requirement in real-world GPU applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-version"><a class="header" href="#simple-version">Simple Version</a></h1>
<p>Implement a kernel that computes a prefix-sum over 1D LayoutTensor <code>a</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>If the size of <code>a</code> is greater than the block size, only store the sum of each block.</em></p>
<h2 id="configuration-10"><a class="header" href="#configuration-10">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE = 8</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 1</li>
<li>Shared memory: <code>TPB</code> elements</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Data loading</strong>: Each thread loads one element using LayoutTensor access</li>
<li><strong>Memory pattern</strong>: Shared memory for intermediate results using <code>LayoutTensorBuild</code></li>
<li><strong>Thread sync</strong>: Coordination between computation phases</li>
<li><strong>Access pattern</strong>: Stride-based parallel computation</li>
<li><strong>Type safety</strong>: Leveraging LayoutTensor‚Äôs type system</li>
</ul>
<h2 id="code-to-complete-18"><a class="header" href="#code-to-complete-18">Code to complete</a></h2>
<pre><code class="language-mojo">alias TPB = 8
alias SIZE = 8
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE)


fn prefix_sum_simple[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 12 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p12/p12.mojo" class="filename">View full file: problems/p12/p12.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load data into <code>shared[local_i]</code></li>
<li>Use <code>offset = 1</code> and double it each step</li>
<li>Add elements where <code>local_i &gt;= offset</code></li>
<li>Call <code>barrier()</code> between steps</li>
</ol>
</div>
</details>
<h3 id="running-the-code-19"><a class="header" href="#running-the-code-19">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p12 --simple
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: DeviceBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])
</code></pre>
<h2 id="solution-18"><a class="header" href="#solution-18">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn prefix_sum_simple[
    layout: Layout
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared = tb[dtype]().row_major[TPB]().shared().alloc()
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    offset = 1
    for i in range(Int(log2(Scalar[dtype](TPB)))):
        if local_i &gt;= offset and local_i &lt; size:
            shared[local_i] += shared[local_i - offset]

        barrier()
        offset *= 2

    if global_i &lt; size:
        out[global_i] = shared[local_i]


</code></pre>
<div class="solution-explanation">
<p>The parallel (inclusive) prefix-sum algorithm works as follows:</p>
<h3 id="setup--configuration"><a class="header" href="#setup--configuration">Setup &amp; Configuration</a></h3>
<ul>
<li><code>TPB</code> (Threads Per Block) = 8</li>
<li><code>SIZE</code> (Array Size) = 8</li>
</ul>
<h3 id="thread-mapping"><a class="header" href="#thread-mapping">Thread Mapping</a></h3>
<ul>
<li><code>thread_idx.x</code>: \([0, 1, 2, 3, 4, 5, 6, 7]\) (<code>local_i</code>)</li>
<li><code>block_idx.x</code>: \([0, 0, 0, 0, 0, 0, 0, 0]\)</li>
<li><code>global_i</code>: \([0, 1, 2, 3, 4, 5, 6, 7]\) (<code>block_idx.x * TPB + thread_idx.x</code>)</li>
</ul>
<h3 id="initial-load-to-shared-memory"><a class="header" href="#initial-load-to-shared-memory">Initial Load to Shared Memory</a></h3>
<pre><code class="language-txt">Threads:      T‚ÇÄ   T‚ÇÅ   T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
Input array:  [0    1    2    3    4    5    6    7]
shared:       [0    1    2    3    4    5    6    7]
               ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë
              T‚ÇÄ   T‚ÇÅ   T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
</code></pre>
<h3 id="offset--1-first-parallel-step"><a class="header" href="#offset--1-first-parallel-step">Offset = 1: First Parallel Step</a></h3>
<p>Active threads: \(T_1 \ldots T_7\) (where <code>local_i ‚â• 1</code>)</p>
<pre><code class="language-txt">Before:      [0    1    2    3    4    5    6    7]
Add:              +0   +1   +2   +3   +4   +5   +6
                   |    |    |    |    |    |    |
Result:      [0    1    3    6    7    9    11   13]
                   ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë
                  T‚ÇÅ   T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
</code></pre>
<h3 id="offset--2-second-parallel-step"><a class="header" href="#offset--2-second-parallel-step">Offset = 2: Second Parallel Step</a></h3>
<p>Active threads: \(T_2 \ldots T_7\) (where <code>local_i ‚â• 2</code>)</p>
<pre><code class="language-txt">Before:      [0    1    3    6    7    9    11   13]
Add:                   +0   +1   +3   +6   +7   +9
                        |    |    |    |    |    |
Result:      [0    1    3    7    10   15   18   22]
                        ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë
                       T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
</code></pre>
<h3 id="offset--4-third-parallel-step"><a class="header" href="#offset--4-third-parallel-step">Offset = 4: Third Parallel Step</a></h3>
<p>Active threads: \(T_4 \ldots T_7\) (where <code>local_i ‚â• 4</code>)</p>
<pre><code class="language-txt">Before:      [0    1    3    7    10   15   18   22]
Add:                              +0   +1   +3   +7
                                  |    |    |    |
Result:      [0    1    3    7    10   16   21   28]
                                  ‚Üë    ‚Üë    ‚Üë    ‚Üë
                                  T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
</code></pre>
<h3 id="final-write-to-output"><a class="header" href="#final-write-to-output">Final Write to Output</a></h3>
<pre><code class="language-txt">Threads:      T‚ÇÄ   T‚ÇÅ   T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
global_i:     0    1    2    3    4    5    6    7
out[]:       [0    1    3    7    10   16   21   28]
              ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë    ‚Üë
              T‚ÇÄ   T‚ÇÅ   T‚ÇÇ   T‚ÇÉ   T‚ÇÑ   T‚ÇÖ   T‚ÇÜ   T‚Çá
</code></pre>
<h3 id="thread-by-thread-execution"><a class="header" href="#thread-by-thread-execution">Thread-by-Thread Execution</a></h3>
<p><strong>\(T_0\) (<code>local_i=0</code>):</strong></p>
<ul>
<li>Loads <code>shared[0] = 0</code></li>
<li>Never adds (<code>local_i &lt; offset</code> always)</li>
<li>Writes <code>out[0] = 0</code></li>
</ul>
<p><strong>\(T_1\) (<code>local_i=1</code>):</strong></p>
<ul>
<li>Loads <code>shared[1] = 1</code></li>
<li><code>offset=1</code>: adds <code>shared[0]</code> ‚Üí 1</li>
<li><code>offset=2,4</code>: no action (<code>local_i &lt; offset</code>)</li>
<li>Writes <code>out[1] = 1</code></li>
</ul>
<p><strong>\(T_2\) (<code>local_i=2</code>):</strong></p>
<ul>
<li>Loads <code>shared[2] = 2</code></li>
<li><code>offset=1</code>: adds <code>shared[1]</code> ‚Üí 3</li>
<li><code>offset=2</code>: adds <code>shared[0]</code> ‚Üí 3</li>
<li><code>offset=4</code>: no action</li>
<li>Writes <code>out[2] = 3</code></li>
</ul>
<p><strong>\(T_3\) (<code>local_i=3</code>):</strong></p>
<ul>
<li>Loads <code>shared[3] = 3</code></li>
<li><code>offset=1</code>: adds <code>shared[2]</code> ‚Üí 6</li>
<li><code>offset=2</code>: adds <code>shared[1]</code> ‚Üí 7</li>
<li><code>offset=4</code>: no action</li>
<li>Writes <code>out[3] = 7</code></li>
</ul>
<p><strong>\(T_4\) (<code>local_i=4</code>):</strong></p>
<ul>
<li>Loads <code>shared[4] = 4</code></li>
<li><code>offset=1</code>: adds <code>shared[3]</code> ‚Üí 7</li>
<li><code>offset=2</code>: adds <code>shared[2]</code> ‚Üí 10</li>
<li><code>offset=4</code>: adds <code>shared[0]</code> ‚Üí 10</li>
<li>Writes <code>out[4] = 10</code></li>
</ul>
<p><strong>\(T_5\) (<code>local_i=5</code>):</strong></p>
<ul>
<li>Loads <code>shared[5] = 5</code></li>
<li><code>offset=1</code>: adds <code>shared[4]</code> ‚Üí 9</li>
<li><code>offset=2</code>: adds <code>shared[3]</code> ‚Üí 15</li>
<li><code>offset=4</code>: adds <code>shared[1]</code> ‚Üí 16</li>
<li>Writes <code>out[5] = 16</code></li>
</ul>
<p><strong>\(T_6\) (<code>local_i=6</code>):</strong></p>
<ul>
<li>Loads <code>shared[6] = 6</code></li>
<li><code>offset=1</code>: adds <code>shared[5]</code> ‚Üí 11</li>
<li><code>offset=2</code>: adds <code>shared[4]</code> ‚Üí 18</li>
<li><code>offset=4</code>: adds <code>shared[2]</code> ‚Üí 21</li>
<li>Writes <code>out[6] = 21</code></li>
</ul>
<p><strong>\(T_7\) (<code>local_i=7</code>):</strong></p>
<ul>
<li>Loads <code>shared[7] = 7</code></li>
<li><code>offset=1</code>: adds <code>shared[6]</code> ‚Üí 13</li>
<li><code>offset=2</code>: adds <code>shared[5]</code> ‚Üí 22</li>
<li><code>offset=4</code>: adds <code>shared[3]</code> ‚Üí 28</li>
<li>Writes <code>out[7] = 28</code></li>
</ul>
<p>The solution ensures correct synchronization between phases using <code>barrier()</code> and handles array bounds checking with <code>if global_i &lt; size</code>. The final result produces the inclusive prefix sum where each element \(i\) contains \(\sum_{j=0}^{i} a[j]\).</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="complete-version"><a class="header" href="#complete-version">Complete Version</a></h1>
<p>Implement a kernel that computes a prefix-sum over 1D LayoutTensor <code>a</code> and stores it in 1D LayoutTensor <code>out</code>.</p>
<p><strong>Note:</strong> <em>If the size of <code>a</code> is greater than the block size, we need to synchronize across multiple blocks to get the correct result.</em></p>
<h2 id="configuration-11"><a class="header" href="#configuration-11">Configuration</a></h2>
<ul>
<li>Array size: <code>SIZE_2 = 15</code> elements</li>
<li>Threads per block: <code>TPB = 8</code></li>
<li>Number of blocks: 2</li>
<li>Shared memory: <code>TPB</code> elements per block</li>
</ul>
<p>Notes:</p>
<ul>
<li><strong>Multiple blocks</strong>: When the input array is larger than one block, we need a multi-phase approach</li>
<li><strong>Block-level sync</strong>: Within a block, use <code>barrier()</code> to synchronize threads</li>
<li><strong>Host-level sync</strong>: Between blocks, use <code>ctx.synchronize()</code> at the host level</li>
<li><strong>Auxiliary storage</strong>: Use extra space to store block sums for cross-block communication</li>
</ul>
<h2 id="code-to-complete-19"><a class="header" href="#code-to-complete-19">Code to complete</a></h2>
<p>You need to complete two separate kernel functions for the multi-block prefix sum:</p>
<ol>
<li><strong>First kernel</strong> (<code>prefix_sum_local_phase</code>): Computes local prefix sums within each block and stores block sums</li>
<li><strong>Second kernel</strong> (<code>prefix_sum_block_sum_phase</code>): Adds previous block sums to elements in subsequent blocks</li>
</ol>
<p>The main function will handle the necessary host-side synchronization between these kernels.</p>
<pre><code class="language-mojo">alias SIZE_2 = 15
alias BLOCKS_PER_GRID_2 = (2, 1)
alias THREADS_PER_BLOCK_2 = (TPB, 1)
alias EXTENDED_SIZE = SIZE_2 + 2  # up to 2 blocks
alias extended_layout = Layout.row_major(EXTENDED_SIZE)


# Kernel 1: Compute local prefix sums and store block sums in out
fn prefix_sum_local_phase[
    out_layout: Layout, in_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
    num_blocks: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # FILL ME IN (roughly 14 lines)


# Kernel 2: Add block sums to their respective blocks
fn prefix_sum_block_sum_phase[
    layout: Layout
](out: LayoutTensor[mut=False, dtype, layout], size: Int):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 3 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p12/p12.mojo" class="filename">View full file: problems/p12/p12.mojo</a></p>
<p>The key to this puzzle is understanding that <a href="https://docs.modular.com/mojo/stdlib/gpu/sync/barrier/">barrier</a> only synchronizes threads within a block, not across blocks. For cross-block synchronization, you need to use host-level synchronization:</p>
<pre><code class="language-mojo">            # Phase 1: Local prefix sums
            ctx.enqueue_function[
                prefix_sum_local_phase[extended_layout, extended_layout]
            ](
                out_tensor,
                a_tensor,
                size,
                num_blocks,
                grid_dim=BLOCKS_PER_GRID_2,
                block_dim=THREADS_PER_BLOCK_2,
            )

            # Wait for all `blocks` to complete with using host `ctx.synchronize()`
            # Note this is in contrast with using `barrier()` in the kernel
            # which is a synchronization point for all threads in the same block and not across blocks.
            ctx.synchronize()

            # Phase 2: Add block sums
            ctx.enqueue_function[prefix_sum_block_sum_phase[extended_layout]](
                out_tensor,
                size,
                grid_dim=BLOCKS_PER_GRID_2,
                block_dim=THREADS_PER_BLOCK_2,
            )
</code></pre>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<h3 id="1-build-on-the-simple-prefix-sum"><a class="header" href="#1-build-on-the-simple-prefix-sum">1. Build on the simple prefix sum</a></h3>
<p>The <a href="puzzle_12/./simple.html">Simple Version</a> shows how to implement a single-block prefix sum. You‚Äôll need to extend that approach to work across multiple blocks:</p>
<pre><code>Simple version (single block): [0,1,2,3,4,5,6,7] ‚Üí [0,1,3,6,10,15,21,28]

Complete version (two blocks):
Block 0: [0,1,2,3,4,5,6,7] ‚Üí [0,1,3,6,10,15,21,28]
Block 1: [8,9,10,11,12,13,14] ‚Üí [8,17,27,38,50,63,77]
</code></pre>
<p>But how do we handle the second block‚Äôs values? They need to include sums from the first block!</p>
<h3 id="2-two-phase-approach"><a class="header" href="#2-two-phase-approach">2. Two-phase approach</a></h3>
<p>The simple prefix sum can‚Äôt synchronize across blocks, so split the work:</p>
<ol>
<li><strong>First phase</strong>: Each block computes its own local prefix sum (just like the simple version)</li>
<li><strong>Second phase</strong>: Blocks incorporate the sums from previous blocks</li>
</ol>
<p>Remember: <code>barrier()</code> only synchronizes threads within one block. You need host-level synchronization between phases.</p>
<h3 id="3-extended-memory-strategy"><a class="header" href="#3-extended-memory-strategy">3. Extended memory strategy</a></h3>
<p>Since blocks can‚Äôt directly communicate, you need somewhere to store block sums:</p>
<ul>
<li>Allocate extra memory at the end of your output buffer</li>
<li>Last thread in each block stores its final sum in this extra space</li>
<li>Subsequent blocks can read these sums and add them to their elements</li>
</ul>
<h3 id="4-key-implementation-insights"><a class="header" href="#4-key-implementation-insights">4. Key implementation insights</a></h3>
<ul>
<li><strong>Different layouts</strong>: Input and output may have different shapes</li>
<li><strong>Boundary handling</strong>: Always check <code>global_i &lt; size</code> for array bounds</li>
<li><strong>Thread role specialization</strong>: Only specific threads (e.g., last thread) should store block sums</li>
<li><strong>Two kernel synchronization</strong>: Use <code>ctx.synchronize()</code> between kernel launches</li>
</ul>
<h3 id="5-debugging-strategy"><a class="header" href="#5-debugging-strategy">5. Debugging Strategy</a></h3>
<p>If you encounter issues, try visualizing the intermediate state after the first phase:</p>
<pre><code>After first phase: [0,1,3,6,10,15,21,28, 8,17,27,38,50,63,77, ???,???]
</code></pre>
<p>Where <code>???</code> should contain your block sums that will be used in the second phase.</p>
</div>
</details>
<h3 id="running-the-code-20"><a class="header" href="#running-the-code-20">Running the code</a></h3>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p12 --complete
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0])
</code></pre>
<h2 id="solution-19"><a class="header" href="#solution-19">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">

# Kernel 1: Compute local prefix sums and store block sums in out
fn prefix_sum_local_phase[
    out_layout: Layout, in_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
    num_blocks: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    shared = tb[dtype]().row_major[TPB]().shared().alloc()

    # Load data into shared memory
    # Example with SIZE_2=15, TPB=8, BLOCKS=2:
    # Block 0 shared mem: [0,1,2,3,4,5,6,7]
    # Block 1 shared mem: [8,9,10,11,12,13,14,0]  (last value padded with 0)
    if global_i &lt; size:
        shared[local_i] = a[global_i]

    barrier()

    # Compute local prefix sum using parallel reduction
    # This uses a tree-based algorithm with log(TPB) iterations
    # Iteration 1 (offset=1):
    #   Block 0: [0,0+1,2+1,3+2,4+3,5+4,6+5,7+6] = [0,1,3,5,7,9,11,13]
    # Iteration 2 (offset=2):
    #   Block 0: [0,1,3+0,5+1,7+3,9+5,11+7,13+9] = [0,1,3,6,10,14,18,22]
    # Iteration 3 (offset=4):
    #   Block 0: [0,1,3,6,10+0,14+1,18+3,22+6] = [0,1,3,6,10,15,21,28]
    # Block 1 follows same pattern to get [8,17,27,38,50,63,77,...]
    offset = 1
    for i in range(Int(log2(Scalar[dtype](TPB)))):
        if local_i &gt;= offset and local_i &lt; TPB:
            shared[local_i] += shared[local_i - offset]
        barrier()
        offset *= 2

    # Write local results to output
    # Block 0 writes: [0,1,3,6,10,15,21,28]
    # Block 1 writes: [8,17,27,38,50,63,77,...]
    if global_i &lt; size:
        out[global_i] = shared[local_i]

    # Store block sums in auxiliary space
    # Block 0: Thread 7 stores 28 at position size+0 (position 15)
    # Block 1: Thread 7 stores 77 at position size+1 (position 16)
    # This gives us: [0,1,3,6,10,15,21,28, 8,17,27,38,50,63,77, 28,77]
    #                                                           ‚Üë  ‚Üë
    #                                                     Block sums here
    if local_i == TPB - 1:
        out[size + block_idx.x] = shared[local_i]


# Kernel 2: Add block sums to their respective blocks
fn prefix_sum_block_sum_phase[
    layout: Layout
](out: LayoutTensor[mut=False, dtype, layout], size: Int):
    global_i = block_dim.x * block_idx.x + thread_idx.x

    # Second pass: add previous block's sum to each element
    # Block 0: No change needed - already correct
    # Block 1: Add Block 0's sum (28) to each element
    #   Before: [8,17,27,38,50,63,77]
    #   After: [36,45,55,66,78,91,105]
    # Final result combines both blocks:
    # [0,1,3,6,10,15,21,28, 36,45,55,66,78,91,105]
    if block_idx.x &gt; 0 and global_i &lt; size:
        prev_block_sum = out[size + block_idx.x - 1]
        out[global_i] += prev_block_sum


</code></pre>
<div class="solution-explanation">
<p>This solution implements a multi-block prefix sum using a two-kernel approach to handle an array that spans multiple thread blocks. Let‚Äôs break down each aspect in detail:</p>
<h2 id="the-challenge-of-cross-block-communication"><a class="header" href="#the-challenge-of-cross-block-communication">The challenge of cross-block communication</a></h2>
<p>The fundamental limitation in GPU programming is that threads can only synchronize within a block using <code>barrier()</code>. When data spans multiple blocks, we face the challenge: <strong>How do we ensure blocks can communicate their partial results to other blocks?</strong></p>
<h3 id="memory-layout-visualization"><a class="header" href="#memory-layout-visualization">Memory layout visualization</a></h3>
<p>For our test case with <code>SIZE_2 = 15</code> and <code>TPB = 8</code>:</p>
<pre><code>Input array:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]

Block 0 processes: [0, 1, 2, 3, 4, 5, 6, 7]
Block 1 processes: [8, 9, 10, 11, 12, 13, 14, (padding)]
</code></pre>
<p>We extend the output buffer to include space for block sums:</p>
<pre><code>Extended buffer: [data values (15 elements)] + [block sums (2 elements)]
                 [0...14] + [block0_sum, block1_sum]
</code></pre>
<p>The size of this extended buffer is: <code>EXTENDED_SIZE = SIZE_2 + num_blocks = 15 + 2 = 17</code></p>
<h2 id="phase-1-kernel-local-prefix-sums"><a class="header" href="#phase-1-kernel-local-prefix-sums">Phase 1 kernel: Local prefix sums</a></h2>
<h3 id="step-by-step-execution-for-block-0"><a class="header" href="#step-by-step-execution-for-block-0">Step-by-step execution for Block 0</a></h3>
<ol>
<li>
<p><strong>Load values into shared memory</strong>:</p>
<pre><code>shared = [0, 1, 2, 3, 4, 5, 6, 7]
</code></pre>
</li>
<li>
<p><strong>Iterations of parallel reduction</strong> (\(\log_2(TPB) = 3\) iterations):</p>
<p><strong>Iteration 1</strong> (offset=1):</p>
<pre><code>shared[0] = 0              (unchanged)
shared[1] = 1 + 0 = 1
shared[2] = 2 + 1 = 3
shared[3] = 3 + 2 = 5
shared[4] = 4 + 3 = 7
shared[5] = 5 + 4 = 9
shared[6] = 6 + 5 = 11
shared[7] = 7 + 6 = 13
</code></pre>
<p>After barrier: <code>shared = [0, 1, 3, 5, 7, 9, 11, 13]</code></p>
<p><strong>Iteration 2</strong> (offset=2):</p>
<pre><code>shared[0] = 0              (unchanged)
shared[1] = 1              (unchanged)
shared[2] = 3 + 0 = 3      (unchanged)
shared[3] = 5 + 1 = 6
shared[4] = 7 + 3 = 10
shared[5] = 9 + 5 = 14
shared[6] = 11 + 7 = 18
shared[7] = 13 + 9 = 22
</code></pre>
<p>After barrier: <code>shared = [0, 1, 3, 6, 10, 14, 18, 22]</code></p>
<p><strong>Iteration 3</strong> (offset=4):</p>
<pre><code>shared[0] = 0              (unchanged)
shared[1] = 1              (unchanged)
shared[2] = 3              (unchanged)
shared[3] = 6              (unchanged)
shared[4] = 10 + 0 = 10    (unchanged)
shared[5] = 14 + 1 = 15
shared[6] = 18 + 3 = 21
shared[7] = 22 + 6 = 28
</code></pre>
<p>After barrier: <code>shared = [0, 1, 3, 6, 10, 15, 21, 28]</code></p>
</li>
<li>
<p><strong>Write local results back to global memory</strong>:</p>
<pre><code>out[0...7] = [0, 1, 3, 6, 10, 15, 21, 28]
</code></pre>
</li>
<li>
<p><strong>Store block sum in auxiliary space</strong> (only last thread):</p>
<pre><code>out[15] = 28  // at position size + block_idx.x = 15 + 0
</code></pre>
</li>
</ol>
<h3 id="step-by-step-execution-for-block-1"><a class="header" href="#step-by-step-execution-for-block-1">Step-by-step execution for Block 1</a></h3>
<ol>
<li>
<p><strong>Load values into shared memory</strong>:</p>
<pre><code>shared = [8, 9, 10, 11, 12, 13, 14, 0] // Last value padded with 0
</code></pre>
</li>
<li>
<p><strong>Iterations of parallel reduction</strong> (\(\log_2(TPB) = 3\) iterations):</p>
<p>With similar iterations as Block 0, after all three iterations:</p>
<pre><code>shared = [8, 17, 27, 38, 50, 63, 77, 77]
</code></pre>
</li>
<li>
<p><strong>Write local results back to global memory</strong>:</p>
<pre><code>out[8...14] = [8, 17, 27, 38, 50, 63, 77]
</code></pre>
</li>
<li>
<p><strong>Store block sum in auxiliary space</strong> (only last thread):</p>
<pre><code>out[16] = 77  // at position size + block_idx.x = 15 + 1
</code></pre>
</li>
</ol>
<p>After Phase 1, the output buffer contains:</p>
<pre><code>[0, 1, 3, 6, 10, 15, 21, 28, 8, 17, 27, 38, 50, 63, 77, 28, 77]
                                                        ^   ^
                                                Block sums stored here
</code></pre>
<h2 id="host-side-synchronization-the-critical-step"><a class="header" href="#host-side-synchronization-the-critical-step">Host-side synchronization: The critical step</a></h2>
<p>Between phases 1 and 2, we call:</p>
<pre><code class="language-mojo">ctx.synchronize()
</code></pre>
<p>This is the most crucial part of the algorithm! Without this synchronization, the second kernel might start before the first one completes, leading to race conditions and incorrect results. This is a fundamental difference from single-block algorithms where <code>barrier()</code> would be sufficient.</p>
<h2 id="phase-2-kernel-block-sum-addition"><a class="header" href="#phase-2-kernel-block-sum-addition">Phase 2 kernel: Block sum addition</a></h2>
<ol>
<li>
<p><strong>Block 0</strong>: No changes needed (it‚Äôs already correct).</p>
</li>
<li>
<p><strong>Block 1</strong>: Each thread adds Block 0‚Äôs sum to its element:</p>
<pre><code>prev_block_sum = out[size + block_idx.x - 1] = out[15] = 28
out[global_i] += prev_block_sum
</code></pre>
<p>Block 1 values are transformed:</p>
<pre><code>Before: [8, 17, 27, 38, 50, 63, 77]
After:  [36, 45, 55, 66, 78, 91, 105]
</code></pre>
</li>
</ol>
<h2 id="performance-and-optimization-considerations"><a class="header" href="#performance-and-optimization-considerations">Performance and optimization considerations</a></h2>
<ol>
<li>
<p><strong>Work efficiency</strong>: This implementation has \(O(n \log n)\) work complexity, while the sequential algorithm is \(O(n)\). This is a classic space-time tradeoff in parallel algorithms.</p>
</li>
<li>
<p><strong>Memory overhead</strong>: The extra space for block sums is minimal (just one element per block).</p>
</li>
</ol>
<p>This two-kernel approach is a fundamental pattern in GPU programming for algorithms that require cross-block communication. The same strategy can be applied to other parallel algorithms like radix sort, histogram calculation, and reduction operations.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-13-axis-sum"><a class="header" href="#puzzle-13-axis-sum">Puzzle 13: Axis Sum</a></h1>
<h2 id="overview-24"><a class="header" href="#overview-24">Overview</a></h2>
<p>Implement a kernel that computes a sum over each row of 2D matrix <code>a</code> and stores it in <code>out</code> using LayoutTensor.</p>
<p><img src="puzzle_13/./media/videos/720p30/puzzle_13_viz.gif" alt="Axis Sum visualization" /></p>
<h2 id="key-concepts-22"><a class="header" href="#key-concepts-22">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Parallel reduction along matrix dimensions using LayoutTensor</li>
<li>Using block coordinates for data partitioning</li>
<li>Efficient shared memory reduction patterns</li>
<li>Working with multi-dimensional tensor layouts</li>
</ul>
<p>The key insight is understanding how to map thread blocks to matrix rows and perform efficient parallel reduction within each block while leveraging LayoutTensor‚Äôs dimensional indexing.</p>
<h2 id="configuration-12"><a class="header" href="#configuration-12">Configuration</a></h2>
<ul>
<li>Matrix dimensions: \(\text{BATCH} \times \text{SIZE} = 4 \times 6\)</li>
<li>Threads per block: \(\text{TPB} = 8\)</li>
<li>Grid dimensions: \(1 \times \text{BATCH}\)</li>
<li>Shared memory: \(\text{TPB}\) elements per block</li>
<li>Input layout: <code>Layout.row_major(BATCH, SIZE)</code></li>
<li>Output layout: <code>Layout.row_major(BATCH, 1)</code></li>
</ul>
<p>Matrix visualization:</p>
<pre><code class="language-txt">Row 0: [0, 1, 2, 3, 4, 5]       ‚Üí Block(0,0)
Row 1: [6, 7, 8, 9, 10, 11]     ‚Üí Block(0,1)
Row 2: [12, 13, 14, 15, 16, 17] ‚Üí Block(0,2)
Row 3: [18, 19, 20, 21, 22, 23] ‚Üí Block(0,3)
</code></pre>
<h2 id="code-to-complete-20"><a class="header" href="#code-to-complete-20">Code to Complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 8
alias BATCH = 4
alias SIZE = 6
alias BLOCKS_PER_GRID = (1, BATCH)
alias THREADS_PER_BLOCK = (TPB, 1)
alias dtype = DType.float32
alias in_layout = Layout.row_major(BATCH, SIZE)
alias out_layout = Layout.row_major(BATCH, 1)


fn axis_sum[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    batch = block_idx.y
    # FILL ME IN (roughly 15 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p13/p13.mojo" class="filename">View full file: problems/p13/p13.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Use <code>batch = block_idx.y</code> to select row</li>
<li>Load elements: <code>cache[local_i] = a[batch * size + local_i]</code></li>
<li>Perform parallel reduction with halving stride</li>
<li>Thread 0 writes final sum to <code>out[batch]</code></li>
</ol>
</div>
</details>
<h2 id="running-the-code-21"><a class="header" href="#running-the-code-21">Running the Code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p13
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: DeviceBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([15.0, 51.0, 87.0, 123.0])
</code></pre>
<h2 id="solution-20"><a class="header" href="#solution-20">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn axis_sum[
    in_layout: Layout, out_layout: Layout
](
    out: LayoutTensor[mut=False, dtype, out_layout],
    a: LayoutTensor[mut=False, dtype, in_layout],
    size: Int,
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    batch = block_idx.y
    cache = tb[dtype]().row_major[TPB]().shared().alloc()

    # Visualize:
    # Block(0,0): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 0: [0,1,2,3,4,5]
    # Block(0,1): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 1: [6,7,8,9,10,11]
    # Block(0,2): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 2: [12,13,14,15,16,17]
    # Block(0,3): [T0,T1,T2,T3,T4,T5,T6,T7] -&gt; Row 3: [18,19,20,21,22,23]

    # each row is handled by each block bc we have grid_dim=(1, BATCH)

    if local_i &lt; size:
        cache[local_i] = a[batch, local_i]
    else:
        # Add zero-initialize padding elements for later reduction
        cache[local_i] = 0

    barrier()

    # do reduction sum per each block
    stride = TPB // 2
    while stride &gt; 0:
        if local_i &lt; stride:
            cache[local_i] += cache[local_i + stride]

        barrier()
        stride //= 2

    # writing with local thread = 0 that has the sum for each batch
    if local_i == 0:
        out[batch, 0] = cache[0]


</code></pre>
<div class="solution-explanation">
<p>The solution implements a parallel row-wise sum reduction for a 2D matrix using LayoutTensor. Here‚Äôs a comprehensive breakdown:</p>
<h3 id="matrix-layout-and-block-mapping"><a class="header" href="#matrix-layout-and-block-mapping">Matrix Layout and Block Mapping</a></h3>
<pre><code class="language-txt">Input Matrix (4√ó6) with LayoutTensor:                Block Assignment:
[[ a[0,0]  a[0,1]  a[0,2]  a[0,3]  a[0,4]  a[0,5] ] ‚Üí Block(0,0)
 [ a[1,0]  a[1,1]  a[1,2]  a[1,3]  a[1,4]  a[1,5] ] ‚Üí Block(0,1)
 [ a[2,0]  a[2,1]  a[2,2]  a[2,3]  a[2,4]  a[2,5] ] ‚Üí Block(0,2)
 [ a[3,0]  a[3,1]  a[3,2]  a[3,3]  a[3,4]  a[3,5] ] ‚Üí Block(0,3)
</code></pre>
<h3 id="parallel-reduction-process"><a class="header" href="#parallel-reduction-process">Parallel Reduction Process</a></h3>
<ol>
<li>
<p><strong>Initial Data Loading</strong>:</p>
<pre><code class="language-txt">Block(0,0): cache = [a[0,0] a[0,1] a[0,2] a[0,3] a[0,4] a[0,5] * *]  // * = padding
Block(0,1): cache = [a[1,0] a[1,1] a[1,2] a[1,3] a[1,4] a[1,5] * *]
Block(0,2): cache = [a[2,0] a[2,1] a[2,2] a[2,3] a[2,4] a[2,5] * *]
Block(0,3): cache = [a[3,0] a[3,1] a[3,2] a[3,3] a[3,4] a[3,5] * *]
</code></pre>
</li>
<li>
<p><strong>Reduction Steps</strong> (for Block 0,0):</p>
<pre><code class="language-txt">Initial:  [0  1  2  3  4  5  *  *]
Stride 4: [4  5  6  7  4  5  *  *]
Stride 2: [10 12 6  7  4  5  *  *]
Stride 1: [15 12 6  7  4  5  *  *]
</code></pre>
</li>
</ol>
<h3 id="key-implementation-features-2"><a class="header" href="#key-implementation-features-2">Key Implementation Features:</a></h3>
<ol>
<li>
<p><strong>Layout Configuration</strong>:</p>
<ul>
<li>Input: row-major layout (BATCH √ó SIZE)</li>
<li>Output: row-major layout (BATCH √ó 1)</li>
<li>Each block processes one complete row</li>
</ul>
</li>
<li>
<p><strong>Memory Access Pattern</strong>:</p>
<ul>
<li>LayoutTensor 2D indexing for input: <code>a[batch, local_i]</code></li>
<li>Shared memory for efficient reduction</li>
<li>LayoutTensor 2D indexing for output: <code>out[batch, 0]</code></li>
</ul>
</li>
<li>
<p><strong>Parallel Reduction Logic</strong>:</p>
<pre><code class="language-mojo">stride = TPB // 2
while stride &gt; 0:
    if local_i &lt; size:
        cache[local_i] += cache[local_i + stride]
    barrier()
    stride //= 2
</code></pre>
</li>
<li>
<p><strong>Output Writing</strong>:</p>
<pre><code class="language-mojo">if local_i == 0:
    out[batch, 0] = cache[0]  --&gt; One result per batch
</code></pre>
</li>
</ol>
<h3 id="performance-optimizations-1"><a class="header" href="#performance-optimizations-1">Performance Optimizations:</a></h3>
<ol>
<li>
<p><strong>Memory Efficiency</strong>:</p>
<ul>
<li>Coalesced memory access through LayoutTensor</li>
<li>Shared memory for fast reduction</li>
<li>Single write per row result</li>
</ul>
</li>
<li>
<p><strong>Thread Utilization</strong>:</p>
<ul>
<li>Perfect load balancing across rows</li>
<li>No thread divergence in main computation</li>
<li>Efficient parallel reduction pattern</li>
</ul>
</li>
<li>
<p><strong>Synchronization</strong>:</p>
<ul>
<li>Minimal barriers (only during reduction)</li>
<li>Independent processing between rows</li>
<li>No inter-block communication needed</li>
</ul>
</li>
</ol>
<h3 id="complexity-analysis"><a class="header" href="#complexity-analysis">Complexity Analysis:</a></h3>
<ul>
<li>Time: \(O(\log n)\) per row, where n is row length</li>
<li>Space: \(O(TPB)\) shared memory per block</li>
<li>Total parallel time: \(O(\log n)\) with sufficient threads</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-14-matrix-multiplication-matmul"><a class="header" href="#puzzle-14-matrix-multiplication-matmul">Puzzle 14: Matrix Multiplication (MatMul)</a></h1>
<h2 id="overview-25"><a class="header" href="#overview-25">Overview</a></h2>
<p>Matrix multiplication is a fundamental operation in scientific computing, machine learning, and graphics. Given two matrices \(A\) and \(B\), we want to compute their product \(C = A \times B.\)</p>
<p>For matrices \(A_{m\times k}\) and \(B_{k\times n}\), each element of the result \(C_{m\times n}\) is computed as:</p>
<p>\[\Large C_{ij} = \sum_{l=0}^{k-1} A_{il} \cdot B_{lj} \]</p>
<p><img src="puzzle_14/./media/videos/720p30/puzzle_14_viz.gif" alt="Matrix Multiply visualization" /></p>
<p>This puzzle explores different approaches to implementing matrix multiplication on GPUs, each with its own performance characteristics:</p>
<ul>
<li>
<p><a href="puzzle_14/./naive.html">Naive Version</a>
The straightforward implementation where each thread computes one element of the output matrix. While simple to understand, this approach makes many redundant memory accesses.</p>
</li>
<li>
<p><a href="puzzle_14/./shared_memory.html">Shared Memory Version</a>
Improves performance by loading blocks of input matrices into fast shared memory, reducing global memory accesses. Each thread still computes one output element but reads from shared memory.</p>
</li>
<li>
<p><a href="puzzle_14/./tiled.html">Tiled Version</a>
Further optimizes by dividing the computation into tiles, allowing threads to cooperate on loading and computing blocks of the output matrix. This approach better utilizes memory hierarchy and thread cooperation.</p>
</li>
</ul>
<p>Each version builds upon the previous one, introducing new optimization techniques common in GPU programming. You‚Äôll learn how different memory access patterns and thread cooperation strategies affect performance.</p>
<p>The progression illustrates a common pattern in GPU optimization:</p>
<ol>
<li>Start with a correct but naive implementation</li>
<li>Reduce global memory access with shared memory</li>
<li>Improve data locality and thread cooperation with tiling</li>
<li>Use high-level abstractions while maintaining performance</li>
</ol>
<p>Choose a version to begin your matrix multiplication journey!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="naive-matrix-multiplication"><a class="header" href="#naive-matrix-multiplication">Naive Matrix Multiplication</a></h1>
<h2 id="overview-26"><a class="header" href="#overview-26">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(B\) and stores the result in \(\text{out}\).
This is the most straightforward implementation where each thread computes one element of the output matrix.</p>
<h2 id="key-concepts-23"><a class="header" href="#key-concepts-23">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>2D thread organization for matrix operations</li>
<li>Global memory access patterns</li>
<li>Matrix indexing in row-major layout</li>
<li>Thread-to-output element mapping</li>
</ul>
<p>The key insight is understanding how to map 2D thread indices to matrix elements and compute dot products in parallel.</p>
<h2 id="configuration-13"><a class="header" href="#configuration-13">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE} \times \text{SIZE} = 2 \times 2\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(1 \times 1\)</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Input B: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Output: <code>Layout.row_major(SIZE, SIZE)</code></li>
</ul>
<h2 id="code-to-complete-21"><a class="header" href="#code-to-complete-21">Code to complete</a></h2>
<pre><code class="language-mojo">from gpu import thread_idx, block_idx, block_dim, barrier
from layout import Layout, LayoutTensor
from layout.tensor_builder import LayoutTensorBuild as tb


alias TPB = 3
alias SIZE = 2
alias BLOCKS_PER_GRID = (1, 1)
alias THREADS_PER_BLOCK = (TPB, TPB)
alias dtype = DType.float32
alias layout = Layout.row_major(SIZE, SIZE)


fn naive_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    # FILL ME IN (roughly 6 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate <code>row</code> and <code>col</code> from thread indices</li>
<li>Check if indices are within <code>size</code></li>
<li>Accumulate products in a local variable</li>
<li>Write final sum to correct output position</li>
</ol>
</div>
</details>
<h2 id="running-the-code-22"><a class="header" href="#running-the-code-22">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --naive
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([4.0, 6.0, 12.0, 22.0])
</code></pre>
<h2 id="solution-21"><a class="header" href="#solution-21">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn naive_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x

    if row &lt; size and col &lt; size:
        var acc: out.element_type = 0

        @parameter
        for k in range(size):
            acc += a[row, k] * b[k, col]

        out[row, col] = acc


</code></pre>
<div class="solution-explanation">
<p>The naive matrix multiplication using LayoutTensor demonstrates the basic approach:</p>
<h3 id="matrix-layout-22-example"><a class="header" href="#matrix-layout-22-example">Matrix Layout (2√ó2 example)</a></h3>
<pre><code class="language-txt">Matrix A:          Matrix B:                   Output C:
[a[0,0] a[0,1]]    [b[0,0] b[0,1]]             [c[0,0] c[0,1]]
[a[1,0] a[1,1]]    [b[1,0] b[1,1]]             [c[1,0] c[1,1]]
</code></pre>
<h3 id="implementation-details-2"><a class="header" href="#implementation-details-2">Implementation Details:</a></h3>
<ol>
<li>
<p><strong>Thread mapping</strong>:</p>
<pre><code class="language-mojo">row = block_dim.y * block_idx.y + thread_idx.y
col = block_dim.x * block_idx.x + thread_idx.x
</code></pre>
</li>
<li>
<p><strong>Memory access pattern</strong>:</p>
<ul>
<li>Direct 2D indexing: <code>a[row, k]</code></li>
<li>Transposed access: <code>b[k, col]</code></li>
<li>Output writing: <code>out[row, col]</code></li>
</ul>
</li>
<li>
<p><strong>Computation flow</strong>:</p>
<pre><code class="language-mojo"># Use var for mutable accumulator with tensor's element type
var acc: out.element_type = 0

# @parameter for compile-time loop unrolling
@parameter
for k in range(size):
    acc += a[row, k] * b[k, col]
</code></pre>
</li>
</ol>
<h3 id="key-language-features"><a class="header" href="#key-language-features">Key language features:</a></h3>
<ol>
<li>
<p><strong>Variable declaration</strong>:</p>
<ul>
<li>The use of <code>var</code> in <code>var acc: out.element_type = 0</code> allows for type inference with <code>out.element_type</code> ensures type compatibility with the output tensor</li>
<li>Initialized to zero before accumulation</li>
</ul>
</li>
<li>
<p><strong>Loop pptimization</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-for-statement"><code>@parameter</code></a> decorator unrolls the loop at compile time</li>
<li>Improves performance for small, known matrix sizes</li>
<li>Enables better instruction scheduling</li>
</ul>
</li>
</ol>
<h3 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance characteristics:</a></h3>
<ol>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Each thread makes <code>2 x SIZE</code> global memory reads</li>
<li>One global memory write per thread</li>
<li>No data reuse between threads</li>
</ul>
</li>
<li>
<p><strong>Computational efficiency</strong>:</p>
<ul>
<li>Simple implementation but suboptimal performance</li>
<li>Many redundant global memory accesses</li>
<li>No use of fast shared memory</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>High global memory bandwidth usage</li>
<li>Poor data locality</li>
<li>Limited scalability for large matrices</li>
</ul>
</li>
</ol>
<p>This naive implementation serves as a baseline for understanding matrix multiplication on GPUs, highlighting the need for optimization in memory access patterns.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-memory-matrix-multiplication"><a class="header" href="#shared-memory-matrix-multiplication">Shared Memory Matrix Multiplication</a></h1>
<h2 id="overview-27"><a class="header" href="#overview-27">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(B\) and stores the result in \(\text{out}\), using shared memory to improve memory access efficiency. This version loads matrix blocks into shared memory before computation.</p>
<h2 id="key-concepts-24"><a class="header" href="#key-concepts-24">Key concepts</a></h2>
<p>In this puzzle, you‚Äôll learn about:</p>
<ul>
<li>Block-local memory management with LayoutTensor</li>
<li>Thread synchronization patterns</li>
<li>Memory access optimization using shared memory</li>
<li>Collaborative data loading with 2D indexing</li>
<li>Efficient use of LayoutTensor for matrix operations</li>
</ul>
<p>The key insight is understanding how to use fast shared memory with LayoutTensor to reduce expensive global memory operations.</p>
<h2 id="configuration-14"><a class="header" href="#configuration-14">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE} \times \text{SIZE} = 2 \times 2\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(1 \times 1\)</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Input B: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Output: <code>Layout.row_major(SIZE, SIZE)</code></li>
<li>Shared Memory: Two <code>TPB √ó TPB</code> LayoutTensors</li>
</ul>
<p>Memory organization:</p>
<pre><code class="language-txt">Global Memory (LayoutTensor):          Shared Memory (LayoutTensor):
A[i,j]: Direct access                  a_shared[local_row, local_col]
B[i,j]: Direct access                  b_shared[local_row, local_col]
</code></pre>
<h2 id="code-to-complete-22"><a class="header" href="#code-to-complete-22">Code to complete</a></h2>
<pre><code class="language-mojo">fn single_block_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    local_row = thread_idx.y
    local_col = thread_idx.x
    # FILL ME IN (roughly 12 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Load matrices to shared memory using global and local indices</li>
<li>Call <code>barrier()</code> after loading</li>
<li>Compute dot product using shared memory indices</li>
<li>Check array bounds for all operations</li>
</ol>
</div>
</details>
<h2 id="running-the-code-23"><a class="header" href="#running-the-code-23">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --single-block
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([4.0, 6.0, 12.0, 22.0])
</code></pre>
<h2 id="solution-22"><a class="header" href="#solution-22">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn single_block_matmul[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    row = block_dim.y * block_idx.y + thread_idx.y
    col = block_dim.x * block_idx.x + thread_idx.x
    local_row = thread_idx.y
    local_col = thread_idx.x

    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    if row &lt; size and col &lt; size:
        a_shared[local_row, local_col] = a[row, col]
        b_shared[local_row, local_col] = b[row, col]

    barrier()

    if row &lt; size and col &lt; size:
        var acc: out.element_type = 0

        @parameter
        for k in range(size):
            acc += a_shared[local_row, k] * b_shared[k, local_col]

        out[row, col] = acc


</code></pre>
<div class="solution-explanation">
<p>The shared memory implementation with LayoutTensor improves performance through efficient memory access patterns:</p>
<h3 id="memory-organization"><a class="header" href="#memory-organization">Memory organization</a></h3>
<pre><code class="language-txt">Input Tensors (2√ó2):                Shared Memory (3√ó3):
Matrix A:                           a_shared:
 [a[0,0] a[0,1]]                     [s[0,0] s[0,1] s[0,2]]
 [a[1,0] a[1,1]]                     [s[1,0] s[1,1] s[1,2]]
                                     [s[2,0] s[2,1] s[2,2]]
Matrix B:                           b_shared: (similar layout)
 [b[0,0] b[0,1]]                     [t[0,0] t[0,1] t[0,2]]
 [b[1,0] b[1,1]]                     [t[1,0] t[1,1] t[1,2]]
                                     [t[2,0] t[2,1] t[2,2]]
</code></pre>
<h3 id="implementation-phases"><a class="header" href="#implementation-phases">Implementation Phases:</a></h3>
<ol>
<li>
<p><strong>Shared Memory Setup</strong>:</p>
<pre><code class="language-mojo"># Create 2D shared memory tensors using TensorBuilder
a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
</code></pre>
</li>
<li>
<p><strong>Thread Indexing</strong>:</p>
<pre><code class="language-mojo"># Global indices for matrix access
row = block_dim.y * block_idx.y + thread_idx.y
col = block_dim.x * block_idx.x + thread_idx.x

# Local indices for shared memory
local_row = thread_idx.y
local_col = thread_idx.x
</code></pre>
</li>
<li>
<p><strong>Data Loading</strong>:</p>
<pre><code class="language-mojo"># Load data into shared memory using LayoutTensor indexing
if row &lt; size and col &lt; size:
    a_shared[local_row, local_col] = a[row, col]
    b_shared[local_row, local_col] = b[row, col]
</code></pre>
</li>
<li>
<p><strong>Computation with Shared Memory</strong>:</p>
<pre><code class="language-mojo"># Guard ensures we only compute for valid matrix elements
if row &lt; size and col &lt; size:
    # Initialize accumulator with output tensor's type
    var acc: out.element_type = 0

    # Compile-time unrolled loop for matrix multiplication
    @parameter
    for k in range(size):
        acc += a_shared[local_row, k] * b_shared[k, local_col]

    # Write result only for threads within matrix bounds
    out[row, col] = acc
</code></pre>
<p>Key aspects:</p>
<ul>
<li>
<p><strong>Boundary check</strong>: <code>if row &lt; size and col &lt; size</code></p>
<ul>
<li>Prevents out-of-bounds computation</li>
<li>Only valid threads perform work</li>
<li>Essential because TPB (3√ó3) &gt; SIZE (2√ó2)</li>
</ul>
</li>
<li>
<p><strong>Accumulator Type</strong>: <code>var acc: out.element_type</code></p>
<ul>
<li>Uses output tensor‚Äôs element type for type safety</li>
<li>Ensures consistent numeric precision</li>
<li>Initialized to zero before accumulation</li>
</ul>
</li>
<li>
<p><strong>Loop Optimization</strong>: <code>@parameter for k in range(size)</code></p>
<ul>
<li>Unrolls the loop at compile time</li>
<li>Enables better instruction scheduling</li>
<li>Efficient for small, known matrix sizes</li>
</ul>
</li>
<li>
<p><strong>Result Writing</strong>: <code>out[row, col] = acc</code></p>
<ul>
<li>Protected by the same guard condition</li>
<li>Only valid threads write results</li>
<li>Maintains matrix bounds safety</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="thread-safety-and-synchronization"><a class="header" href="#thread-safety-and-synchronization">Thread safety and synchronization:</a></h3>
<ol>
<li>
<p><strong>Guard conditions</strong>:</p>
<ul>
<li>Input Loading: <code>if row &lt; size and col &lt; size</code></li>
<li>Computation: Same guard ensures thread safety</li>
<li>Output Writing: Protected by the same condition</li>
<li>Prevents invalid memory access and race conditions</li>
</ul>
</li>
<li>
<p><strong>Memory access safety</strong>:</p>
<ul>
<li>Shared memory: Accessed only within TPB bounds</li>
<li>Global memory: Protected by size checks</li>
<li>Output: Guarded writes prevent corruption</li>
</ul>
</li>
</ol>
<h3 id="key-language-features-1"><a class="header" href="#key-language-features-1">Key language features:</a></h3>
<ol>
<li>
<p><strong>LayoutTensor benefits</strong>:</p>
<ul>
<li>Direct 2D indexing simplifies code</li>
<li>Type safety through <code>element_type</code></li>
<li>Efficient memory layout handling</li>
</ul>
</li>
<li>
<p><strong>Shared memory allocation</strong>:</p>
<ul>
<li>TensorBuilder for structured allocation</li>
<li>Row-major layout matching input tensors</li>
<li>Proper alignment for efficient access</li>
</ul>
</li>
<li>
<p><strong>Synchronization</strong>:</p>
<ul>
<li><code>barrier()</code> ensures shared memory consistency</li>
<li>Proper synchronization between load and compute</li>
<li>Thread cooperation within block</li>
</ul>
</li>
</ol>
<h3 id="performance-optimizations-2"><a class="header" href="#performance-optimizations-2">Performance optimizations:</a></h3>
<ol>
<li>
<p><strong>Memory Access Efficiency</strong>:</p>
<ul>
<li>Single global memory load per element</li>
<li>Multiple reuse through shared memory</li>
<li>Coalesced memory access patterns</li>
</ul>
</li>
<li>
<p><strong>Thread cooperation</strong>:</p>
<ul>
<li>Collaborative data loading</li>
<li>Shared data reuse</li>
<li>Efficient thread synchronization</li>
</ul>
</li>
<li>
<p><strong>Computational benefits</strong>:</p>
<ul>
<li>Reduced global memory traffic</li>
<li>Better cache utilization</li>
<li>Improved instruction throughput</li>
</ul>
</li>
</ol>
<p>This implementation significantly improves performance over the naive version by:</p>
<ul>
<li>Reducing global memory accesses</li>
<li>Enabling data reuse through shared memory</li>
<li>Using efficient 2D indexing with LayoutTensor</li>
<li>Maintaining proper thread synchronization</li>
</ul>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tiled-matrix-multiplication"><a class="header" href="#tiled-matrix-multiplication">Tiled Matrix Multiplication</a></h1>
<h2 id="overview-28"><a class="header" href="#overview-28">Overview</a></h2>
<p>Implement a kernel that multiplies square matrices \(A\) and \(B\) using tiled matrix multiplication with LayoutTensor. This approach handles large matrices by processing them in smaller chunks (tiles).</p>
<h2 id="key-concepts-25"><a class="header" href="#key-concepts-25">Key concepts</a></h2>
<ul>
<li>Matrix tiling with LayoutTensor for efficient computation</li>
<li>Multi-block coordination with proper layouts</li>
<li>Efficient shared memory usage through TensorBuilder</li>
<li>Boundary handling for tiles with LayoutTensor indexing</li>
</ul>
<h2 id="configuration-15"><a class="header" href="#configuration-15">Configuration</a></h2>
<ul>
<li>Matrix size: \(\text{SIZE_TILED} = 8\)</li>
<li>Threads per block: \(\text{TPB} \times \text{TPB} = 3 \times 3\)</li>
<li>Grid dimensions: \(3 \times 3\) blocks</li>
<li>Shared memory: Two \(\text{TPB} \times \text{TPB}\) LayoutTensors per block</li>
</ul>
<p>Layout configuration:</p>
<ul>
<li>Input A: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code></li>
<li>Input B: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code></li>
<li>Output: <code>Layout.row_major(SIZE_TILED, SIZE_TILED)</code></li>
<li>Shared Memory: Two <code>TPB √ó TPB</code> LayoutTensors using TensorBuilder</li>
</ul>
<h2 id="tiling-strategy"><a class="header" href="#tiling-strategy">Tiling strategy</a></h2>
<h3 id="block-organization"><a class="header" href="#block-organization">Block organization</a></h3>
<pre><code class="language-txt">Grid Layout (3√ó3):           Thread Layout per Block (3√ó3):
[B00][B01][B02]               [T00 T01 T02]
[B10][B11][B12]               [T10 T11 T12]
[B20][B21][B22]               [T20 T21 T22]

Each block processes a tile using LayoutTensor indexing
</code></pre>
<h3 id="tile-processing-steps"><a class="header" href="#tile-processing-steps">Tile processing steps</a></h3>
<ol>
<li>Calculate global and local indices for thread position</li>
<li>Allocate shared memory for A and B tiles</li>
<li>For each tile:
<ul>
<li>Reset shared memory</li>
<li>Load tile from matrix A and B</li>
<li>Compute partial products</li>
<li>Accumulate results in registers</li>
</ul>
</li>
<li>Write final accumulated result</li>
</ol>
<h3 id="memory-access-pattern-1"><a class="header" href="#memory-access-pattern-1">Memory access pattern</a></h3>
<pre><code class="language-txt">Matrix A (8√ó8)                 Matrix B (8√ó8)               Matrix C (8√ó8)
+---+---+---+                  +---+---+---+                +---+---+---+
|T00|T01|T02| ...              |T00|T01|T02| ...            |T00|T01|T02| ...
+---+---+---+                  +---+---+---+                +---+---+---+
|T10|T11|T12|                  |T10|T11|T12|                |T10|T11|T12|
+---+---+---+                  +---+---+---+                +---+---+---+
|T20|T21|T22|                  |T20|T21|T22|                |T20|T21|T22|
+---+---+---+                  +---+---+---+                +---+---+---+
  ...                            ...                          ...

Tile Processing (for computing C[T11]):
1. Load tiles from A and B:
   +---+      +---+
   |A11| √ó    |B11|     For each phase k:
   +---+      +---+     C[T11] += A[row, k] √ó B[k, col]

2. Tile movement:
   Phase 1     Phase 2     Phase 3
   A: [T10]    A: [T11]    A: [T12]
   B: [T01]    B: [T11]    B: [T21]

3. Each thread (i,j) in tile computes:
   C[i,j] = Œ£ (A[i,k] √ó B[k,j]) for k in tile width

Synchronization required:
* After loading tiles to shared memory
* After computing each phase
</code></pre>
<h2 id="code-to-complete-23"><a class="header" href="#code-to-complete-23">Code to complete</a></h2>
<pre><code class="language-mojo">alias SIZE_TILED = 8
alias BLOCKS_PER_GRID_TILED = (3, 3)  # each block convers 3x3 elements
alias THREADS_PER_BLOCK_TILED = (TPB, TPB)
alias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)


fn matmul_tiled[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    local_row = thread_idx.x
    local_col = thread_idx.y
    global_row = block_idx.x * TPB + local_row
    global_col = block_idx.y * TPB + local_col
    # FILL ME IN (roughly 20 lines)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p14/p14.mojo" class="filename">View full file: problems/p14/p14.mojo</a></p>
<details>
<summary><strong>Tips</strong></summary>
<div class="solution-tips">
<ol>
<li>Calculate global thread positions from block and thread indices correctly</li>
<li>Clear shared memory before loading new tiles</li>
<li>Load tiles with proper bounds checking</li>
<li>Accumulate results across tiles with proper synchronization</li>
</ol>
</div>
</details>
<h2 id="running-the-code-24"><a class="header" href="#running-the-code-24">Running the code</a></h2>
<p>To test your solution, run the following command in your terminal:</p>
<pre><code class="language-bash">magic run p14 --tiled
</code></pre>
<p>Your output will look like this if the puzzle isn‚Äôt solved yet:</p>
<pre><code class="language-txt">out: HostBuffer([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
expected: HostBuffer([2240.0, 2296.0, 2352.0, 2408.0, 2464.0, 2520.0, 2576.0, 2632.0, 5824.0, 6008.0, 6192.0, 6376.0, 6560.0, 6744.0, 6928.0, 7112.0, 9408.0, 9720.0, 10032.0, 10344.0, 10656.0, 10968.0, 11280.0, 11592.0, 12992.0, 13432.0, 13872.0, 14312.0, 14752.0, 15192.0, 15632.0, 16072.0, 16576.0, 17144.0, 17712.0, 18280.0, 18848.0, 19416.0, 19984.0, 20552.0, 20160.0, 20856.0, 21552.0, 22248.0, 22944.0, 23640.0, 24336.0, 25032.0, 23744.0, 24568.0, 25392.0, 26216.0, 27040.0, 27864.0, 28688.0, 29512.0, 27328.0, 28280.0, 29232.0, 30184.0, 31136.0, 32088.0, 33040.0, 33992.0])
</code></pre>
<h2 id="solution-manual-tiling"><a class="header" href="#solution-manual-tiling">Solution: Manual tiling</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">fn matmul_tiled[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    tiled_col = block_idx.x * TPB + thread_idx.x
    tiled_row = block_idx.y * TPB + thread_idx.y
    local_col = thread_idx.x
    local_row = thread_idx.y

    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()

    var acc: out.element_type = 0

    # Iterate over tiles to compute matrix product
    @parameter
    for tile in range((size + TPB - 1) // TPB):
        # Reset shared memory tiles
        if local_row &lt; TPB and local_col &lt; TPB:
            a_shared[local_row, local_col] = 0
            b_shared[local_row, local_col] = 0

        barrier()

        # Load A tile - global row stays the same, col determined by tile
        if tiled_row &lt; size and (tile * TPB + local_col) &lt; size:
            a_shared[local_row, local_col] = a[
                tiled_row, tile * TPB + local_col
            ]

        # Load B tile - row determined by tile, global col stays the same
        if (tile * TPB + local_row) &lt; size and tiled_col &lt; size:
            b_shared[local_row, local_col] = b[
                tile * TPB + local_row, tiled_col
            ]

        barrier()

        # Matrix multiplication within the tile
        if tiled_row &lt; size and tiled_col &lt; size:

            @parameter
            for k in range(min(TPB, size - tile * TPB)):
                acc += a_shared[local_row, k] * b_shared[k, local_col]

        barrier()

    # Write out final result
    if tiled_row &lt; size and tiled_col &lt; size:
        out[tiled_row, tiled_col] = acc


</code></pre>
<div class="solution-explanation">
<p>The tiled matrix multiplication implementation demonstrates efficient handling of large matrices \((8 \times 8)\) using small tiles \((3 \times 3)\). Here‚Äôs how it works:</p>
<ol>
<li>
<p><strong>Thread indexing setup</strong></p>
<ul>
<li>Global position calculation:
<pre><code class="language-txt">tiled_row = block_idx.y * TPB + thread_idx.y
tiled_col = block_idx.x * TPB + thread_idx.x
</code></pre>
</li>
<li>Local position in tile:
<pre><code class="language-txt">local_row = thread_idx.y
local_col = thread_idx.x
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Shared memory allocation</strong></p>
<pre><code class="language-txt">Input matrices (8√ó8):
A = [0  1  2  3  4  5  6  7 ]    B = [0  2  4  6  8  10 12 14]
    [8  9  10 11 12 13 14 15]        [16 18 20 22 24 26 28 30]
    [16 17 18 19 20 21 22 23]        [32 34 36 38 40 42 44 46]
    [24 25 26 27 28 29 30 31]        [48 50 52 54 56 58 60 62]
    [32 33 34 35 36 37 38 39]        [64 66 68 70 72 74 76 78]
    [40 41 42 43 44 45 46 47]        [80 82 84 86 88 90 92 94]
    [48 49 50 51 52 53 54 55]        [96 98 100 102 104 106 108 110]
    [56 57 58 59 60 61 62 63]        [112 114 116 118 120 122 124 126]

Shared memory per block (3√ó3):
a_shared[TPB, TPB]  b_shared[TPB, TPB]
</code></pre>
</li>
<li>
<p><strong>Tile processing loop</strong></p>
<pre><code class="language-txt">Number of tiles = (8 + 3 - 1) // 3 = 3 tiles

For each tile:
1. Reset shared memory
2. Load tile from A and B
3. Compute partial products
4. Accumulate in register
</code></pre>
</li>
<li>
<p><strong>Memory loading pattern</strong></p>
<ul>
<li>Loading A tile:
<pre><code class="language-txt">if tiled_row &lt; size and (tile * TPB + local_col) &lt; size:
    a_shared[local_row, local_col] = a[tiled_row, tile * TPB + local_col]
</code></pre>
</li>
<li>Loading B tile:
<pre><code class="language-txt">if (tile * TPB + local_row) &lt; size and tiled_col &lt; size:
    b_shared[local_row, local_col] = b[tile * TPB + local_row, tiled_col]
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Computation within tile</strong></p>
<pre><code class="language-txt">For k in range(min(TPB, size - tile * TPB)):
    acc += a_shared[local_row, k] * b_shared[k, local_col]
</code></pre>
<ul>
<li>Maximizes memory coalescing:
<pre><code class="language-txt">Coalesced Access (Good):          Non-Coalesced Access (Bad):
Thread0: [M0][M1][M2][M3]         Thread0: [M0][ ][ ][ ]
Thread1: [M4][M5][M6][M7]    vs   Thread1: [ ][M1][ ][ ]
Thread2: [M8][M9][MA][MB]         Thread2: [ ][ ][M2][ ]
Thread3: [MC][MD][ME][MF]         Thread3: [ ][ ][ ][M3]
‚Üì                                 ‚Üì
1 memory transaction              4 memory transactions
</code></pre>
When threads access consecutive memory locations (left), the GPU can combine multiple reads into a single transaction.
When threads access scattered locations (right), each access requires a separate transaction, reducing performance.</li>
</ul>
</li>
<li>
<p><strong>Synchronization points</strong></p>
<pre><code class="language-txt">barrier() after:
1. Shared memory reset
2. Tile loading
3. Tile computation
</code></pre>
</li>
</ol>
<p>Key performance features:</p>
<ul>
<li>Processes 8√ó8 matrix using 3√ó3 tiles</li>
<li>Uses shared memory for fast tile access</li>
<li>Minimizes global memory transactions</li>
<li>Handles matrix boundaries correctly</li>
<li>Maintains coalesced memory access</li>
</ul>
<ol start="2">
<li><strong>Boundary handling</strong>:
<pre><code class="language-mojo">if row &lt; size and col &lt; size:
    out[row, col] = acc
</code></pre>
<ul>
<li>Prevents out-of-bounds access</li>
<li>Handles matrix edges</li>
<li>Safe result writing</li>
</ul>
</li>
</ol>
<h3 id="key-optimizations"><a class="header" href="#key-optimizations">Key optimizations</a></h3>
<ol>
<li>
<p><strong>Layout optimization</strong>:</p>
<ul>
<li>Row-major layout for all tensors</li>
<li>Efficient 2D indexing</li>
</ul>
</li>
<li>
<p><strong>Memory access</strong>:</p>
<ul>
<li>Coalesced global memory loads</li>
<li>Efficient shared memory usage</li>
</ul>
</li>
<li>
<p><strong>Computation</strong>:</p>
<ul>
<li>Register-based accumulation i.e. <code>var acc: out.element_type = 0</code></li>
<li>Compile-time loop unrolling via <code>@parameter</code></li>
</ul>
</li>
</ol>
<p>This implementation achieves high performance through:</p>
<ul>
<li>Efficient use of LayoutTensor for memory access</li>
<li>Optimal tiling strategy</li>
<li>Proper thread synchronization</li>
<li>Careful boundary handling</li>
</ul>
</div>
</details>
<h2 id="solution-idiomatic-layouttensor-tiling"><a class="header" href="#solution-idiomatic-layouttensor-tiling">Solution: Idiomatic LayoutTensor tiling</a></h2>
<details class="solution-details">
<summary></summary>
<pre><code class="language-mojo">from gpu.memory import async_copy_wait_all
from layout.layout_tensor import copy_dram_to_sram_async


fn matmul_idiomatic_tiled[
    layout: Layout, size: Int
](
    out: LayoutTensor[mut=False, dtype, layout],
    a: LayoutTensor[mut=False, dtype, layout],
    b: LayoutTensor[mut=False, dtype, layout],
):
    # Get the tile of the output matrix `out` that this thread block is responsible for
    out_tile = out.tile[TPB, TPB](block_idx.y, block_idx.x)
    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()
    local_row = thread_idx.y
    local_col = thread_idx.x

    var acc: out.element_type = 0

    alias load_a_layout = Layout.row_major(1, TPB)
    alias load_b_layout = Layout.row_major(TPB, 1)
    for idx in range((size + TPB - 1) // TPB):
        a_tile = a.tile[TPB, TPB](block_idx.y, idx)
        b_tile = b.tile[TPB, TPB](idx, block_idx.x)

        copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)
        copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)

        async_copy_wait_all()

        barrier()

        @parameter
        for k in range(TPB):
            acc += a_shared[local_row, k] * b_shared[k, local_col]

        barrier()

    if (
        block_idx.y * TPB + local_row &lt; size
        and block_idx.x * TPB + local_col &lt; size
    ):
        out_tile[local_row, local_col] = acc


</code></pre>
<div class="solution-explanation">
<p>The idiomatic tiled matrix multiplication leverages Mojo‚Äôs LayoutTensor API and asynchronous memory operations for a cleaner implementation:</p>
<ol>
<li>
<p><strong>LayoutTensor tile API</strong></p>
<pre><code class="language-mojo">out_tile = out.tile[TPB, TPB](block_idx.y, block_idx.x)
a_tile = a.tile[TPB, TPB](block_idx.y, idx)
b_tile = b.tile[TPB, TPB](idx, block_idx.x)
</code></pre>
<p>This directly expresses ‚Äúget the tile at position (block_idx.y, block_idx.x)‚Äù without manual coordinate calculation. See the <a href="https://docs.modular.com/mojo/kernels/layout/layout_tensor/LayoutTensor/#tile">documentation</a> for more details.</p>
</li>
<li>
<p><strong>Asynchronous memory operations</strong></p>
<pre><code class="language-mojo">copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)
copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)
async_copy_wait_all()
</code></pre>
<p>These operations:</p>
<ul>
<li>Launch asynchronous memory transfers that may overlap with computation via <a href="https://docs.modular.com/mojo/kernels/layout/layout_tensor/copy_dram_to_sram_async/">copy_dram_to_sram_async</a></li>
<li>Use specialized thread layouts for optimal memory access patterns</li>
<li>Eliminate the need for manual memory initialization</li>
</ul>
</li>
<li>
<p><strong>Specialized compile-time load layouts</strong></p>
<pre><code class="language-mojo">alias load_a_layout = Layout.row_major(1, TPB)
alias load_b_layout = Layout.row_major(TPB, 1)
</code></pre>
<p>These layouts optimize how threads cooperate during memory transfers:</p>
<ul>
<li><code>load_a_layout</code>: Each thread loads a slice of a row (coalesced access)</li>
<li><code>load_b_layout</code>: Each thread loads a slice of a column (transposed access)</li>
</ul>
</li>
<li>
<p><strong>Efficient thread synchronization</strong></p>
<pre><code class="language-mojo">// Wait for async operations to complete
async_copy_wait_all()
// Ensure all threads can see the shared memory contents
barrier()
</code></pre>
<p>The barriers ensure proper synchronization:</p>
<ul>
<li>After memory transfers complete</li>
<li>After computation for each tile</li>
</ul>
</li>
<li>
<p><strong>Proper boundary handling</strong></p>
<pre><code class="language-mojo">if block_idx.y * TPB + local_row &lt; size and block_idx.x * TPB + local_col &lt; size:
    out_tile[local_row, local_col] = acc
</code></pre>
<p>This critical check prevents out-of-bounds writes for blocks at the matrix boundaries.</p>
</li>
<li>
<p><strong>Tile processing loop</strong></p>
<pre><code class="language-mojo">for idx in range((size + TPB - 1) // TPB):
   // Process one tile
</code></pre>
<p>Uses ceiling division to handle matrices whose dimensions aren‚Äôt perfect multiples of the tile size.</p>
</li>
</ol>
<h3 id="performance-considerations"><a class="header" href="#performance-considerations">Performance considerations</a></h3>
<p>The idiomatic implementation maintains the performance benefits of tiling while providing cleaner abstractions:</p>
<ol>
<li><strong>Memory locality</strong>: Exploits spatial and temporal locality through tiling</li>
<li><strong>Coalesced access</strong>: Specialized load layouts ensure coalesced memory access patterns</li>
<li><strong>Compute-memory overlap</strong>: Potential overlap through asynchronous memory operations</li>
<li><strong>Shared memory efficiency</strong>: No redundant initialization of shared memory</li>
<li><strong>Register pressure</strong>: Uses accumulation registers for optimal compute throughput</li>
</ol>
<p>This implementation shows how high-level abstractions can express complex GPU algorithms without sacrificing performance. It‚Äôs a prime example of Mojo‚Äôs philosophy: combining high-level expressiveness with low-level performance control.</p>
<h3 id="key-differences-from-manual-tiling"><a class="header" href="#key-differences-from-manual-tiling">Key differences from manual tiling</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Manual Tiling</th><th>Idiomatic Tiling</th></tr></thead><tbody>
<tr><td>Memory access</td><td>Direct indexing with bounds checks</td><td>LayoutTensor tile API</td></tr>
<tr><td>Tile loading</td><td>Explicit element-by-element copying</td><td>Asynchronous bulk transfers</td></tr>
<tr><td>Shared memory</td><td>Manual initialization (zeroing)</td><td>Managed by copy functions</td></tr>
<tr><td>Code complexity</td><td>More verbose with explicit indexing</td><td>More concise with higher-level APIs</td></tr>
<tr><td>Bounds checking</td><td>Multiple checks during loading and computing</td><td>Single check at final write</td></tr>
</tbody></table>
</div>
<p>The idiomatic approach is not just cleaner but also potentially more performant due to the use of specialized memory layouts and asynchronous operations.</p>
</div>
</details>
<div style="break-before: page; page-break-before: always;"></div><h1 id="puzzle-15-1d-convolution-op"><a class="header" href="#puzzle-15-1d-convolution-op">Puzzle 15: 1D Convolution Op</a></h1>
<blockquote>
<h2 id="bridging-to-python-with-max-graph"><a class="header" href="#bridging-to-python-with-max-graph">Bridging to Python with MAX Graph</a></h2>
<p>We‚Äôre now entering Part III of our GPU puzzle journey: <strong>Interfacing with Python via MAX Graph Custom Ops</strong>.</p>
<p>In previous puzzles, we‚Äôve learned how to write efficient GPU kernels in Mojo. Now we‚Äôll explore how to:</p>
<ul>
<li>Package these kernels as custom operations that can be called from Python</li>
<li>Integrate with the MAX Graph system for accelerated machine learning</li>
<li>Bridge the gap between high-level Python APIs and low-level GPU code</li>
</ul>
<p>This integration allows us to leverage the performance of Mojo GPU kernels while working in familiar Python environments.</p>
</blockquote>
<h2 id="overview-29"><a class="header" href="#overview-29">Overview</a></h2>
<p>In <a href="puzzle_15/../puzzle_11/puzzle_11.html">Puzzle 11</a>, we implemented a 1D convolution kernel that runs efficiently on the GPU. Now we‚Äôll take this kernel and transform it into a custom operation that can be called directly from Python using <a href="https://docs.modular.com/max/api/python/graph/">MAX Graph</a>.</p>
<p>The 1D convolution kernel we‚Äôll be working with is already implemented:</p>
<pre><code class="language-mojo">alias TPB = 15
alias BLOCKS_PER_GRID = (2, 1)


fn conv1d_kernel[
    in_layout: Layout,
    out_layout: Layout,
    conv_layout: Layout,
    input_size: Int,
    conv_size: Int,
    dtype: DType = DType.float32,
](
    out: LayoutTensor[mut=True, dtype, out_layout],
    input: LayoutTensor[mut=True, dtype, in_layout],
    kernel: LayoutTensor[mut=True, dtype, conv_layout],
):
    global_i = block_dim.x * block_idx.x + thread_idx.x
    local_i = thread_idx.x
    # first: need to account for padding
    shared_a = tb[dtype]().row_major[TPB + conv_size - 1]().shared().alloc()
    shared_b = tb[dtype]().row_major[conv_size]().shared().alloc()
    if global_i &lt; input_size:
        shared_a[local_i] = input[global_i]

    # second: load elements needed for convolution at block boundary
    if local_i &lt; conv_size - 1:
        # indices from next block
        next_idx = global_i + TPB
        if next_idx &lt; input_size:
            shared_a[TPB + local_i] = input[next_idx]

    if local_i &lt; conv_size:
        shared_b[local_i] = kernel[local_i]

    barrier()

    if global_i &lt; input_size:
        var local_sum: out.element_type = 0

        @parameter
        for j in range(conv_size):
            if local_i + j &lt; TPB + conv_size - 1:
                local_sum += shared_a[local_i + j] * shared_b[j]

        out[global_i] = local_sum


</code></pre>
<p>The key aspects of this puzzle include:</p>
<ol>
<li><strong>Custom op registration</strong>: Understanding how to expose Mojo functions to Python via the <code>@compiler.register</code> decorator</li>
<li><strong>Packaging custom ops</strong>: Learning how to package Mojo code for use with MAX Graph</li>
<li><strong>Python integration</strong>: Calling custom operations from Python through MAX Graph</li>
<li><strong>Cross-language data flow</strong>: Managing data types and memory between Python and GPU</li>
</ol>
<p>This custom operation will:</p>
<ul>
<li>Accept <a href="https://numpy.org/doc/stable/">NumPy</a> arrays as input from Python</li>
<li>Transfer this data to the GPU</li>
<li>Execute our optimized convolution kernel</li>
<li>Return the results back to Python</li>
</ul>
<p>When you complete this puzzle, you‚Äôll have created a seamless bridge between Python‚Äôs rich ecosystem and Mojo‚Äôs powerful GPU performance.</p>
<h2 id="code-to-complete-24"><a class="header" href="#code-to-complete-24">Code to complete</a></h2>
<p>To complete this puzzle, you only need to fill one line to call the <code>conv1d_kernel</code>:</p>
<pre><code class="language-mojo">import compiler
from runtime.asyncrt import DeviceContextPtr
from tensor import InputTensor, OutputTensor
from memory import UnsafePointer
from gpu.host import DeviceBuffer


@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[
        # The kind of device this will be run on: "cpu" or "gpu"
        target: StaticString,
        input_size: Int,
        conv_size: Int,
        dtype: DType = DType.float32,
    ](
        out: OutputTensor[rank=1],
        input: InputTensor[type = out.type, rank = out.rank],
        kernel: InputTensor[type = out.type, rank = out.rank],
        # the context is needed for some GPU calls
        ctx: DeviceContextPtr,
    ) raises:
        out_tensor = out.to_layout_tensor()
        input_tensor = input.to_layout_tensor()
        kernel_tensor = kernel.to_layout_tensor()
        alias in_layout = input_tensor.layout
        alias out_layout = out_tensor.layout
        alias conv_layout = kernel_tensor.layout

        @parameter
        if target == "gpu":
            gpu_ctx = ctx.get_device_context()
            # making sure the output tensor is zeroed out before the kernel is called
            gpu_ctx.enqueue_memset(
                DeviceBuffer[out.type](
                    gpu_ctx,
                    rebind[UnsafePointer[Scalar[out.type]]](out_tensor.ptr),
                    input_size,
                    owning=False,
                ),
                0,
            )

            # FILL ME IN with 1 line calling our conv1d_kernel

        elif target == "cpu":
            # we can fallback to CPU
            pass
        else:
            raise Error("Unsupported target: " + target)


</code></pre>
<p><a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p15/op/conv1d.mojo" class="filename">View full file: problems/p15/op/conv1d.mojo</a></p>
<p>You can run the puzzle with:</p>
<pre><code class="language-bash">magic run p15
</code></pre>
<p>When successful, you should see output similar to:</p>
<pre><code>Input array: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]
Convolution kernel: [0. 1. 2. 3.]
Expected result (NumPy calculation): [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]
Compiling 1D convolution graph...
Executing 1D convolution...
1D Convolution result (custom Mojo kernel): [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]
Verification passed: Custom kernel results match NumPy calculation
</code></pre>
<p>This indicates that your custom MAX Graph operation correctly implements the 1D convolution algorithm.</p>
<h2 id="solution-23"><a class="header" href="#solution-23">Solution</a></h2>
<details class="solution-details">
<summary></summary>
<p>To solve this puzzle, we need to integrate our 1D convolution kernel with the MAX Graph system. The key is to properly call our kernel from the <code>execute</code> method in the <code>Conv1DCustomOp</code> struct.</p>
<p>The solution is:</p>
<pre><code class="language-mojo">            gpu_ctx.enqueue_function[
                conv1d_kernel[
                    in_layout, out_layout, conv_layout, input_size, conv_size
                ]
            ](
                out_tensor,
                input_tensor,
                kernel_tensor,
                input_size,
                conv_size,
                grid_dim=BLOCKS_PER_GRID,
                block_dim=(TPB, 1),
            )
</code></pre>
<p>This single line does several important things:</p>
<ol>
<li>Calls <a href="https://docs.modular.com/mojo/stdlib/gpu/host/device_context/DeviceContext/#enqueue_function">enqueue_function</a> on the GPU context (<code>gpu_ctx</code> is of type <a href="https://docs.modular.com/mojo/stdlib/gpu/host/device_context/DeviceContext/">DeviceContext</a>) to schedule our kernel execution</li>
<li>Passes the necessary layout and size information as <strong>compile-time</strong> parameters</li>
<li>Provides the output, input, and kernel tensors as runtime arguments</li>
<li>Configures the execution grid with the appropriate dimensions</li>
</ol>
<p>Let‚Äôs break down how this works in the larger context:</p>
<h3 id="python-mojo-integration-flow"><a class="header" href="#python-mojo-integration-flow">Python-Mojo integration flow</a></h3>
<ol>
<li>
<p><strong>Python side (<a href="https://github.com/modular/mojo-gpu-puzzles/blob/main/problems/p15/p15.py" class="filename">problems/p15/p15.py</a>)</strong>:</p>
<ul>
<li>Creates NumPy arrays for input and kernel</li>
<li>Calls <code>conv_1d()</code> function which wraps our operation in MAX Graph</li>
<li>Converts NumPy arrays to <a href="https://docs.modular.com/max/api/python/driver">MAX driver</a> Tensors with <code>Tensor.from_numpy(input).to(device)</code></li>
<li>Loads the custom operation package with <code>custom_extensions=[mojo_kernels]</code></li>
</ul>
</li>
<li>
<p><strong>Graph building</strong>:</p>
<ul>
<li>Defines input and output tensor types with <a href="https://docs.modular.com/max/api/python/graph/type/#max.graph.type.TensorType">TensorType</a></li>
<li>Specifies parameters for our operation via <code>parameters={...}</code></li>
<li>Creates a computation graph with <a href="https://docs.modular.com/max/api/python/graph/Graph"><code>Graph("conv_1d_graph", ...)</code></a></li>
<li>Calls our operation using <a href="https://docs.modular.com/max/api/python/graph/ops#custom"><code>ops.custom(name="conv1d", ...)</code></a></li>
</ul>
</li>
<li>
<p><strong>Custom op registration</strong>:</p>
<ul>
<li>The <code>@compiler.register("conv1d")</code> decorator exposes our operation to MAX Graph. See <a href="https://docs.modular.com/mojo/manual/decorators/compiler-register/">@compiler.register</a></li>
<li>The <code>execute</code> method parameters define the interface (inputs, outputs, context)</li>
<li>Input/output tensors are converted to LayoutTensors for use in our kernel</li>
<li>Device context manages GPU memory allocation and kernel execution</li>
</ul>
</li>
<li>
<p><strong>Kernel execution</strong>:</p>
<ul>
<li>When <a href="puzzle_15/">model.execute(‚Ä¶)</a> is called, our <code>conv1d_kernel</code> receives the data</li>
<li>GPU thread configuration is set with <code>grid_dim</code> and <code>block_dim</code></li>
<li>Results are transferred back to CPU with <code>result.to(CPU())</code></li>
<li>NumPy verification compares our results with the expected output</li>
</ul>
</li>
</ol>
<h3 id="key-components-in-detail"><a class="header" href="#key-components-in-detail">Key Components in Detail</a></h3>
<ol>
<li>
<p><strong>Custom Op Structure</strong>:</p>
<pre><code class="language-mojo">@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[target: StaticString, input_size: Int, conv_size: Int, dtype: DType = DType.float32](
        out: OutputTensor[rank=1],
        input: InputTensor[type = out.type, rank = out.rank],
        kernel: InputTensor[type = out.type, rank = out.rank],
        ctx: DeviceContextPtr,
    ) raises:
        # Implementation
</code></pre>
<ul>
<li><code>target</code> indicates the device type (‚Äúgpu‚Äù or ‚Äúcpu‚Äù)</li>
<li><code>input_size</code> and <code>conv_size</code> are parameters passed from Python</li>
<li>Tensor types ensure correct shape and type checking</li>
<li>Return type is <code>raises</code> for proper error handling</li>
</ul>
</li>
<li>
<p><strong>Tensor Conversion</strong>:</p>
<pre><code class="language-mojo">out_tensor = out.to_layout_tensor()
input_tensor = input.to_layout_tensor()
kernel_tensor = kernel.to_layout_tensor()
</code></pre>
<ul>
<li>MAX Graph tensors are converted to Mojo LayoutTensors</li>
<li>This allows our kernel to work with them directly</li>
<li>The layouts are extracted for compile-time optimization</li>
</ul>
</li>
<li>
<p><strong>Device Context Usage</strong>:</p>
<pre><code class="language-mojo">gpu_ctx = ctx.get_device_context()
gpu_ctx.enqueue_memset(...)  # Zero output buffer
gpu_ctx.enqueue_function[...](...) # Schedule kernel
</code></pre>
<ul>
<li>Device context manages GPU resources</li>
<li>Memory operations ensure correct buffer state</li>
<li>Function enqueueing schedules our kernel for execution</li>
</ul>
</li>
</ol>
<p>This solution demonstrates the complete flow from Python data through MAX Graph to GPU execution and back, leveraging Mojo‚Äôs powerful type system and parametric functions to create efficient, type-safe, accelerated operations.</p>
</details>
<h2 id="understanding-max-graph-custom-ops"><a class="header" href="#understanding-max-graph-custom-ops">Understanding MAX Graph custom ops</a></h2>
<blockquote>
<p>Check out the follow tutorials for more details:</p>
<ul>
<li><a href="https://docs.modular.com/max/tutorials/get-started-with-max-graph-in-python/">Get started with MAX Graph in Python</a></li>
<li><a href="https://docs.modular.com/max/tutorials/build-custom-ops/">MAX Graph custom op for GPUs</a></li>
</ul>
</blockquote>
<h3 id="custom-op-registration"><a class="header" href="#custom-op-registration">Custom op registration</a></h3>
<p>The core of creating a custom operation is the <code>@compiler.register</code> decorator and the associated structure:</p>
<pre><code class="language-mojo">@compiler.register("conv1d")
struct Conv1DCustomOp:
    @staticmethod
    fn execute[...](
        out: OutputTensor[rank=1],
        input: InputTensor[type = out.type, rank = out.rank],
        kernel: InputTensor[type = out.type, rank = out.rank],
        ctx: DeviceContextPtr,
    ) raises:
        # Implementation here
</code></pre>
<p>Key components of the registration:</p>
<ul>
<li>The <strong>name</strong> passed to the decorator (<code>"conv1d"</code>) is what Python code will use to call this operation</li>
<li>The <strong>struct</strong> must have an <code>execute</code> method with the correct signature</li>
<li><strong>OutputTensor</strong> and <strong>InputTensor</strong> types define the interface for Python data</li>
<li><strong>DeviceContextPtr</strong> provides access to the execution environment</li>
</ul>
<h3 id="packaging-custom-ops"><a class="header" href="#packaging-custom-ops">Packaging custom ops</a></h3>
<p>Before the custom operation can be used from Python, it needs to be packaged:</p>
<pre><code class="language-bash">mojo package op -o op.mojopkg
</code></pre>
<p>This command:</p>
<ol>
<li>Compiles the Mojo code into a deployable package</li>
<li>Creates the necessary metadata for MAX Graph to understand the operation</li>
<li>Produces a binary artifact (<code>op.mojopkg</code>) that can be loaded by Python</li>
</ol>
<p>The package must be placed in a location where MAX Graph can find it, typically in a directory accessible to the Python code.</p>
<blockquote>
<p>In the puzzles repository, this is done via a <code>magic</code> task to automate the step. See <a href="puzzle_15/../../../mojoproject.toml">mojoproject.toml</a> for more details.</p>
</blockquote>
<h3 id="python-integration"><a class="header" href="#python-integration">Python integration</a></h3>
<p>On the Python side, here‚Äôs how the custom operation is used:</p>
<pre><code class="language-python"># Path to the directory containing our Mojo operations
mojo_kernels = Path(__file__).parent / "op"

# Configure our graph with the custom conv1d operation
with Graph(
    "conv_1d_graph",
    input_types=[...],
    custom_extensions=[mojo_kernels],  # Load our custom op package
) as graph:
    # Define inputs to the graph
    input_value, kernel_value = graph.inputs

    # Use our custom operation by name
    output = ops.custom(
        name="conv1d",  # Must match the name in @compiler.register
        values=[input_value, kernel_value],
        out_types=[...],
        parameters={
            "input_size": input_tensor.shape[0],
            "conv_size": kernel_tensor.shape[0],
            "dtype": dtype,
        },
    )[0].tensor
</code></pre>
<p>The key elements are:</p>
<ol>
<li>Specifying the path to our custom operations with <code>custom_extensions</code></li>
<li>Calling <code>ops.custom</code> with the registered operation name</li>
<li>Passing input values and parameters that match our operation‚Äôs signature</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/mojolang.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/solution.js"></script>
        <script src="theme/init-amplitude.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
